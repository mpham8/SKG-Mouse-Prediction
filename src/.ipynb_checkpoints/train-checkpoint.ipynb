{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from os.path import abspath\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils.generate_network import generate_network\n",
    "from utils.prepare_data import prepare_data\n",
    "from utils.popphy_io import save_params, load_params\n",
    "from utils.popphy_io import get_stat, get_stat_dict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from models.PopPhy import PopPhyCNN\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import tensorflow as tf\n",
    "#from models.PopPhy2 import ResNet\n",
    "from models.PopPhy2 import ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "### Reading Configuration\n",
    "Configuring which data to read in, minimun threshold needed in an OTU (individual sample must have at least set threshold relative abundance), and how many k folds for k fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'T2D'\n",
    "threshold = 0\n",
    "k = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Features\n",
    "Reduce amount of OTU features by filtering out OTUs that contain no individual sample with a relative abundance greater than the set threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>431</th>\n",
       "      <th>432</th>\n",
       "      <th>433</th>\n",
       "      <th>434</th>\n",
       "      <th>435</th>\n",
       "      <th>436</th>\n",
       "      <th>437</th>\n",
       "      <th>438</th>\n",
       "      <th>439</th>\n",
       "      <th>440</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>k__Archaea|p__Euryarchaeota|c__Methanobacteria|o__Methanobacteriales|f__Methanobacteriaceae|g__Methanobrevibacter|s__Methanobrevibacter_smithii</th>\n",
       "      <td>0.33364</td>\n",
       "      <td>0.49776</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49446</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.76247</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.96027</td>\n",
       "      <td>7.44320</td>\n",
       "      <td>0.02598</td>\n",
       "      <td>2.78607</td>\n",
       "      <td>2.46789</td>\n",
       "      <td>6.72433</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k__Archaea|p__Euryarchaeota|c__Methanobacteria|o__Methanobacteriales|f__Methanobacteriaceae|g__Methanobrevibacter|s__Methanobrevibacter_unclassified</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.12802</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06786</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.07156</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k__Archaea|p__Euryarchaeota|c__Methanobacteria|o__Methanobacteriales|f__Methanobacteriaceae|g__Methanosphaera|s__Methanosphaera_stadtmanae</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.55541</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k__Bacteria|p__Acidobacteria|c__Acidobacteriia|o__Acidobacteriales|f__Acidobacteriaceae|g__Acidobacteriaceae_unclassified</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k__Bacteria|p__Actinobacteria|c__Actinobacteria|o__Actinomycetales|f__Actinomycetaceae|g__Actinomyces|s__Actinomyces_graevenitzii</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.06781</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k__Bacteria|p__Planctomycetes|c__Planctomycetia|o__Planctomycetales|f__Planctomycetaceae|g__Rhodopirellula|s__Rhodopirellula_unclassified</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Vibrionales|f__Vibrionaceae|g__Vibrio|s__Vibrio_furnissii</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k__Bacteria|p__Bacteroidetes|c__Bacteroidia|o__Bacteroidales|f__Bacteroidaceae|g__Bacteroides|s__Bacteroides_sp_2_2_4</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k__Bacteria|p__Firmicutes|c__Bacilli|o__Bacillales|f__Bacillaceae|g__Lysinibacillus|s__Lysinibacillus_fusiformis</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.02423</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k__Bacteria|p__Firmicutes|c__Bacilli|o__Bacillales|f__Bacillaceae|g__Lysinibacillus|s__Lysinibacillus_sphaericus</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.76404</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>606 rows × 440 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        1        2    3    \\\n",
       "0                                                                           \n",
       "k__Archaea|p__Euryarchaeota|c__Methanobacteria|...  0.33364  0.49776  0.0   \n",
       "k__Archaea|p__Euryarchaeota|c__Methanobacteria|...  0.00000  0.12802  0.0   \n",
       "k__Archaea|p__Euryarchaeota|c__Methanobacteria|...  0.00000  0.00000  0.0   \n",
       "k__Bacteria|p__Acidobacteria|c__Acidobacteriia|...  0.00000  0.00000  0.0   \n",
       "k__Bacteria|p__Actinobacteria|c__Actinobacteria...  0.00000  0.00000  0.0   \n",
       "...                                                     ...      ...  ...   \n",
       "k__Bacteria|p__Planctomycetes|c__Planctomycetia...  0.00000  0.00000  0.0   \n",
       "k__Bacteria|p__Proteobacteria|c__Gammaproteobac...  0.00000  0.00000  0.0   \n",
       "k__Bacteria|p__Bacteroidetes|c__Bacteroidia|o__...  0.00000  0.00000  0.0   \n",
       "k__Bacteria|p__Firmicutes|c__Bacilli|o__Bacilla...  0.00000  0.00000  0.0   \n",
       "k__Bacteria|p__Firmicutes|c__Bacilli|o__Bacilla...  0.00000  0.00000  0.0   \n",
       "\n",
       "                                                    4        5    6    7    \\\n",
       "0                                                                            \n",
       "k__Archaea|p__Euryarchaeota|c__Methanobacteria|...  0.0  0.49446  0.0  0.0   \n",
       "k__Archaea|p__Euryarchaeota|c__Methanobacteria|...  0.0  0.06786  0.0  0.0   \n",
       "k__Archaea|p__Euryarchaeota|c__Methanobacteria|...  0.0  0.00000  0.0  0.0   \n",
       "k__Bacteria|p__Acidobacteria|c__Acidobacteriia|...  0.0  0.00000  0.0  0.0   \n",
       "k__Bacteria|p__Actinobacteria|c__Actinobacteria...  0.0  0.00000  0.0  0.0   \n",
       "...                                                 ...      ...  ...  ...   \n",
       "k__Bacteria|p__Planctomycetes|c__Planctomycetia...  0.0  0.00000  0.0  0.0   \n",
       "k__Bacteria|p__Proteobacteria|c__Gammaproteobac...  0.0  0.00000  0.0  0.0   \n",
       "k__Bacteria|p__Bacteroidetes|c__Bacteroidia|o__...  0.0  0.00000  0.0  0.0   \n",
       "k__Bacteria|p__Firmicutes|c__Bacilli|o__Bacilla...  0.0  0.00000  0.0  0.0   \n",
       "k__Bacteria|p__Firmicutes|c__Bacilli|o__Bacilla...  0.0  0.00000  0.0  0.0   \n",
       "\n",
       "                                                    8    9        10   ...  \\\n",
       "0                                                                      ...   \n",
       "k__Archaea|p__Euryarchaeota|c__Methanobacteria|...  0.0  0.0  0.00000  ...   \n",
       "k__Archaea|p__Euryarchaeota|c__Methanobacteria|...  0.0  0.0  0.00000  ...   \n",
       "k__Archaea|p__Euryarchaeota|c__Methanobacteria|...  0.0  0.0  0.00000  ...   \n",
       "k__Bacteria|p__Acidobacteria|c__Acidobacteriia|...  0.0  0.0  0.00000  ...   \n",
       "k__Bacteria|p__Actinobacteria|c__Actinobacteria...  0.0  0.0  0.01089  ...   \n",
       "...                                                 ...  ...      ...  ...   \n",
       "k__Bacteria|p__Planctomycetes|c__Planctomycetia...  0.0  0.0  0.00000  ...   \n",
       "k__Bacteria|p__Proteobacteria|c__Gammaproteobac...  0.0  0.0  0.00000  ...   \n",
       "k__Bacteria|p__Bacteroidetes|c__Bacteroidia|o__...  0.0  0.0  0.00000  ...   \n",
       "k__Bacteria|p__Firmicutes|c__Bacilli|o__Bacilla...  0.0  0.0  0.00000  ...   \n",
       "k__Bacteria|p__Firmicutes|c__Bacilli|o__Bacilla...  0.0  0.0  0.00000  ...   \n",
       "\n",
       "                                                    431      432  433  \\\n",
       "0                                                                       \n",
       "k__Archaea|p__Euryarchaeota|c__Methanobacteria|...  0.0  1.76247  0.0   \n",
       "k__Archaea|p__Euryarchaeota|c__Methanobacteria|...  0.0  0.00000  0.0   \n",
       "k__Archaea|p__Euryarchaeota|c__Methanobacteria|...  0.0  0.55541  0.0   \n",
       "k__Bacteria|p__Acidobacteria|c__Acidobacteriia|...  0.0  0.00000  0.0   \n",
       "k__Bacteria|p__Actinobacteria|c__Actinobacteria...  0.0  0.00000  0.0   \n",
       "...                                                 ...      ...  ...   \n",
       "k__Bacteria|p__Planctomycetes|c__Planctomycetia...  0.0  0.00000  0.0   \n",
       "k__Bacteria|p__Proteobacteria|c__Gammaproteobac...  0.0  0.00000  0.0   \n",
       "k__Bacteria|p__Bacteroidetes|c__Bacteroidia|o__...  0.0  0.00000  0.0   \n",
       "k__Bacteria|p__Firmicutes|c__Bacilli|o__Bacilla...  0.0  0.00000  0.0   \n",
       "k__Bacteria|p__Firmicutes|c__Bacilli|o__Bacilla...  0.0  0.00000  0.0   \n",
       "\n",
       "                                                        434      435      436  \\\n",
       "0                                                                               \n",
       "k__Archaea|p__Euryarchaeota|c__Methanobacteria|...  2.96027  7.44320  0.02598   \n",
       "k__Archaea|p__Euryarchaeota|c__Methanobacteria|...  0.00000  0.00000  0.00000   \n",
       "k__Archaea|p__Euryarchaeota|c__Methanobacteria|...  0.00000  0.00000  0.00000   \n",
       "k__Bacteria|p__Acidobacteria|c__Acidobacteriia|...  0.00000  0.00000  0.00000   \n",
       "k__Bacteria|p__Actinobacteria|c__Actinobacteria...  0.00000  0.06781  0.00000   \n",
       "...                                                     ...      ...      ...   \n",
       "k__Bacteria|p__Planctomycetes|c__Planctomycetia...  0.00000  0.00000  0.00000   \n",
       "k__Bacteria|p__Proteobacteria|c__Gammaproteobac...  0.00000  0.00000  0.00000   \n",
       "k__Bacteria|p__Bacteroidetes|c__Bacteroidia|o__...  0.00000  0.00000  0.00000   \n",
       "k__Bacteria|p__Firmicutes|c__Bacilli|o__Bacilla...  0.00000  0.02423  0.00000   \n",
       "k__Bacteria|p__Firmicutes|c__Bacilli|o__Bacilla...  0.00000  0.76404  0.00000   \n",
       "\n",
       "                                                        437      438      439  \\\n",
       "0                                                                               \n",
       "k__Archaea|p__Euryarchaeota|c__Methanobacteria|...  2.78607  2.46789  6.72433   \n",
       "k__Archaea|p__Euryarchaeota|c__Methanobacteria|...  0.00000  0.00000  0.07156   \n",
       "k__Archaea|p__Euryarchaeota|c__Methanobacteria|...  0.00000  0.00000  0.00000   \n",
       "k__Bacteria|p__Acidobacteria|c__Acidobacteriia|...  0.00000  0.00000  0.00000   \n",
       "k__Bacteria|p__Actinobacteria|c__Actinobacteria...  0.00000  0.00000  0.00000   \n",
       "...                                                     ...      ...      ...   \n",
       "k__Bacteria|p__Planctomycetes|c__Planctomycetia...  0.00000  0.00000  0.00000   \n",
       "k__Bacteria|p__Proteobacteria|c__Gammaproteobac...  0.00000  0.00000  0.00000   \n",
       "k__Bacteria|p__Bacteroidetes|c__Bacteroidia|o__...  0.00000  0.00000  0.00000   \n",
       "k__Bacteria|p__Firmicutes|c__Bacilli|o__Bacilla...  0.00000  0.00000  0.00000   \n",
       "k__Bacteria|p__Firmicutes|c__Bacilli|o__Bacilla...  0.00000  0.00000  0.00000   \n",
       "\n",
       "                                                    440  \n",
       "0                                                        \n",
       "k__Archaea|p__Euryarchaeota|c__Methanobacteria|...  0.0  \n",
       "k__Archaea|p__Euryarchaeota|c__Methanobacteria|...  0.0  \n",
       "k__Archaea|p__Euryarchaeota|c__Methanobacteria|...  0.0  \n",
       "k__Bacteria|p__Acidobacteria|c__Acidobacteriia|...  0.0  \n",
       "k__Bacteria|p__Actinobacteria|c__Actinobacteria...  0.0  \n",
       "...                                                 ...  \n",
       "k__Bacteria|p__Planctomycetes|c__Planctomycetia...  0.0  \n",
       "k__Bacteria|p__Proteobacteria|c__Gammaproteobac...  0.0  \n",
       "k__Bacteria|p__Bacteroidetes|c__Bacteroidia|o__...  0.0  \n",
       "k__Bacteria|p__Firmicutes|c__Bacilli|o__Bacilla...  0.0  \n",
       "k__Bacteria|p__Firmicutes|c__Bacilli|o__Bacilla...  0.0  \n",
       "\n",
       "[606 rows x 440 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../data/\" + dataset\n",
    "data = pd.read_csv(path + '/abundance.tsv', index_col=0, sep='\\t', header=None)\n",
    "to_drop = data.loc[(data < threshold).all(axis=1)]\n",
    "data = data.drop(to_drop.index)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 2d Matrix Representing OTU Data\n",
    "Dai et al. PopPhy-CNN's (2019) algorithm creates Phylogenetic tree from OTUs and populates tree based on OTU abundances. This tree graph structure is then converted to a 2d Matrix by taking each parent node in the tree graph and pushing them all to the left and childrens' nodes in the same order from left to right the parents were ordered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(606, 440)\n",
      "There are 606 raw features...\n",
      "Building tree structure...\n",
      "Found tree file...\n",
      "Populating trees...\n",
      "There are 1054 tree features...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>386</th>\n",
       "      <th>387</th>\n",
       "      <th>388</th>\n",
       "      <th>389</th>\n",
       "      <th>390</th>\n",
       "      <th>391</th>\n",
       "      <th>392</th>\n",
       "      <th>393</th>\n",
       "      <th>394</th>\n",
       "      <th>395</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003336</td>\n",
       "      <td>0.996664</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028760</td>\n",
       "      <td>0.494275</td>\n",
       "      <td>0.079702</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019336</td>\n",
       "      <td>0.006233</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003191</td>\n",
       "      <td>0.494275</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006233</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019336</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001260</td>\n",
       "      <td>0.017147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024597</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16 rows × 396 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0         1         2         3         4         5         6         7    \\\n",
       "0   1.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1   0.0  0.003336  0.996664  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2   0.0  0.003336  0.000000  0.000000  0.000000  0.028760  0.494275  0.079702   \n",
       "3   0.0  0.003336  0.000000  0.000000  0.000000  0.019336  0.006233  0.000000   \n",
       "4   0.0  0.000000  0.003336  0.000000  0.000000  0.000000  0.019336  0.000000   \n",
       "5   0.0  0.000000  0.003336  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "6   0.0  0.000000  0.003336  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "7   0.0  0.000000  0.000000  0.000000  0.003336  0.000000  0.000000  0.000000   \n",
       "8   0.0  0.000000  0.000000  0.003336  0.000000  0.000000  0.000000  0.000000   \n",
       "9   0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "10  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "11  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "12  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "13  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "14  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "15  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "         8         9    ...  386  387  388  389  390  391  392  393  394  395  \n",
       "0   0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1   0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2   0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3   0.003191  0.494275  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4   0.006233  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "5   0.000000  0.019336  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "6   0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "7   0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "8   0.001260  0.017147  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9   0.024597  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "10  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "11  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "12  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "13  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "14  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "15  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[16 rows x 396 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_maps, raw_x, tree_x, raw_features, tree_features, labels, label_set, g, feature_df = prepare_data(path, data)\n",
    "\n",
    "# norms = np.linalg.norm(my_maps, axis=2, keepdims=True)\n",
    "# my_maps = my_maps / norms\n",
    "pd.DataFrame(my_maps[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating training and test sets\n",
    "Splitting data into k training and k test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding\n",
    "input = my_maps\n",
    "target = tf.keras.utils.to_categorical(labels, 2, dtype='int64')\n",
    "    \n",
    "\n",
    "#shuffle dataset\n",
    "seed = np.random.randint(100)\n",
    "# np.random.seed(seed)\n",
    "# np.random.shuffle(input)\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(target)\n",
    "\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(my_maps)\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(raw_x)\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(tree_x)\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(labels)\n",
    "\n",
    "\n",
    "#create k training and k test sets\n",
    "groups_input = []\n",
    "groups_target = []\n",
    "k_size = len(input)//k\n",
    "start, end = 0, k_size\n",
    "for i in range(k):\n",
    "    if i == k-1:\n",
    "        group_input = input[start:]\n",
    "        group_target = target[start:]\n",
    "    else:\n",
    "        group_input = input[start:end]\n",
    "        group_target = target[start:end]\n",
    "    start += k_size\n",
    "    end += k_size\n",
    "    groups_input.append(group_input)\n",
    "    groups_target.append(group_target)\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "for i in range(k-1, -1, -1):\n",
    "    x_train.append(np.concatenate((groups_input[i-1], groups_input[i-2], groups_input[i-3], groups_input[i-4])))\n",
    "    y_train.append(np.concatenate((groups_target[i-1], groups_target[i-2], groups_target[i-3], groups_target[i-4])))\n",
    "\n",
    "    x_test.append(groups_input[i])\n",
    "    y_test.append(groups_target[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "### Training model\n",
    "Data is log transformed and then a MinMax transformation. Uses CNN that employs skipped residual identity blocks borrowed from the classic ResNet model then a FC Neural Network to make phenotype prediction. Model dimensions printed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "Epoch 1/65\n",
      "3/3 [==============================] - 2s 661ms/step - loss: 2.8042 - accuracy: 0.5256 - val_loss: 2.4189 - val_accuracy: 0.5000\n",
      "Epoch 2/65\n",
      "3/3 [==============================] - 1s 372ms/step - loss: 2.3282 - accuracy: 0.5966 - val_loss: 2.1699 - val_accuracy: 0.5000\n",
      "Epoch 3/65\n",
      "3/3 [==============================] - 1s 353ms/step - loss: 2.0021 - accuracy: 0.6847 - val_loss: 1.9442 - val_accuracy: 0.5000\n",
      "Epoch 4/65\n",
      "3/3 [==============================] - 1s 335ms/step - loss: 1.7522 - accuracy: 0.7131 - val_loss: 1.7386 - val_accuracy: 0.5000\n",
      "Epoch 5/65\n",
      "3/3 [==============================] - 1s 364ms/step - loss: 1.5022 - accuracy: 0.7443 - val_loss: 1.5619 - val_accuracy: 0.5000\n",
      "Epoch 6/65\n",
      "3/3 [==============================] - 1s 329ms/step - loss: 1.3151 - accuracy: 0.7784 - val_loss: 1.4215 - val_accuracy: 0.5000\n",
      "Epoch 7/65\n",
      "3/3 [==============================] - 1s 332ms/step - loss: 1.1217 - accuracy: 0.8210 - val_loss: 1.3076 - val_accuracy: 0.5000\n",
      "Epoch 8/65\n",
      "3/3 [==============================] - 1s 332ms/step - loss: 0.9838 - accuracy: 0.8494 - val_loss: 1.2198 - val_accuracy: 0.5000\n",
      "Epoch 9/65\n",
      "3/3 [==============================] - 1s 334ms/step - loss: 0.8747 - accuracy: 0.8551 - val_loss: 1.1482 - val_accuracy: 0.5000\n",
      "Epoch 10/65\n",
      "3/3 [==============================] - 1s 327ms/step - loss: 0.7776 - accuracy: 0.8949 - val_loss: 1.0950 - val_accuracy: 0.5000\n",
      "Epoch 11/65\n",
      "3/3 [==============================] - 1s 378ms/step - loss: 0.7100 - accuracy: 0.8892 - val_loss: 1.0546 - val_accuracy: 0.5000\n",
      "Epoch 12/65\n",
      "3/3 [==============================] - 1s 352ms/step - loss: 0.6384 - accuracy: 0.9233 - val_loss: 1.0205 - val_accuracy: 0.5000\n",
      "Epoch 13/65\n",
      "3/3 [==============================] - 1s 336ms/step - loss: 0.6009 - accuracy: 0.9205 - val_loss: 0.9932 - val_accuracy: 0.5000\n",
      "Epoch 14/65\n",
      "3/3 [==============================] - 1s 331ms/step - loss: 0.5603 - accuracy: 0.9261 - val_loss: 0.9713 - val_accuracy: 0.5000\n",
      "Epoch 15/65\n",
      "3/3 [==============================] - 1s 326ms/step - loss: 0.5080 - accuracy: 0.9432 - val_loss: 0.9522 - val_accuracy: 0.5000\n",
      "Epoch 16/65\n",
      "3/3 [==============================] - 1s 340ms/step - loss: 0.4745 - accuracy: 0.9602 - val_loss: 0.9361 - val_accuracy: 0.5000\n",
      "Epoch 17/65\n",
      "3/3 [==============================] - 1s 326ms/step - loss: 0.4463 - accuracy: 0.9659 - val_loss: 0.9210 - val_accuracy: 0.5000\n",
      "Epoch 18/65\n",
      "3/3 [==============================] - 1s 340ms/step - loss: 0.4236 - accuracy: 0.9574 - val_loss: 0.9071 - val_accuracy: 0.5000\n",
      "Epoch 19/65\n",
      "3/3 [==============================] - 1s 330ms/step - loss: 0.4031 - accuracy: 0.9631 - val_loss: 0.8957 - val_accuracy: 0.5000\n",
      "Epoch 20/65\n",
      "3/3 [==============================] - 1s 330ms/step - loss: 0.3669 - accuracy: 0.9744 - val_loss: 0.8851 - val_accuracy: 0.5000\n",
      "Epoch 21/65\n",
      "3/3 [==============================] - 1s 325ms/step - loss: 0.3556 - accuracy: 0.9744 - val_loss: 0.8751 - val_accuracy: 0.5000\n",
      "Epoch 22/65\n",
      "3/3 [==============================] - 1s 325ms/step - loss: 0.3311 - accuracy: 0.9801 - val_loss: 0.8648 - val_accuracy: 0.5000\n",
      "Epoch 23/65\n",
      "3/3 [==============================] - 1s 345ms/step - loss: 0.3180 - accuracy: 0.9858 - val_loss: 0.8549 - val_accuracy: 0.5000\n",
      "Epoch 24/65\n",
      "3/3 [==============================] - 1s 330ms/step - loss: 0.3052 - accuracy: 0.9830 - val_loss: 0.8465 - val_accuracy: 0.5227\n",
      "Epoch 25/65\n",
      "3/3 [==============================] - 1s 343ms/step - loss: 0.2872 - accuracy: 0.9830 - val_loss: 0.8380 - val_accuracy: 0.5227\n",
      "Epoch 26/65\n",
      "3/3 [==============================] - 1s 332ms/step - loss: 0.2778 - accuracy: 0.9858 - val_loss: 0.8305 - val_accuracy: 0.5227\n",
      "Epoch 27/65\n",
      "3/3 [==============================] - 1s 327ms/step - loss: 0.2652 - accuracy: 0.9773 - val_loss: 0.8236 - val_accuracy: 0.5227\n",
      "Epoch 28/65\n",
      "3/3 [==============================] - 1s 327ms/step - loss: 0.2597 - accuracy: 0.9830 - val_loss: 0.8177 - val_accuracy: 0.5227\n",
      "Epoch 29/65\n",
      "3/3 [==============================] - 1s 326ms/step - loss: 0.2444 - accuracy: 0.9886 - val_loss: 0.8127 - val_accuracy: 0.5227\n",
      "Epoch 30/65\n",
      "3/3 [==============================] - 1s 327ms/step - loss: 0.2312 - accuracy: 0.9830 - val_loss: 0.8062 - val_accuracy: 0.5227\n",
      "Epoch 31/65\n",
      "3/3 [==============================] - 1s 327ms/step - loss: 0.2162 - accuracy: 0.9915 - val_loss: 0.8011 - val_accuracy: 0.5341\n",
      "Epoch 32/65\n",
      "3/3 [==============================] - 1s 325ms/step - loss: 0.2118 - accuracy: 0.9886 - val_loss: 0.7963 - val_accuracy: 0.5227\n",
      "Epoch 33/65\n",
      "3/3 [==============================] - 1s 326ms/step - loss: 0.2020 - accuracy: 0.9886 - val_loss: 0.7918 - val_accuracy: 0.5114\n",
      "Epoch 34/65\n",
      "3/3 [==============================] - 1s 329ms/step - loss: 0.1971 - accuracy: 0.9886 - val_loss: 0.7865 - val_accuracy: 0.5341\n",
      "Epoch 35/65\n",
      "3/3 [==============================] - 1s 335ms/step - loss: 0.1884 - accuracy: 0.9858 - val_loss: 0.7818 - val_accuracy: 0.5568\n",
      "Epoch 36/65\n",
      "3/3 [==============================] - 1s 330ms/step - loss: 0.1828 - accuracy: 0.9830 - val_loss: 0.7788 - val_accuracy: 0.5227\n",
      "Epoch 37/65\n",
      "3/3 [==============================] - 1s 327ms/step - loss: 0.1767 - accuracy: 0.9915 - val_loss: 0.7743 - val_accuracy: 0.5682\n",
      "Epoch 38/65\n",
      "3/3 [==============================] - 1s 342ms/step - loss: 0.1669 - accuracy: 0.9886 - val_loss: 0.7715 - val_accuracy: 0.5455\n",
      "Epoch 39/65\n",
      "3/3 [==============================] - 1s 342ms/step - loss: 0.1596 - accuracy: 0.9915 - val_loss: 0.7670 - val_accuracy: 0.5682\n",
      "Epoch 40/65\n",
      "3/3 [==============================] - 1s 333ms/step - loss: 0.1509 - accuracy: 0.9972 - val_loss: 0.7630 - val_accuracy: 0.5795\n",
      "Epoch 41/65\n",
      "3/3 [==============================] - 1s 342ms/step - loss: 0.1526 - accuracy: 0.9943 - val_loss: 0.7594 - val_accuracy: 0.5795\n",
      "Epoch 42/65\n",
      "3/3 [==============================] - 1s 327ms/step - loss: 0.1443 - accuracy: 0.9943 - val_loss: 0.7567 - val_accuracy: 0.5682\n",
      "Epoch 43/65\n",
      "3/3 [==============================] - 1s 347ms/step - loss: 0.1399 - accuracy: 0.9886 - val_loss: 0.7534 - val_accuracy: 0.5682\n",
      "Epoch 44/65\n",
      "3/3 [==============================] - 1s 328ms/step - loss: 0.1311 - accuracy: 1.0000 - val_loss: 0.7506 - val_accuracy: 0.5682\n",
      "Epoch 45/65\n",
      "3/3 [==============================] - 1s 328ms/step - loss: 0.1252 - accuracy: 0.9972 - val_loss: 0.7470 - val_accuracy: 0.5682\n",
      "Epoch 46/65\n",
      "3/3 [==============================] - 1s 377ms/step - loss: 0.1199 - accuracy: 1.0000 - val_loss: 0.7443 - val_accuracy: 0.5795\n",
      "Epoch 47/65\n",
      "3/3 [==============================] - 1s 329ms/step - loss: 0.1205 - accuracy: 1.0000 - val_loss: 0.7406 - val_accuracy: 0.6023\n",
      "Epoch 48/65\n",
      "3/3 [==============================] - 1s 322ms/step - loss: 0.1145 - accuracy: 0.9943 - val_loss: 0.7391 - val_accuracy: 0.5795\n",
      "Epoch 49/65\n",
      "3/3 [==============================] - 1s 346ms/step - loss: 0.1108 - accuracy: 0.9972 - val_loss: 0.7350 - val_accuracy: 0.6136\n",
      "Epoch 50/65\n",
      "3/3 [==============================] - 1s 328ms/step - loss: 0.1077 - accuracy: 0.9943 - val_loss: 0.7328 - val_accuracy: 0.6136\n",
      "Epoch 51/65\n",
      "3/3 [==============================] - 1s 362ms/step - loss: 0.1056 - accuracy: 1.0000 - val_loss: 0.7303 - val_accuracy: 0.6136\n",
      "Epoch 52/65\n",
      "3/3 [==============================] - 1s 358ms/step - loss: 0.0993 - accuracy: 1.0000 - val_loss: 0.7271 - val_accuracy: 0.6136\n",
      "Epoch 53/65\n",
      "3/3 [==============================] - 1s 357ms/step - loss: 0.0951 - accuracy: 1.0000 - val_loss: 0.7248 - val_accuracy: 0.6136\n",
      "Epoch 54/65\n",
      "3/3 [==============================] - 1s 412ms/step - loss: 0.0918 - accuracy: 1.0000 - val_loss: 0.7209 - val_accuracy: 0.6136\n",
      "Epoch 55/65\n",
      "3/3 [==============================] - 1s 341ms/step - loss: 0.0878 - accuracy: 0.9972 - val_loss: 0.7188 - val_accuracy: 0.6136\n",
      "Epoch 56/65\n",
      "3/3 [==============================] - 1s 353ms/step - loss: 0.0847 - accuracy: 0.9972 - val_loss: 0.7153 - val_accuracy: 0.6136\n",
      "Epoch 57/65\n",
      "3/3 [==============================] - 1s 332ms/step - loss: 0.0859 - accuracy: 1.0000 - val_loss: 0.7152 - val_accuracy: 0.6136\n",
      "Epoch 58/65\n",
      "3/3 [==============================] - 1s 329ms/step - loss: 0.0789 - accuracy: 0.9972 - val_loss: 0.7129 - val_accuracy: 0.6136\n",
      "Epoch 59/65\n",
      "3/3 [==============================] - 1s 347ms/step - loss: 0.0775 - accuracy: 1.0000 - val_loss: 0.7115 - val_accuracy: 0.6136\n",
      "Epoch 60/65\n",
      "3/3 [==============================] - 1s 335ms/step - loss: 0.0772 - accuracy: 0.9972 - val_loss: 0.7046 - val_accuracy: 0.6136\n",
      "Epoch 61/65\n",
      "3/3 [==============================] - 1s 329ms/step - loss: 0.0761 - accuracy: 0.9943 - val_loss: 0.7027 - val_accuracy: 0.6136\n",
      "Epoch 62/65\n",
      "3/3 [==============================] - 1s 350ms/step - loss: 0.0702 - accuracy: 1.0000 - val_loss: 0.6992 - val_accuracy: 0.6136\n",
      "Epoch 63/65\n",
      "3/3 [==============================] - 1s 347ms/step - loss: 0.0709 - accuracy: 1.0000 - val_loss: 0.6967 - val_accuracy: 0.6136\n",
      "Epoch 64/65\n",
      "3/3 [==============================] - 1s 357ms/step - loss: 0.0649 - accuracy: 1.0000 - val_loss: 0.6938 - val_accuracy: 0.6591\n",
      "Epoch 65/65\n",
      "3/3 [==============================] - 1s 332ms/step - loss: 0.0663 - accuracy: 1.0000 - val_loss: 0.6938 - val_accuracy: 0.6364\n",
      "3/3 [==============================] - 0s 41ms/step\n",
      "auc_roc: 0.6973140495867769\n",
      "auc_pr: 0.6931659060934845\n",
      "f1_score: 0.6204851752021563\n",
      "mcc: 0.29888128883203074\n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "[[0.3851658  0.61483425]\n",
      " [0.77900326 0.22099674]\n",
      " [0.39283687 0.60716313]\n",
      " [0.16679333 0.83320665]\n",
      " [0.4123672  0.5876328 ]\n",
      " [0.2239238  0.77607626]\n",
      " [0.26993072 0.73006934]\n",
      " [0.4816205  0.51837957]\n",
      " [0.16436394 0.8356361 ]\n",
      " [0.29206458 0.70793545]\n",
      " [0.6061116  0.3938884 ]\n",
      " [0.3552534  0.64474654]\n",
      " [0.05030639 0.9496937 ]\n",
      " [0.628783   0.37121704]\n",
      " [0.05582562 0.9441744 ]\n",
      " [0.3776741  0.62232596]\n",
      " [0.12791958 0.87208045]\n",
      " [0.4499337  0.5500663 ]\n",
      " [0.7726788  0.22732121]\n",
      " [0.6660825  0.3339175 ]\n",
      " [0.3410029  0.65899706]\n",
      " [0.06437165 0.93562835]\n",
      " [0.36240342 0.63759655]\n",
      " [0.4282462  0.57175386]\n",
      " [0.46228173 0.53771836]\n",
      " [0.46459937 0.5354006 ]\n",
      " [0.36893278 0.6310673 ]\n",
      " [0.676345   0.32365507]\n",
      " [0.37252483 0.6274752 ]\n",
      " [0.4044729  0.5955271 ]\n",
      " [0.37553173 0.62446827]\n",
      " [0.3801553  0.6198447 ]\n",
      " [0.17510594 0.824894  ]\n",
      " [0.59135705 0.40864298]\n",
      " [0.43080658 0.56919336]\n",
      " [0.5085827  0.49141726]\n",
      " [0.13562247 0.86437756]\n",
      " [0.5103015  0.48969853]\n",
      " [0.21361731 0.7863827 ]\n",
      " [0.25892213 0.7410779 ]\n",
      " [0.57803994 0.4219601 ]\n",
      " [0.12634732 0.8736527 ]\n",
      " [0.54922324 0.45077682]\n",
      " [0.16931994 0.8306801 ]\n",
      " [0.44163764 0.55836236]\n",
      " [0.608384   0.391616  ]\n",
      " [0.25681084 0.74318916]\n",
      " [0.52765423 0.47234583]\n",
      " [0.5035213  0.49647865]\n",
      " [0.37457168 0.6254284 ]\n",
      " [0.35836136 0.6416387 ]\n",
      " [0.3916999  0.6083001 ]\n",
      " [0.33089995 0.6691001 ]\n",
      " [0.47252375 0.52747625]\n",
      " [0.32939273 0.6706073 ]\n",
      " [0.3875782  0.6124219 ]\n",
      " [0.4999612  0.50003886]\n",
      " [0.23373462 0.76626545]\n",
      " [0.45995432 0.54004574]\n",
      " [0.06980085 0.9301992 ]\n",
      " [0.10196058 0.89803946]\n",
      " [0.6456496  0.35435033]\n",
      " [0.65657264 0.34342736]\n",
      " [0.49726668 0.50273335]\n",
      " [0.43542758 0.56457245]\n",
      " [0.5474277  0.4525723 ]\n",
      " [0.60934055 0.39065945]\n",
      " [0.08990831 0.9100917 ]\n",
      " [0.211965   0.788035  ]\n",
      " [0.6175153  0.38248476]\n",
      " [0.3000121  0.6999879 ]\n",
      " [0.53622156 0.46377847]\n",
      " [0.4686483  0.53135175]\n",
      " [0.61689025 0.38310975]\n",
      " [0.31532687 0.6846732 ]\n",
      " [0.29136407 0.7086359 ]\n",
      " [0.38134056 0.6186595 ]\n",
      " [0.38821244 0.6117876 ]\n",
      " [0.28465894 0.7153411 ]\n",
      " [0.13097289 0.8690271 ]\n",
      " [0.13628063 0.86371934]\n",
      " [0.7155884  0.28441158]\n",
      " [0.6377469  0.36225316]\n",
      " [0.14826621 0.85173374]\n",
      " [0.610282   0.38971806]\n",
      " [0.48228505 0.5177149 ]\n",
      " [0.65410894 0.3458911 ]\n",
      " [0.61448854 0.3855115 ]]\n",
      "Model: \"res\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 16, 396, 1)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " res2a-conv1 (Conv2D)           (None, 16, 396, 64)  576         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " res2a-batchnorm1 (BatchNormali  (None, 16, 396, 64)  256        ['res2a-conv1[0][0]']            \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 16, 396, 64)  0           ['res2a-batchnorm1[0][0]']       \n",
      "                                                                                                  \n",
      " res2a-conv2 (Conv2D)           (None, 16, 396, 64)  32832       ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " res2a-batchnorm2 (BatchNormali  (None, 16, 396, 64)  256        ['res2a-conv2[0][0]']            \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 16, 396, 64)  0           ['res2a-batchnorm2[0][0]',       \n",
      "                                                                  'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 16, 396, 64)  0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " res2b-conv1 (Conv2D)           (None, 16, 396, 64)  32832       ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " res2b-batchnorm1 (BatchNormali  (None, 16, 396, 64)  256        ['res2b-conv1[0][0]']            \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 16, 396, 64)  0           ['res2b-batchnorm1[0][0]']       \n",
      "                                                                                                  \n",
      " res2b-conv2 (Conv2D)           (None, 16, 396, 64)  32832       ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " res2b-batchnorm2 (BatchNormali  (None, 16, 396, 64)  256        ['res2b-conv2[0][0]']            \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 16, 396, 64)  0           ['res2b-batchnorm2[0][0]',       \n",
      "                                                                  'activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 16, 396, 64)  0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2 (Conv2D)                 (None, 16, 396, 1)   65          ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 6336)         0           ['conv2[0][0]']                  \n",
      "                                                                                                  \n",
      " fc (Dense)                     (None, 100)          633700      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 100)          0           ['fc[0][0]']                     \n",
      "                                                                                                  \n",
      " softmax (Dense)                (None, 2)            202         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 734,063\n",
      "Trainable params: 733,551\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/65\n",
      "3/3 [==============================] - 2s 635ms/step - loss: 2.7968 - accuracy: 0.5256 - val_loss: 2.4225 - val_accuracy: 0.5000\n",
      "Epoch 2/65\n",
      "3/3 [==============================] - 1s 403ms/step - loss: 2.3656 - accuracy: 0.6250 - val_loss: 2.1761 - val_accuracy: 0.5000\n",
      "Epoch 3/65\n",
      "3/3 [==============================] - 1s 391ms/step - loss: 2.0156 - accuracy: 0.6761 - val_loss: 1.9703 - val_accuracy: 0.5000\n",
      "Epoch 4/65\n",
      "3/3 [==============================] - 1s 355ms/step - loss: 1.7345 - accuracy: 0.7557 - val_loss: 1.7833 - val_accuracy: 0.5000\n",
      "Epoch 5/65\n",
      "3/3 [==============================] - 1s 389ms/step - loss: 1.5207 - accuracy: 0.7955 - val_loss: 1.6219 - val_accuracy: 0.5000\n",
      "Epoch 6/65\n",
      "3/3 [==============================] - 1s 407ms/step - loss: 1.3139 - accuracy: 0.8267 - val_loss: 1.4853 - val_accuracy: 0.5000\n",
      "Epoch 7/65\n",
      "3/3 [==============================] - 1s 345ms/step - loss: 1.1675 - accuracy: 0.8295 - val_loss: 1.3721 - val_accuracy: 0.5000\n",
      "Epoch 8/65\n",
      "3/3 [==============================] - 1s 343ms/step - loss: 1.0065 - accuracy: 0.8778 - val_loss: 1.2764 - val_accuracy: 0.5000\n",
      "Epoch 9/65\n",
      "3/3 [==============================] - 1s 350ms/step - loss: 0.8911 - accuracy: 0.8949 - val_loss: 1.1999 - val_accuracy: 0.5000\n",
      "Epoch 10/65\n",
      "3/3 [==============================] - 1s 348ms/step - loss: 0.7888 - accuracy: 0.9148 - val_loss: 1.1396 - val_accuracy: 0.5000\n",
      "Epoch 11/65\n",
      "3/3 [==============================] - 1s 329ms/step - loss: 0.7056 - accuracy: 0.9375 - val_loss: 1.0868 - val_accuracy: 0.5000\n",
      "Epoch 12/65\n",
      "3/3 [==============================] - 1s 339ms/step - loss: 0.6504 - accuracy: 0.9347 - val_loss: 1.0491 - val_accuracy: 0.5000\n",
      "Epoch 13/65\n",
      "3/3 [==============================] - 1s 363ms/step - loss: 0.5971 - accuracy: 0.9375 - val_loss: 1.0199 - val_accuracy: 0.5000\n",
      "Epoch 14/65\n",
      "3/3 [==============================] - 1s 331ms/step - loss: 0.5438 - accuracy: 0.9602 - val_loss: 0.9925 - val_accuracy: 0.5114\n",
      "Epoch 15/65\n",
      "3/3 [==============================] - 1s 336ms/step - loss: 0.5042 - accuracy: 0.9688 - val_loss: 0.9729 - val_accuracy: 0.5114\n",
      "Epoch 16/65\n",
      "3/3 [==============================] - 1s 330ms/step - loss: 0.4715 - accuracy: 0.9744 - val_loss: 0.9562 - val_accuracy: 0.5000\n",
      "Epoch 17/65\n",
      "3/3 [==============================] - 1s 363ms/step - loss: 0.4432 - accuracy: 0.9744 - val_loss: 0.9389 - val_accuracy: 0.5227\n",
      "Epoch 18/65\n",
      "3/3 [==============================] - 1s 347ms/step - loss: 0.4123 - accuracy: 0.9801 - val_loss: 0.9276 - val_accuracy: 0.5000\n",
      "Epoch 19/65\n",
      "3/3 [==============================] - 1s 329ms/step - loss: 0.3870 - accuracy: 0.9830 - val_loss: 0.9139 - val_accuracy: 0.5114\n",
      "Epoch 20/65\n",
      "3/3 [==============================] - 1s 358ms/step - loss: 0.3633 - accuracy: 0.9744 - val_loss: 0.9014 - val_accuracy: 0.5227\n",
      "Epoch 21/65\n",
      "3/3 [==============================] - 1s 343ms/step - loss: 0.3430 - accuracy: 0.9801 - val_loss: 0.8941 - val_accuracy: 0.5000\n",
      "Epoch 22/65\n",
      "3/3 [==============================] - 1s 332ms/step - loss: 0.3192 - accuracy: 0.9943 - val_loss: 0.8835 - val_accuracy: 0.5000\n",
      "Epoch 23/65\n",
      "3/3 [==============================] - 1s 349ms/step - loss: 0.3071 - accuracy: 0.9915 - val_loss: 0.8747 - val_accuracy: 0.5000\n",
      "Epoch 24/65\n",
      "3/3 [==============================] - 1s 349ms/step - loss: 0.2901 - accuracy: 0.9886 - val_loss: 0.8656 - val_accuracy: 0.5000\n",
      "Epoch 25/65\n",
      "3/3 [==============================] - 1s 361ms/step - loss: 0.2730 - accuracy: 0.9943 - val_loss: 0.8559 - val_accuracy: 0.5114\n",
      "Epoch 26/65\n",
      "3/3 [==============================] - 1s 355ms/step - loss: 0.2666 - accuracy: 0.9915 - val_loss: 0.8494 - val_accuracy: 0.5000\n",
      "Epoch 27/65\n",
      "3/3 [==============================] - 1s 350ms/step - loss: 0.2540 - accuracy: 0.9915 - val_loss: 0.8411 - val_accuracy: 0.5114\n",
      "Epoch 28/65\n",
      "3/3 [==============================] - 1s 354ms/step - loss: 0.2387 - accuracy: 0.9943 - val_loss: 0.8346 - val_accuracy: 0.5114\n",
      "Epoch 29/65\n",
      "3/3 [==============================] - 1s 331ms/step - loss: 0.2270 - accuracy: 0.9886 - val_loss: 0.8286 - val_accuracy: 0.5000\n",
      "Epoch 30/65\n",
      "3/3 [==============================] - 1s 347ms/step - loss: 0.2087 - accuracy: 1.0000 - val_loss: 0.8214 - val_accuracy: 0.5227\n",
      "Epoch 31/65\n",
      "3/3 [==============================] - 1s 329ms/step - loss: 0.2058 - accuracy: 1.0000 - val_loss: 0.8154 - val_accuracy: 0.5227\n",
      "Epoch 32/65\n",
      "3/3 [==============================] - 1s 340ms/step - loss: 0.1963 - accuracy: 1.0000 - val_loss: 0.8100 - val_accuracy: 0.5227\n",
      "Epoch 33/65\n",
      "3/3 [==============================] - 1s 336ms/step - loss: 0.1885 - accuracy: 0.9972 - val_loss: 0.8050 - val_accuracy: 0.5227\n",
      "Epoch 34/65\n",
      "3/3 [==============================] - 1s 331ms/step - loss: 0.1809 - accuracy: 0.9972 - val_loss: 0.7990 - val_accuracy: 0.5227\n",
      "Epoch 35/65\n",
      "3/3 [==============================] - 1s 331ms/step - loss: 0.1733 - accuracy: 0.9943 - val_loss: 0.7935 - val_accuracy: 0.5227\n",
      "Epoch 36/65\n",
      "3/3 [==============================] - 1s 337ms/step - loss: 0.1632 - accuracy: 1.0000 - val_loss: 0.7895 - val_accuracy: 0.5227\n",
      "Epoch 37/65\n",
      "3/3 [==============================] - 1s 352ms/step - loss: 0.1560 - accuracy: 0.9972 - val_loss: 0.7850 - val_accuracy: 0.5227\n",
      "Epoch 38/65\n",
      "3/3 [==============================] - 1s 333ms/step - loss: 0.1527 - accuracy: 1.0000 - val_loss: 0.7801 - val_accuracy: 0.5114\n",
      "Epoch 39/65\n",
      "3/3 [==============================] - 1s 337ms/step - loss: 0.1440 - accuracy: 0.9972 - val_loss: 0.7763 - val_accuracy: 0.5114\n",
      "Epoch 40/65\n",
      "3/3 [==============================] - 1s 355ms/step - loss: 0.1388 - accuracy: 1.0000 - val_loss: 0.7725 - val_accuracy: 0.5114\n",
      "Epoch 41/65\n",
      "3/3 [==============================] - 1s 331ms/step - loss: 0.1336 - accuracy: 1.0000 - val_loss: 0.7675 - val_accuracy: 0.5114\n",
      "Epoch 42/65\n",
      "3/3 [==============================] - 1s 333ms/step - loss: 0.1321 - accuracy: 1.0000 - val_loss: 0.7654 - val_accuracy: 0.5000\n",
      "Epoch 43/65\n",
      "3/3 [==============================] - 1s 341ms/step - loss: 0.1261 - accuracy: 0.9972 - val_loss: 0.7627 - val_accuracy: 0.5341\n",
      "Epoch 44/65\n",
      "3/3 [==============================] - 1s 331ms/step - loss: 0.1199 - accuracy: 1.0000 - val_loss: 0.7593 - val_accuracy: 0.5341\n",
      "Epoch 45/65\n",
      "3/3 [==============================] - 1s 352ms/step - loss: 0.1148 - accuracy: 1.0000 - val_loss: 0.7567 - val_accuracy: 0.5341\n",
      "Epoch 46/65\n",
      "3/3 [==============================] - 1s 337ms/step - loss: 0.1091 - accuracy: 1.0000 - val_loss: 0.7543 - val_accuracy: 0.5341\n",
      "Epoch 47/65\n",
      "3/3 [==============================] - 1s 339ms/step - loss: 0.1082 - accuracy: 1.0000 - val_loss: 0.7517 - val_accuracy: 0.5341\n",
      "Epoch 48/65\n",
      "3/3 [==============================] - 1s 331ms/step - loss: 0.1046 - accuracy: 1.0000 - val_loss: 0.7489 - val_accuracy: 0.5341\n",
      "Epoch 49/65\n",
      "3/3 [==============================] - 1s 328ms/step - loss: 0.0978 - accuracy: 1.0000 - val_loss: 0.7435 - val_accuracy: 0.5341\n",
      "Epoch 50/65\n",
      "3/3 [==============================] - 1s 333ms/step - loss: 0.0972 - accuracy: 1.0000 - val_loss: 0.7408 - val_accuracy: 0.5227\n",
      "Epoch 51/65\n",
      "3/3 [==============================] - 1s 341ms/step - loss: 0.0920 - accuracy: 1.0000 - val_loss: 0.7386 - val_accuracy: 0.5341\n",
      "Epoch 52/65\n",
      "3/3 [==============================] - 1s 347ms/step - loss: 0.0886 - accuracy: 1.0000 - val_loss: 0.7372 - val_accuracy: 0.5227\n",
      "Epoch 53/65\n",
      "3/3 [==============================] - 1s 331ms/step - loss: 0.0883 - accuracy: 1.0000 - val_loss: 0.7342 - val_accuracy: 0.5341\n",
      "Epoch 54/65\n",
      "3/3 [==============================] - 1s 343ms/step - loss: 0.0820 - accuracy: 1.0000 - val_loss: 0.7306 - val_accuracy: 0.5568\n",
      "Epoch 55/65\n",
      "3/3 [==============================] - 1s 349ms/step - loss: 0.0794 - accuracy: 1.0000 - val_loss: 0.7288 - val_accuracy: 0.5682\n",
      "Epoch 56/65\n",
      "3/3 [==============================] - 1s 345ms/step - loss: 0.0773 - accuracy: 1.0000 - val_loss: 0.7270 - val_accuracy: 0.5568\n",
      "Epoch 57/65\n",
      "3/3 [==============================] - 1s 342ms/step - loss: 0.0750 - accuracy: 1.0000 - val_loss: 0.7239 - val_accuracy: 0.5568\n",
      "Epoch 58/65\n",
      "3/3 [==============================] - 1s 342ms/step - loss: 0.0723 - accuracy: 1.0000 - val_loss: 0.7217 - val_accuracy: 0.5568\n",
      "Epoch 59/65\n",
      "3/3 [==============================] - 1s 346ms/step - loss: 0.0688 - accuracy: 1.0000 - val_loss: 0.7201 - val_accuracy: 0.5568\n",
      "Epoch 60/65\n",
      "3/3 [==============================] - 1s 336ms/step - loss: 0.0679 - accuracy: 1.0000 - val_loss: 0.7158 - val_accuracy: 0.6023\n",
      "Epoch 61/65\n",
      "3/3 [==============================] - 1s 338ms/step - loss: 0.0659 - accuracy: 1.0000 - val_loss: 0.7155 - val_accuracy: 0.5909\n",
      "Epoch 62/65\n",
      "3/3 [==============================] - 1s 372ms/step - loss: 0.0639 - accuracy: 1.0000 - val_loss: 0.7110 - val_accuracy: 0.6023\n",
      "Epoch 63/65\n",
      "3/3 [==============================] - 1s 332ms/step - loss: 0.0635 - accuracy: 0.9972 - val_loss: 0.7155 - val_accuracy: 0.5682\n",
      "Epoch 64/65\n",
      "3/3 [==============================] - 1s 334ms/step - loss: 0.0687 - accuracy: 1.0000 - val_loss: 0.7042 - val_accuracy: 0.6477\n",
      "Epoch 65/65\n",
      "3/3 [==============================] - 1s 333ms/step - loss: 0.0888 - accuracy: 0.9972 - val_loss: 0.7031 - val_accuracy: 0.6364\n",
      "3/3 [==============================] - 0s 40ms/step\n",
      "auc_roc: 0.6482438016528926\n",
      "auc_pr: 0.6487002010515954\n",
      "f1_score: 0.631606488749346\n",
      "mcc: 0.28005601680560194\n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "[[0.4738706  0.5261294 ]\n",
      " [0.5941269  0.40587318]\n",
      " [0.23260552 0.7673944 ]\n",
      " [0.53574514 0.46425495]\n",
      " [0.70031744 0.2996826 ]\n",
      " [0.36100686 0.63899314]\n",
      " [0.6428526  0.35714746]\n",
      " [0.3830364  0.6169636 ]\n",
      " [0.4179142  0.5820858 ]\n",
      " [0.15975638 0.8402436 ]\n",
      " [0.6582284  0.34177163]\n",
      " [0.42640504 0.57359505]\n",
      " [0.7306204  0.2693797 ]\n",
      " [0.4460575  0.55394256]\n",
      " [0.53333384 0.46666622]\n",
      " [0.60707456 0.39292553]\n",
      " [0.52048194 0.47951815]\n",
      " [0.58834165 0.41165838]\n",
      " [0.42100593 0.5789941 ]\n",
      " [0.34740046 0.65259963]\n",
      " [0.6321046  0.36789542]\n",
      " [0.2655123  0.7344878 ]\n",
      " [0.46483618 0.5351639 ]\n",
      " [0.5423847  0.45761538]\n",
      " [0.6633996  0.33660048]\n",
      " [0.44465935 0.5553407 ]\n",
      " [0.07033984 0.92966014]\n",
      " [0.3376712  0.66232884]\n",
      " [0.4850797  0.5149204 ]\n",
      " [0.5834459  0.41655403]\n",
      " [0.2914879  0.70851207]\n",
      " [0.2913807  0.70861936]\n",
      " [0.5065937  0.49340636]\n",
      " [0.2595293  0.74047077]\n",
      " [0.3587018  0.64129823]\n",
      " [0.08875779 0.91124225]\n",
      " [0.26942456 0.7305755 ]\n",
      " [0.24822918 0.75177085]\n",
      " [0.6003222  0.3996778 ]\n",
      " [0.39400536 0.60599464]\n",
      " [0.4578005  0.5421995 ]\n",
      " [0.23166288 0.76833713]\n",
      " [0.4436424  0.5563576 ]\n",
      " [0.27664214 0.72335786]\n",
      " [0.02381275 0.9761872 ]\n",
      " [0.22366212 0.7763379 ]\n",
      " [0.37643808 0.623562  ]\n",
      " [0.5969169  0.40308312]\n",
      " [0.560356   0.439644  ]\n",
      " [0.31299064 0.6870094 ]\n",
      " [0.5233028  0.4766973 ]\n",
      " [0.3675392  0.6324608 ]\n",
      " [0.44318002 0.55682003]\n",
      " [0.6085242  0.3914758 ]\n",
      " [0.16976456 0.8302354 ]\n",
      " [0.3669428  0.63305724]\n",
      " [0.35491595 0.6450841 ]\n",
      " [0.18760726 0.8123927 ]\n",
      " [0.61162597 0.38837406]\n",
      " [0.46871337 0.5312866 ]\n",
      " [0.7783864  0.22161362]\n",
      " [0.2817762  0.71822375]\n",
      " [0.57172805 0.42827195]\n",
      " [0.4931134  0.5068866 ]\n",
      " [0.23358464 0.7664154 ]\n",
      " [0.7419799  0.25802013]\n",
      " [0.7146881  0.28531194]\n",
      " [0.30077487 0.6992251 ]\n",
      " [0.36646155 0.6335385 ]\n",
      " [0.40474936 0.5952506 ]\n",
      " [0.5767919  0.4232082 ]\n",
      " [0.650526   0.34947398]\n",
      " [0.56319124 0.43680874]\n",
      " [0.33776274 0.6622372 ]\n",
      " [0.25797418 0.74202585]\n",
      " [0.46910983 0.5308902 ]\n",
      " [0.4689358  0.5310643 ]\n",
      " [0.5132636  0.48673642]\n",
      " [0.51882905 0.48117098]\n",
      " [0.24037887 0.75962114]\n",
      " [0.38146642 0.6185336 ]\n",
      " [0.57928145 0.4207186 ]\n",
      " [0.51327366 0.4867263 ]\n",
      " [0.29400507 0.70599496]\n",
      " [0.4961181  0.5038819 ]\n",
      " [0.425593   0.57440704]\n",
      " [0.69195724 0.30804276]\n",
      " [0.55799466 0.44200534]]\n",
      "Model: \"res\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 16, 396, 1)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " res2a-conv1 (Conv2D)           (None, 16, 396, 64)  576         ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " res2a-batchnorm1 (BatchNormali  (None, 16, 396, 64)  256        ['res2a-conv1[0][0]']            \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 16, 396, 64)  0           ['res2a-batchnorm1[0][0]']       \n",
      "                                                                                                  \n",
      " res2a-conv2 (Conv2D)           (None, 16, 396, 64)  32832       ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " res2a-batchnorm2 (BatchNormali  (None, 16, 396, 64)  256        ['res2a-conv2[0][0]']            \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 16, 396, 64)  0           ['res2a-batchnorm2[0][0]',       \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 16, 396, 64)  0           ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " res2b-conv1 (Conv2D)           (None, 16, 396, 64)  32832       ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " res2b-batchnorm1 (BatchNormali  (None, 16, 396, 64)  256        ['res2b-conv1[0][0]']            \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 16, 396, 64)  0           ['res2b-batchnorm1[0][0]']       \n",
      "                                                                                                  \n",
      " res2b-conv2 (Conv2D)           (None, 16, 396, 64)  32832       ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " res2b-batchnorm2 (BatchNormali  (None, 16, 396, 64)  256        ['res2b-conv2[0][0]']            \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 16, 396, 64)  0           ['res2b-batchnorm2[0][0]',       \n",
      "                                                                  'activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 16, 396, 64)  0           ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2 (Conv2D)                 (None, 16, 396, 1)   65          ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 6336)         0           ['conv2[0][0]']                  \n",
      "                                                                                                  \n",
      " fc (Dense)                     (None, 100)          633700      ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 100)          0           ['fc[0][0]']                     \n",
      "                                                                                                  \n",
      " softmax (Dense)                (None, 2)            202         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 734,063\n",
      "Trainable params: 733,551\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/65\n",
      "3/3 [==============================] - 3s 803ms/step - loss: 2.7960 - accuracy: 0.5170 - val_loss: 2.4245 - val_accuracy: 0.5114\n",
      "Epoch 2/65\n",
      "3/3 [==============================] - 1s 356ms/step - loss: 2.3743 - accuracy: 0.6080 - val_loss: 2.1952 - val_accuracy: 0.5114\n",
      "Epoch 3/65\n",
      "3/3 [==============================] - 1s 370ms/step - loss: 1.9861 - accuracy: 0.7415 - val_loss: 2.0015 - val_accuracy: 0.5114\n",
      "Epoch 4/65\n",
      "3/3 [==============================] - 1s 386ms/step - loss: 1.7456 - accuracy: 0.7812 - val_loss: 1.8204 - val_accuracy: 0.5114\n",
      "Epoch 5/65\n",
      "3/3 [==============================] - 1s 370ms/step - loss: 1.5245 - accuracy: 0.8153 - val_loss: 1.6619 - val_accuracy: 0.5114\n",
      "Epoch 6/65\n",
      "3/3 [==============================] - 1s 369ms/step - loss: 1.3205 - accuracy: 0.8381 - val_loss: 1.5324 - val_accuracy: 0.5114\n",
      "Epoch 7/65\n",
      "3/3 [==============================] - 1s 342ms/step - loss: 1.1448 - accuracy: 0.8523 - val_loss: 1.4088 - val_accuracy: 0.5114\n",
      "Epoch 8/65\n",
      "3/3 [==============================] - 1s 353ms/step - loss: 1.0029 - accuracy: 0.8949 - val_loss: 1.3124 - val_accuracy: 0.5114\n",
      "Epoch 9/65\n",
      "3/3 [==============================] - 1s 350ms/step - loss: 0.8862 - accuracy: 0.9176 - val_loss: 1.2316 - val_accuracy: 0.5114\n",
      "Epoch 10/65\n",
      "3/3 [==============================] - 1s 403ms/step - loss: 0.7766 - accuracy: 0.9460 - val_loss: 1.1706 - val_accuracy: 0.5114\n",
      "Epoch 11/65\n",
      "3/3 [==============================] - 1s 345ms/step - loss: 0.7016 - accuracy: 0.9432 - val_loss: 1.1097 - val_accuracy: 0.5114\n",
      "Epoch 12/65\n",
      "3/3 [==============================] - 1s 366ms/step - loss: 0.6316 - accuracy: 0.9517 - val_loss: 1.0649 - val_accuracy: 0.5114\n",
      "Epoch 13/65\n",
      "3/3 [==============================] - 1s 351ms/step - loss: 0.5801 - accuracy: 0.9517 - val_loss: 1.0330 - val_accuracy: 0.5114\n",
      "Epoch 14/65\n",
      "3/3 [==============================] - 1s 348ms/step - loss: 0.5267 - accuracy: 0.9631 - val_loss: 1.0013 - val_accuracy: 0.5000\n",
      "Epoch 15/65\n",
      "3/3 [==============================] - 1s 330ms/step - loss: 0.4935 - accuracy: 0.9716 - val_loss: 0.9780 - val_accuracy: 0.5114\n",
      "Epoch 16/65\n",
      "3/3 [==============================] - 1s 335ms/step - loss: 0.4484 - accuracy: 0.9773 - val_loss: 0.9587 - val_accuracy: 0.5114\n",
      "Epoch 17/65\n",
      "3/3 [==============================] - 1s 346ms/step - loss: 0.4209 - accuracy: 0.9801 - val_loss: 0.9401 - val_accuracy: 0.5114\n",
      "Epoch 18/65\n",
      "3/3 [==============================] - 1s 344ms/step - loss: 0.3920 - accuracy: 0.9801 - val_loss: 0.9257 - val_accuracy: 0.5114\n",
      "Epoch 19/65\n",
      "3/3 [==============================] - 1s 343ms/step - loss: 0.3633 - accuracy: 0.9830 - val_loss: 0.9123 - val_accuracy: 0.5114\n",
      "Epoch 20/65\n",
      "3/3 [==============================] - 1s 330ms/step - loss: 0.3408 - accuracy: 0.9886 - val_loss: 0.8997 - val_accuracy: 0.5114\n",
      "Epoch 21/65\n",
      "3/3 [==============================] - 1s 331ms/step - loss: 0.3230 - accuracy: 0.9801 - val_loss: 0.8890 - val_accuracy: 0.5000\n",
      "Epoch 22/65\n",
      "3/3 [==============================] - 1s 327ms/step - loss: 0.3037 - accuracy: 0.9858 - val_loss: 0.8794 - val_accuracy: 0.5114\n",
      "Epoch 23/65\n",
      "3/3 [==============================] - 1s 330ms/step - loss: 0.2860 - accuracy: 0.9943 - val_loss: 0.8714 - val_accuracy: 0.5114\n",
      "Epoch 24/65\n",
      "3/3 [==============================] - 1s 334ms/step - loss: 0.2700 - accuracy: 0.9943 - val_loss: 0.8637 - val_accuracy: 0.5114\n",
      "Epoch 25/65\n",
      "3/3 [==============================] - 1s 341ms/step - loss: 0.2451 - accuracy: 1.0000 - val_loss: 0.8550 - val_accuracy: 0.5114\n",
      "Epoch 26/65\n",
      "3/3 [==============================] - 1s 326ms/step - loss: 0.2414 - accuracy: 0.9943 - val_loss: 0.8485 - val_accuracy: 0.5114\n",
      "Epoch 27/65\n",
      "3/3 [==============================] - 1s 343ms/step - loss: 0.2243 - accuracy: 0.9943 - val_loss: 0.8407 - val_accuracy: 0.5114\n",
      "Epoch 28/65\n",
      "3/3 [==============================] - 1s 381ms/step - loss: 0.2120 - accuracy: 0.9943 - val_loss: 0.8321 - val_accuracy: 0.5114\n",
      "Epoch 29/65\n",
      "3/3 [==============================] - 1s 368ms/step - loss: 0.2071 - accuracy: 0.9915 - val_loss: 0.8271 - val_accuracy: 0.5114\n",
      "Epoch 30/65\n",
      "3/3 [==============================] - 1s 331ms/step - loss: 0.1958 - accuracy: 0.9943 - val_loss: 0.8221 - val_accuracy: 0.5114\n",
      "Epoch 31/65\n",
      "3/3 [==============================] - 1s 331ms/step - loss: 0.1896 - accuracy: 0.9886 - val_loss: 0.8155 - val_accuracy: 0.4886\n",
      "Epoch 32/65\n",
      "3/3 [==============================] - 1s 328ms/step - loss: 0.1771 - accuracy: 1.0000 - val_loss: 0.8095 - val_accuracy: 0.5000\n",
      "Epoch 33/65\n",
      "3/3 [==============================] - 1s 330ms/step - loss: 0.1681 - accuracy: 1.0000 - val_loss: 0.8061 - val_accuracy: 0.5114\n",
      "Epoch 34/65\n",
      "3/3 [==============================] - 1s 327ms/step - loss: 0.1612 - accuracy: 0.9972 - val_loss: 0.8000 - val_accuracy: 0.5114\n",
      "Epoch 35/65\n",
      "3/3 [==============================] - 1s 329ms/step - loss: 0.1563 - accuracy: 0.9943 - val_loss: 0.7973 - val_accuracy: 0.5114\n",
      "Epoch 36/65\n",
      "3/3 [==============================] - 1s 327ms/step - loss: 0.1491 - accuracy: 0.9972 - val_loss: 0.7918 - val_accuracy: 0.5227\n",
      "Epoch 37/65\n",
      "3/3 [==============================] - 1s 329ms/step - loss: 0.1448 - accuracy: 0.9972 - val_loss: 0.7878 - val_accuracy: 0.5114\n",
      "Epoch 38/65\n",
      "3/3 [==============================] - 1s 327ms/step - loss: 0.1370 - accuracy: 0.9943 - val_loss: 0.7853 - val_accuracy: 0.5227\n",
      "Epoch 39/65\n",
      "3/3 [==============================] - 1s 327ms/step - loss: 0.1357 - accuracy: 0.9972 - val_loss: 0.7794 - val_accuracy: 0.5114\n",
      "Epoch 40/65\n",
      "3/3 [==============================] - 1s 325ms/step - loss: 0.1290 - accuracy: 0.9972 - val_loss: 0.7791 - val_accuracy: 0.5341\n",
      "Epoch 41/65\n",
      "3/3 [==============================] - 1s 330ms/step - loss: 0.1236 - accuracy: 0.9943 - val_loss: 0.7762 - val_accuracy: 0.5455\n",
      "Epoch 42/65\n",
      "3/3 [==============================] - 1s 359ms/step - loss: 0.1231 - accuracy: 1.0000 - val_loss: 0.7718 - val_accuracy: 0.5227\n",
      "Epoch 43/65\n",
      "3/3 [==============================] - 1s 330ms/step - loss: 0.1158 - accuracy: 1.0000 - val_loss: 0.7702 - val_accuracy: 0.5114\n",
      "Epoch 44/65\n",
      "3/3 [==============================] - 1s 351ms/step - loss: 0.1100 - accuracy: 1.0000 - val_loss: 0.7663 - val_accuracy: 0.5341\n",
      "Epoch 45/65\n",
      "3/3 [==============================] - 1s 348ms/step - loss: 0.1067 - accuracy: 1.0000 - val_loss: 0.7657 - val_accuracy: 0.5114\n",
      "Epoch 46/65\n",
      "3/3 [==============================] - 1s 331ms/step - loss: 0.1075 - accuracy: 0.9943 - val_loss: 0.7627 - val_accuracy: 0.5114\n",
      "Epoch 47/65\n",
      "3/3 [==============================] - 1s 329ms/step - loss: 0.1049 - accuracy: 0.9943 - val_loss: 0.7625 - val_accuracy: 0.5341\n",
      "Epoch 48/65\n",
      "3/3 [==============================] - 1s 358ms/step - loss: 0.0987 - accuracy: 1.0000 - val_loss: 0.7602 - val_accuracy: 0.5114\n",
      "Epoch 49/65\n",
      "3/3 [==============================] - 1s 345ms/step - loss: 0.0945 - accuracy: 0.9972 - val_loss: 0.7587 - val_accuracy: 0.5114\n",
      "Epoch 50/65\n",
      "3/3 [==============================] - 1s 332ms/step - loss: 0.0919 - accuracy: 0.9972 - val_loss: 0.7568 - val_accuracy: 0.5227\n",
      "Epoch 51/65\n",
      "3/3 [==============================] - 1s 346ms/step - loss: 0.0934 - accuracy: 0.9972 - val_loss: 0.7559 - val_accuracy: 0.5114\n",
      "Epoch 52/65\n",
      "3/3 [==============================] - 1s 335ms/step - loss: 0.0881 - accuracy: 0.9943 - val_loss: 0.7539 - val_accuracy: 0.5114\n",
      "Epoch 53/65\n",
      "3/3 [==============================] - 1s 357ms/step - loss: 0.0801 - accuracy: 1.0000 - val_loss: 0.7517 - val_accuracy: 0.5114\n",
      "Epoch 54/65\n",
      "3/3 [==============================] - 1s 359ms/step - loss: 0.0790 - accuracy: 1.0000 - val_loss: 0.7499 - val_accuracy: 0.5227\n",
      "Epoch 55/65\n",
      "3/3 [==============================] - 1s 338ms/step - loss: 0.0831 - accuracy: 1.0000 - val_loss: 0.7483 - val_accuracy: 0.5114\n",
      "Epoch 56/65\n",
      "3/3 [==============================] - 1s 343ms/step - loss: 0.0759 - accuracy: 1.0000 - val_loss: 0.7466 - val_accuracy: 0.5227\n",
      "Epoch 57/65\n",
      "3/3 [==============================] - 1s 328ms/step - loss: 0.0757 - accuracy: 0.9972 - val_loss: 0.7503 - val_accuracy: 0.5341\n",
      "Epoch 58/65\n",
      "3/3 [==============================] - 1s 335ms/step - loss: 0.0712 - accuracy: 1.0000 - val_loss: 0.7457 - val_accuracy: 0.5455\n",
      "Epoch 59/65\n",
      "3/3 [==============================] - 1s 328ms/step - loss: 0.0705 - accuracy: 1.0000 - val_loss: 0.7471 - val_accuracy: 0.5227\n",
      "Epoch 60/65\n",
      "3/3 [==============================] - 1s 330ms/step - loss: 0.0709 - accuracy: 1.0000 - val_loss: 0.7473 - val_accuracy: 0.5227\n",
      "Epoch 61/65\n",
      "3/3 [==============================] - 1s 339ms/step - loss: 0.0704 - accuracy: 1.0000 - val_loss: 0.7390 - val_accuracy: 0.5682\n",
      "Epoch 62/65\n",
      "3/3 [==============================] - 1s 345ms/step - loss: 0.0728 - accuracy: 0.9943 - val_loss: 0.7496 - val_accuracy: 0.5227\n",
      "Epoch 63/65\n",
      "3/3 [==============================] - 1s 351ms/step - loss: 0.0963 - accuracy: 0.9972 - val_loss: 0.7393 - val_accuracy: 0.5227\n",
      "Epoch 64/65\n",
      "3/3 [==============================] - 1s 335ms/step - loss: 0.0966 - accuracy: 0.9943 - val_loss: 0.7420 - val_accuracy: 0.5227\n",
      "Epoch 65/65\n",
      "3/3 [==============================] - 1s 334ms/step - loss: 0.1106 - accuracy: 0.9943 - val_loss: 0.7531 - val_accuracy: 0.5455\n",
      "3/3 [==============================] - 0s 39ms/step\n",
      "auc_roc: 0.5214470284237727\n",
      "auc_pr: 0.5406585931377828\n",
      "f1_score: 0.5119724025974025\n",
      "mcc: 0.09272650639567577\n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "[[0.2956676  0.7043325 ]\n",
      " [0.19118683 0.80881315]\n",
      " [0.7237689  0.2762311 ]\n",
      " [0.34471437 0.65528566]\n",
      " [0.3882373  0.61176276]\n",
      " [0.19836083 0.8016392 ]\n",
      " [0.25938234 0.74061775]\n",
      " [0.31931478 0.6806852 ]\n",
      " [0.6005744  0.39942557]\n",
      " [0.5183622  0.48163787]\n",
      " [0.22983335 0.77016664]\n",
      " [0.25449303 0.74550694]\n",
      " [0.8063603  0.19363967]\n",
      " [0.46168843 0.53831166]\n",
      " [0.44835588 0.5516441 ]\n",
      " [0.43093517 0.56906486]\n",
      " [0.5908819  0.40911812]\n",
      " [0.3986723  0.6013277 ]\n",
      " [0.28014055 0.7198594 ]\n",
      " [0.20537439 0.7946256 ]\n",
      " [0.09098051 0.9090195 ]\n",
      " [0.22609252 0.77390754]\n",
      " [0.28081387 0.7191861 ]\n",
      " [0.42044365 0.5795563 ]\n",
      " [0.65209645 0.3479035 ]\n",
      " [0.6251273  0.37487265]\n",
      " [0.15458624 0.8454138 ]\n",
      " [0.37580606 0.624194  ]\n",
      " [0.1990318  0.8009682 ]\n",
      " [0.23974468 0.76025534]\n",
      " [0.44470558 0.55529445]\n",
      " [0.3087262  0.69127387]\n",
      " [0.41524217 0.5847579 ]\n",
      " [0.38754308 0.6124569 ]\n",
      " [0.28268105 0.717319  ]\n",
      " [0.40684566 0.5931544 ]\n",
      " [0.37346914 0.6265309 ]\n",
      " [0.3867519  0.6132481 ]\n",
      " [0.4566849  0.5433152 ]\n",
      " [0.26183033 0.73816967]\n",
      " [0.35770497 0.642295  ]\n",
      " [0.7323543  0.26764572]\n",
      " [0.3957942  0.6042058 ]\n",
      " [0.19229943 0.8077005 ]\n",
      " [0.52197564 0.47802433]\n",
      " [0.50605345 0.4939466 ]\n",
      " [0.43057376 0.5694263 ]\n",
      " [0.13598032 0.8640197 ]\n",
      " [0.49700046 0.50299954]\n",
      " [0.39967874 0.60032123]\n",
      " [0.36503524 0.6349648 ]\n",
      " [0.18673465 0.8132654 ]\n",
      " [0.4123896  0.58761036]\n",
      " [0.19878288 0.80121714]\n",
      " [0.6073605  0.39263958]\n",
      " [0.50596136 0.49403867]\n",
      " [0.5389694  0.4610307 ]\n",
      " [0.26702923 0.7329707 ]\n",
      " [0.5290935  0.4709065 ]\n",
      " [0.67872095 0.32127908]\n",
      " [0.39411134 0.6058887 ]\n",
      " [0.56863153 0.43136844]\n",
      " [0.26572943 0.7342706 ]\n",
      " [0.39990264 0.60009736]\n",
      " [0.62000453 0.3799955 ]\n",
      " [0.1284512  0.8715488 ]\n",
      " [0.2835738  0.71642613]\n",
      " [0.48103154 0.51896846]\n",
      " [0.21489052 0.78510946]\n",
      " [0.13795799 0.862042  ]\n",
      " [0.4634941  0.53650594]\n",
      " [0.3430174  0.65698266]\n",
      " [0.10200728 0.89799273]\n",
      " [0.3785977  0.6214023 ]\n",
      " [0.7887715  0.21122845]\n",
      " [0.22065167 0.7793484 ]\n",
      " [0.36948034 0.6305197 ]\n",
      " [0.48411775 0.51588225]\n",
      " [0.25983906 0.74016094]\n",
      " [0.5651789  0.43482113]\n",
      " [0.5679603  0.43203968]\n",
      " [0.36866996 0.6313301 ]\n",
      " [0.45751593 0.5424841 ]\n",
      " [0.34031716 0.6596828 ]\n",
      " [0.281017   0.71898305]\n",
      " [0.6183377  0.38166237]\n",
      " [0.32934073 0.67065924]\n",
      " [0.46990147 0.5300986 ]]\n",
      "Model: \"res\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 16, 396, 1)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " res2a-conv1 (Conv2D)           (None, 16, 396, 64)  576         ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " res2a-batchnorm1 (BatchNormali  (None, 16, 396, 64)  256        ['res2a-conv1[0][0]']            \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 16, 396, 64)  0           ['res2a-batchnorm1[0][0]']       \n",
      "                                                                                                  \n",
      " res2a-conv2 (Conv2D)           (None, 16, 396, 64)  32832       ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " res2a-batchnorm2 (BatchNormali  (None, 16, 396, 64)  256        ['res2a-conv2[0][0]']            \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 16, 396, 64)  0           ['res2a-batchnorm2[0][0]',       \n",
      "                                                                  'input_3[0][0]']                \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 16, 396, 64)  0           ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " res2b-conv1 (Conv2D)           (None, 16, 396, 64)  32832       ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " res2b-batchnorm1 (BatchNormali  (None, 16, 396, 64)  256        ['res2b-conv1[0][0]']            \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 16, 396, 64)  0           ['res2b-batchnorm1[0][0]']       \n",
      "                                                                                                  \n",
      " res2b-conv2 (Conv2D)           (None, 16, 396, 64)  32832       ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " res2b-batchnorm2 (BatchNormali  (None, 16, 396, 64)  256        ['res2b-conv2[0][0]']            \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 16, 396, 64)  0           ['res2b-batchnorm2[0][0]',       \n",
      "                                                                  'activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 16, 396, 64)  0           ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2 (Conv2D)                 (None, 16, 396, 1)   65          ['activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 6336)         0           ['conv2[0][0]']                  \n",
      "                                                                                                  \n",
      " fc (Dense)                     (None, 100)          633700      ['flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 100)          0           ['fc[0][0]']                     \n",
      "                                                                                                  \n",
      " softmax (Dense)                (None, 2)            202         ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 734,063\n",
      "Trainable params: 733,551\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/65\n",
      "3/3 [==============================] - 4s 648ms/step - loss: 2.7711 - accuracy: 0.5227 - val_loss: 2.4073 - val_accuracy: 0.5114\n",
      "Epoch 2/65\n",
      "3/3 [==============================] - 1s 373ms/step - loss: 2.3376 - accuracy: 0.6080 - val_loss: 2.1935 - val_accuracy: 0.5114\n",
      "Epoch 3/65\n",
      "3/3 [==============================] - 1s 345ms/step - loss: 2.0232 - accuracy: 0.7188 - val_loss: 1.9954 - val_accuracy: 0.5114\n",
      "Epoch 4/65\n",
      "3/3 [==============================] - 1s 340ms/step - loss: 1.7813 - accuracy: 0.7358 - val_loss: 1.8116 - val_accuracy: 0.5114\n",
      "Epoch 5/65\n",
      "3/3 [==============================] - 1s 368ms/step - loss: 1.5500 - accuracy: 0.7983 - val_loss: 1.6513 - val_accuracy: 0.5114\n",
      "Epoch 6/65\n",
      "3/3 [==============================] - 1s 348ms/step - loss: 1.3476 - accuracy: 0.8153 - val_loss: 1.5253 - val_accuracy: 0.5114\n",
      "Epoch 7/65\n",
      "3/3 [==============================] - 1s 396ms/step - loss: 1.1767 - accuracy: 0.8466 - val_loss: 1.4102 - val_accuracy: 0.5114\n",
      "Epoch 8/65\n",
      "3/3 [==============================] - 1s 435ms/step - loss: 1.0219 - accuracy: 0.8807 - val_loss: 1.3186 - val_accuracy: 0.5114\n",
      "Epoch 9/65\n",
      "3/3 [==============================] - 1s 395ms/step - loss: 0.9146 - accuracy: 0.8864 - val_loss: 1.2512 - val_accuracy: 0.5114\n",
      "Epoch 10/65\n",
      "3/3 [==============================] - 1s 344ms/step - loss: 0.8032 - accuracy: 0.9176 - val_loss: 1.1865 - val_accuracy: 0.5114\n",
      "Epoch 11/65\n",
      "3/3 [==============================] - 1s 351ms/step - loss: 0.7256 - accuracy: 0.9318 - val_loss: 1.1368 - val_accuracy: 0.5114\n",
      "Epoch 12/65\n",
      "3/3 [==============================] - 1s 349ms/step - loss: 0.6347 - accuracy: 0.9517 - val_loss: 1.0919 - val_accuracy: 0.5114\n",
      "Epoch 13/65\n",
      "3/3 [==============================] - 1s 331ms/step - loss: 0.5945 - accuracy: 0.9347 - val_loss: 1.0532 - val_accuracy: 0.5114\n",
      "Epoch 14/65\n",
      "3/3 [==============================] - 1s 344ms/step - loss: 0.5389 - accuracy: 0.9574 - val_loss: 1.0210 - val_accuracy: 0.5114\n",
      "Epoch 15/65\n",
      "3/3 [==============================] - 1s 338ms/step - loss: 0.4909 - accuracy: 0.9631 - val_loss: 0.9929 - val_accuracy: 0.5114\n",
      "Epoch 16/65\n",
      "3/3 [==============================] - 1s 338ms/step - loss: 0.4532 - accuracy: 0.9830 - val_loss: 0.9715 - val_accuracy: 0.5114\n",
      "Epoch 17/65\n",
      "3/3 [==============================] - 1s 353ms/step - loss: 0.4309 - accuracy: 0.9773 - val_loss: 0.9512 - val_accuracy: 0.5114\n",
      "Epoch 18/65\n",
      "3/3 [==============================] - 1s 334ms/step - loss: 0.4024 - accuracy: 0.9688 - val_loss: 0.9324 - val_accuracy: 0.5114\n",
      "Epoch 19/65\n",
      "3/3 [==============================] - 1s 328ms/step - loss: 0.3712 - accuracy: 0.9744 - val_loss: 0.9172 - val_accuracy: 0.5114\n",
      "Epoch 20/65\n",
      "3/3 [==============================] - 1s 345ms/step - loss: 0.3437 - accuracy: 0.9915 - val_loss: 0.9023 - val_accuracy: 0.5114\n",
      "Epoch 21/65\n",
      "3/3 [==============================] - 1s 394ms/step - loss: 0.3314 - accuracy: 0.9858 - val_loss: 0.8904 - val_accuracy: 0.5114\n",
      "Epoch 22/65\n",
      "3/3 [==============================] - 1s 351ms/step - loss: 0.3095 - accuracy: 0.9830 - val_loss: 0.8821 - val_accuracy: 0.5114\n",
      "Epoch 23/65\n",
      "3/3 [==============================] - 1s 353ms/step - loss: 0.2883 - accuracy: 0.9972 - val_loss: 0.8700 - val_accuracy: 0.5114\n",
      "Epoch 24/65\n",
      "3/3 [==============================] - 1s 352ms/step - loss: 0.2821 - accuracy: 0.9886 - val_loss: 0.8603 - val_accuracy: 0.5114\n",
      "Epoch 25/65\n",
      "3/3 [==============================] - 1s 327ms/step - loss: 0.2602 - accuracy: 0.9972 - val_loss: 0.8540 - val_accuracy: 0.5114\n",
      "Epoch 26/65\n",
      "3/3 [==============================] - 1s 360ms/step - loss: 0.2515 - accuracy: 0.9972 - val_loss: 0.8448 - val_accuracy: 0.5114\n",
      "Epoch 27/65\n",
      "3/3 [==============================] - 1s 348ms/step - loss: 0.2372 - accuracy: 0.9915 - val_loss: 0.8375 - val_accuracy: 0.5114\n",
      "Epoch 28/65\n",
      "3/3 [==============================] - 1s 358ms/step - loss: 0.2284 - accuracy: 0.9943 - val_loss: 0.8309 - val_accuracy: 0.5114\n",
      "Epoch 29/65\n",
      "3/3 [==============================] - 1s 334ms/step - loss: 0.2180 - accuracy: 0.9943 - val_loss: 0.8243 - val_accuracy: 0.5114\n",
      "Epoch 30/65\n",
      "3/3 [==============================] - 1s 333ms/step - loss: 0.2067 - accuracy: 0.9943 - val_loss: 0.8179 - val_accuracy: 0.5114\n",
      "Epoch 31/65\n",
      "3/3 [==============================] - 1s 357ms/step - loss: 0.1995 - accuracy: 0.9943 - val_loss: 0.8104 - val_accuracy: 0.5114\n",
      "Epoch 32/65\n",
      "3/3 [==============================] - 1s 344ms/step - loss: 0.1861 - accuracy: 0.9972 - val_loss: 0.8048 - val_accuracy: 0.5114\n",
      "Epoch 33/65\n",
      "3/3 [==============================] - 1s 356ms/step - loss: 0.1837 - accuracy: 0.9886 - val_loss: 0.7991 - val_accuracy: 0.5114\n",
      "Epoch 34/65\n",
      "3/3 [==============================] - 1s 327ms/step - loss: 0.1695 - accuracy: 0.9943 - val_loss: 0.7945 - val_accuracy: 0.5114\n",
      "Epoch 35/65\n",
      "3/3 [==============================] - 1s 335ms/step - loss: 0.1640 - accuracy: 0.9943 - val_loss: 0.7896 - val_accuracy: 0.5114\n",
      "Epoch 36/65\n",
      "3/3 [==============================] - 1s 328ms/step - loss: 0.1579 - accuracy: 0.9943 - val_loss: 0.7848 - val_accuracy: 0.5227\n",
      "Epoch 37/65\n",
      "3/3 [==============================] - 1s 329ms/step - loss: 0.1498 - accuracy: 0.9972 - val_loss: 0.7804 - val_accuracy: 0.5227\n",
      "Epoch 38/65\n",
      "3/3 [==============================] - 1s 329ms/step - loss: 0.1521 - accuracy: 0.9915 - val_loss: 0.7755 - val_accuracy: 0.5227\n",
      "Epoch 39/65\n",
      "3/3 [==============================] - 1s 331ms/step - loss: 0.1425 - accuracy: 0.9943 - val_loss: 0.7722 - val_accuracy: 0.5341\n",
      "Epoch 40/65\n",
      "3/3 [==============================] - 1s 331ms/step - loss: 0.1341 - accuracy: 0.9943 - val_loss: 0.7697 - val_accuracy: 0.5341\n",
      "Epoch 41/65\n",
      "3/3 [==============================] - 1s 331ms/step - loss: 0.1292 - accuracy: 0.9972 - val_loss: 0.7652 - val_accuracy: 0.5341\n",
      "Epoch 42/65\n",
      "3/3 [==============================] - 1s 347ms/step - loss: 0.1272 - accuracy: 0.9972 - val_loss: 0.7623 - val_accuracy: 0.5341\n",
      "Epoch 43/65\n",
      "3/3 [==============================] - 1s 327ms/step - loss: 0.1215 - accuracy: 1.0000 - val_loss: 0.7595 - val_accuracy: 0.5455\n",
      "Epoch 44/65\n",
      "3/3 [==============================] - 1s 333ms/step - loss: 0.1163 - accuracy: 0.9972 - val_loss: 0.7549 - val_accuracy: 0.5455\n",
      "Epoch 45/65\n",
      "3/3 [==============================] - 1s 358ms/step - loss: 0.1167 - accuracy: 0.9972 - val_loss: 0.7508 - val_accuracy: 0.5455\n",
      "Epoch 46/65\n",
      "3/3 [==============================] - 1s 386ms/step - loss: 0.1076 - accuracy: 0.9972 - val_loss: 0.7486 - val_accuracy: 0.5455\n",
      "Epoch 47/65\n",
      "3/3 [==============================] - 1s 430ms/step - loss: 0.1023 - accuracy: 0.9972 - val_loss: 0.7460 - val_accuracy: 0.5455\n",
      "Epoch 48/65\n",
      "3/3 [==============================] - 1s 343ms/step - loss: 0.1019 - accuracy: 0.9972 - val_loss: 0.7414 - val_accuracy: 0.5455\n",
      "Epoch 49/65\n",
      "3/3 [==============================] - 1s 334ms/step - loss: 0.0983 - accuracy: 0.9943 - val_loss: 0.7404 - val_accuracy: 0.5455\n",
      "Epoch 50/65\n",
      "3/3 [==============================] - 1s 361ms/step - loss: 0.1017 - accuracy: 0.9915 - val_loss: 0.7362 - val_accuracy: 0.5455\n",
      "Epoch 51/65\n",
      "3/3 [==============================] - 1s 359ms/step - loss: 0.0951 - accuracy: 0.9972 - val_loss: 0.7344 - val_accuracy: 0.5455\n",
      "Epoch 52/65\n",
      "3/3 [==============================] - 1s 331ms/step - loss: 0.0912 - accuracy: 0.9943 - val_loss: 0.7328 - val_accuracy: 0.5455\n",
      "Epoch 53/65\n",
      "3/3 [==============================] - 1s 344ms/step - loss: 0.0903 - accuracy: 0.9972 - val_loss: 0.7327 - val_accuracy: 0.5455\n",
      "Epoch 54/65\n",
      "3/3 [==============================] - 1s 336ms/step - loss: 0.0896 - accuracy: 0.9972 - val_loss: 0.7297 - val_accuracy: 0.5568\n",
      "Epoch 55/65\n",
      "3/3 [==============================] - 1s 330ms/step - loss: 0.0802 - accuracy: 1.0000 - val_loss: 0.7291 - val_accuracy: 0.5568\n",
      "Epoch 56/65\n",
      "3/3 [==============================] - 1s 347ms/step - loss: 0.0772 - accuracy: 0.9972 - val_loss: 0.7278 - val_accuracy: 0.5568\n",
      "Epoch 57/65\n",
      "3/3 [==============================] - 1s 331ms/step - loss: 0.0772 - accuracy: 0.9972 - val_loss: 0.7271 - val_accuracy: 0.5455\n",
      "Epoch 58/65\n",
      "3/3 [==============================] - 1s 341ms/step - loss: 0.0763 - accuracy: 0.9972 - val_loss: 0.7248 - val_accuracy: 0.5568\n",
      "Epoch 59/65\n",
      "3/3 [==============================] - 1s 338ms/step - loss: 0.0722 - accuracy: 1.0000 - val_loss: 0.7214 - val_accuracy: 0.5568\n",
      "Epoch 60/65\n",
      "3/3 [==============================] - 1s 338ms/step - loss: 0.0716 - accuracy: 0.9972 - val_loss: 0.7196 - val_accuracy: 0.5455\n",
      "Epoch 61/65\n",
      "3/3 [==============================] - 1s 339ms/step - loss: 0.0711 - accuracy: 0.9972 - val_loss: 0.7197 - val_accuracy: 0.5455\n",
      "Epoch 62/65\n",
      "3/3 [==============================] - 1s 338ms/step - loss: 0.0667 - accuracy: 0.9972 - val_loss: 0.7196 - val_accuracy: 0.5568\n",
      "Epoch 63/65\n",
      "3/3 [==============================] - 1s 372ms/step - loss: 0.0682 - accuracy: 0.9943 - val_loss: 0.7159 - val_accuracy: 0.5568\n",
      "Epoch 64/65\n",
      "3/3 [==============================] - 1s 344ms/step - loss: 0.0641 - accuracy: 0.9972 - val_loss: 0.7164 - val_accuracy: 0.5568\n",
      "Epoch 65/65\n",
      "3/3 [==============================] - 1s 332ms/step - loss: 0.0619 - accuracy: 0.9972 - val_loss: 0.7188 - val_accuracy: 0.5568\n",
      "3/3 [==============================] - 0s 44ms/step\n",
      "auc_roc: 0.6935400516795865\n",
      "auc_pr: 0.672858682150157\n",
      "f1_score: 0.4540524155259813\n",
      "mcc: 0.1865298546977316\n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "[[0.28041324 0.7195868 ]\n",
      " [0.20119472 0.7988053 ]\n",
      " [0.26490206 0.735098  ]\n",
      " [0.14787093 0.85212904]\n",
      " [0.2973355  0.70266455]\n",
      " [0.26729894 0.73270106]\n",
      " [0.16804689 0.8319531 ]\n",
      " [0.14273396 0.85726607]\n",
      " [0.23109545 0.76890457]\n",
      " [0.31052518 0.68947476]\n",
      " [0.4823102  0.5176899 ]\n",
      " [0.10478354 0.89521646]\n",
      " [0.4358241  0.5641759 ]\n",
      " [0.30823505 0.69176495]\n",
      " [0.03094518 0.9690549 ]\n",
      " [0.41302273 0.58697724]\n",
      " [0.40581518 0.5941849 ]\n",
      " [0.7838649  0.21613511]\n",
      " [0.05307629 0.94692373]\n",
      " [0.08296685 0.91703314]\n",
      " [0.5251061  0.47489393]\n",
      " [0.26155224 0.7384477 ]\n",
      " [0.4339882  0.56601185]\n",
      " [0.05815646 0.94184357]\n",
      " [0.16588078 0.83411926]\n",
      " [0.17212057 0.8278794 ]\n",
      " [0.18864639 0.8113536 ]\n",
      " [0.3365074  0.6634926 ]\n",
      " [0.16596754 0.83403254]\n",
      " [0.22547552 0.77452445]\n",
      " [0.27594963 0.7240504 ]\n",
      " [0.4166037  0.5833963 ]\n",
      " [0.26732773 0.7326723 ]\n",
      " [0.2792116  0.7207885 ]\n",
      " [0.20496795 0.79503214]\n",
      " [0.8812233  0.11877672]\n",
      " [0.28312105 0.71687895]\n",
      " [0.40650117 0.5934988 ]\n",
      " [0.19578159 0.8042185 ]\n",
      " [0.04517904 0.95482093]\n",
      " [0.06534656 0.93465346]\n",
      " [0.26926795 0.7307321 ]\n",
      " [0.50522035 0.49477965]\n",
      " [0.31863996 0.68136007]\n",
      " [0.34759158 0.6524085 ]\n",
      " [0.22666204 0.77333796]\n",
      " [0.45583543 0.5441646 ]\n",
      " [0.21664853 0.7833514 ]\n",
      " [0.3008675  0.6991325 ]\n",
      " [0.21012561 0.7898744 ]\n",
      " [0.1263575  0.8736425 ]\n",
      " [0.38369596 0.61630404]\n",
      " [0.09821083 0.9017892 ]\n",
      " [0.50609654 0.4939034 ]\n",
      " [0.22242299 0.77757704]\n",
      " [0.3610539  0.6389461 ]\n",
      " [0.2365709  0.7634291 ]\n",
      " [0.19029279 0.8097072 ]\n",
      " [0.07875386 0.9212462 ]\n",
      " [0.18965083 0.81034917]\n",
      " [0.09339666 0.9066034 ]\n",
      " [0.37535056 0.6246494 ]\n",
      " [0.2540052  0.7459948 ]\n",
      " [0.10432325 0.89567673]\n",
      " [0.29187638 0.7081236 ]\n",
      " [0.4163164  0.58368355]\n",
      " [0.18658982 0.81341016]\n",
      " [0.15485328 0.84514666]\n",
      " [0.47088498 0.529115  ]\n",
      " [0.3534178  0.6465822 ]\n",
      " [0.38815627 0.61184376]\n",
      " [0.20357631 0.7964237 ]\n",
      " [0.48148632 0.51851374]\n",
      " [0.47430038 0.52569956]\n",
      " [0.10346581 0.8965342 ]\n",
      " [0.1854196  0.81458044]\n",
      " [0.2476285  0.7523715 ]\n",
      " [0.20365478 0.79634523]\n",
      " [0.37666103 0.623339  ]\n",
      " [0.42952654 0.5704735 ]\n",
      " [0.24806586 0.7519342 ]\n",
      " [0.23920085 0.76079917]\n",
      " [0.4104536  0.5895464 ]\n",
      " [0.6558068  0.34419328]\n",
      " [0.08372418 0.91627586]\n",
      " [0.48589703 0.51410294]\n",
      " [0.18671525 0.8132848 ]\n",
      " [0.06702467 0.93297535]]\n",
      "Model: \"res\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 16, 396, 1)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " res2a-conv1 (Conv2D)           (None, 16, 396, 64)  576         ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " res2a-batchnorm1 (BatchNormali  (None, 16, 396, 64)  256        ['res2a-conv1[0][0]']            \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 16, 396, 64)  0           ['res2a-batchnorm1[0][0]']       \n",
      "                                                                                                  \n",
      " res2a-conv2 (Conv2D)           (None, 16, 396, 64)  32832       ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " res2a-batchnorm2 (BatchNormali  (None, 16, 396, 64)  256        ['res2a-conv2[0][0]']            \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 16, 396, 64)  0           ['res2a-batchnorm2[0][0]',       \n",
      "                                                                  'input_4[0][0]']                \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 16, 396, 64)  0           ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " res2b-conv1 (Conv2D)           (None, 16, 396, 64)  32832       ['activation_13[0][0]']          \n",
      "                                                                                                  \n",
      " res2b-batchnorm1 (BatchNormali  (None, 16, 396, 64)  256        ['res2b-conv1[0][0]']            \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 16, 396, 64)  0           ['res2b-batchnorm1[0][0]']       \n",
      "                                                                                                  \n",
      " res2b-conv2 (Conv2D)           (None, 16, 396, 64)  32832       ['activation_14[0][0]']          \n",
      "                                                                                                  \n",
      " res2b-batchnorm2 (BatchNormali  (None, 16, 396, 64)  256        ['res2b-conv2[0][0]']            \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 16, 396, 64)  0           ['res2b-batchnorm2[0][0]',       \n",
      "                                                                  'activation_13[0][0]']          \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 16, 396, 64)  0           ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2 (Conv2D)                 (None, 16, 396, 1)   65          ['activation_15[0][0]']          \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 6336)         0           ['conv2[0][0]']                  \n",
      "                                                                                                  \n",
      " fc (Dense)                     (None, 100)          633700      ['flatten_3[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 100)          0           ['fc[0][0]']                     \n",
      "                                                                                                  \n",
      " softmax (Dense)                (None, 2)            202         ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 734,063\n",
      "Trainable params: 733,551\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/65\n",
      "3/3 [==============================] - 3s 566ms/step - loss: 2.7724 - accuracy: 0.5227 - val_loss: 2.3889 - val_accuracy: 0.5114\n",
      "Epoch 2/65\n",
      "3/3 [==============================] - 1s 371ms/step - loss: 2.2734 - accuracy: 0.6676 - val_loss: 2.1348 - val_accuracy: 0.5114\n",
      "Epoch 3/65\n",
      "3/3 [==============================] - 1s 378ms/step - loss: 1.9516 - accuracy: 0.7500 - val_loss: 1.9165 - val_accuracy: 0.5114\n",
      "Epoch 4/65\n",
      "3/3 [==============================] - 1s 353ms/step - loss: 1.6888 - accuracy: 0.7472 - val_loss: 1.7227 - val_accuracy: 0.5114\n",
      "Epoch 5/65\n",
      "3/3 [==============================] - 1s 357ms/step - loss: 1.4568 - accuracy: 0.8040 - val_loss: 1.5636 - val_accuracy: 0.5114\n",
      "Epoch 6/65\n",
      "3/3 [==============================] - 1s 351ms/step - loss: 1.2611 - accuracy: 0.8011 - val_loss: 1.4261 - val_accuracy: 0.5114\n",
      "Epoch 7/65\n",
      "3/3 [==============================] - 1s 348ms/step - loss: 1.0790 - accuracy: 0.8494 - val_loss: 1.3102 - val_accuracy: 0.5114\n",
      "Epoch 8/65\n",
      "3/3 [==============================] - 1s 348ms/step - loss: 0.9354 - accuracy: 0.8835 - val_loss: 1.2163 - val_accuracy: 0.5114\n",
      "Epoch 9/65\n",
      "3/3 [==============================] - 1s 333ms/step - loss: 0.8279 - accuracy: 0.8864 - val_loss: 1.1447 - val_accuracy: 0.5114\n",
      "Epoch 10/65\n",
      "3/3 [==============================] - 1s 335ms/step - loss: 0.7363 - accuracy: 0.9233 - val_loss: 1.0868 - val_accuracy: 0.5114\n",
      "Epoch 11/65\n",
      "3/3 [==============================] - 1s 345ms/step - loss: 0.6556 - accuracy: 0.9176 - val_loss: 1.0419 - val_accuracy: 0.5114\n",
      "Epoch 12/65\n",
      "3/3 [==============================] - 1s 329ms/step - loss: 0.5915 - accuracy: 0.9574 - val_loss: 1.0072 - val_accuracy: 0.5114\n",
      "Epoch 13/65\n",
      "3/3 [==============================] - 1s 330ms/step - loss: 0.5479 - accuracy: 0.9602 - val_loss: 0.9806 - val_accuracy: 0.5114\n",
      "Epoch 14/65\n",
      "3/3 [==============================] - 1s 343ms/step - loss: 0.4947 - accuracy: 0.9716 - val_loss: 0.9589 - val_accuracy: 0.5114\n",
      "Epoch 15/65\n",
      "3/3 [==============================] - 1s 331ms/step - loss: 0.4662 - accuracy: 0.9688 - val_loss: 0.9395 - val_accuracy: 0.5114\n",
      "Epoch 16/65\n",
      "3/3 [==============================] - 1s 336ms/step - loss: 0.4306 - accuracy: 0.9744 - val_loss: 0.9240 - val_accuracy: 0.5114\n",
      "Epoch 17/65\n",
      "3/3 [==============================] - 1s 333ms/step - loss: 0.3974 - accuracy: 0.9858 - val_loss: 0.9096 - val_accuracy: 0.5227\n",
      "Epoch 18/65\n",
      "3/3 [==============================] - 1s 331ms/step - loss: 0.3842 - accuracy: 0.9801 - val_loss: 0.8971 - val_accuracy: 0.5227\n",
      "Epoch 19/65\n",
      "3/3 [==============================] - 1s 331ms/step - loss: 0.3600 - accuracy: 0.9801 - val_loss: 0.8845 - val_accuracy: 0.5000\n",
      "Epoch 20/65\n",
      "3/3 [==============================] - 1s 332ms/step - loss: 0.3415 - accuracy: 0.9858 - val_loss: 0.8738 - val_accuracy: 0.5114\n",
      "Epoch 21/65\n",
      "3/3 [==============================] - 1s 328ms/step - loss: 0.3206 - accuracy: 0.9801 - val_loss: 0.8623 - val_accuracy: 0.5000\n",
      "Epoch 22/65\n",
      "3/3 [==============================] - 1s 328ms/step - loss: 0.2984 - accuracy: 0.9915 - val_loss: 0.8538 - val_accuracy: 0.5114\n",
      "Epoch 23/65\n",
      "3/3 [==============================] - 1s 339ms/step - loss: 0.2761 - accuracy: 0.9915 - val_loss: 0.8448 - val_accuracy: 0.5000\n",
      "Epoch 24/65\n",
      "3/3 [==============================] - 1s 332ms/step - loss: 0.2676 - accuracy: 0.9858 - val_loss: 0.8363 - val_accuracy: 0.5000\n",
      "Epoch 25/65\n",
      "3/3 [==============================] - 1s 335ms/step - loss: 0.2501 - accuracy: 0.9972 - val_loss: 0.8288 - val_accuracy: 0.5000\n",
      "Epoch 26/65\n",
      "3/3 [==============================] - 1s 330ms/step - loss: 0.2444 - accuracy: 0.9972 - val_loss: 0.8222 - val_accuracy: 0.5000\n",
      "Epoch 27/65\n",
      "3/3 [==============================] - 1s 330ms/step - loss: 0.2297 - accuracy: 0.9915 - val_loss: 0.8162 - val_accuracy: 0.5000\n",
      "Epoch 28/65\n",
      "3/3 [==============================] - 1s 332ms/step - loss: 0.2202 - accuracy: 0.9972 - val_loss: 0.8094 - val_accuracy: 0.5000\n",
      "Epoch 29/65\n",
      "3/3 [==============================] - 1s 354ms/step - loss: 0.2118 - accuracy: 1.0000 - val_loss: 0.8037 - val_accuracy: 0.5114\n",
      "Epoch 30/65\n",
      "3/3 [==============================] - 1s 343ms/step - loss: 0.2017 - accuracy: 0.9915 - val_loss: 0.7988 - val_accuracy: 0.5114\n",
      "Epoch 31/65\n",
      "3/3 [==============================] - 1s 331ms/step - loss: 0.1943 - accuracy: 0.9943 - val_loss: 0.7943 - val_accuracy: 0.5000\n",
      "Epoch 32/65\n",
      "3/3 [==============================] - 1s 344ms/step - loss: 0.1842 - accuracy: 0.9915 - val_loss: 0.7887 - val_accuracy: 0.5114\n",
      "Epoch 33/65\n",
      "3/3 [==============================] - 1s 348ms/step - loss: 0.1812 - accuracy: 0.9943 - val_loss: 0.7853 - val_accuracy: 0.5114\n",
      "Epoch 34/65\n",
      "3/3 [==============================] - 1s 360ms/step - loss: 0.1658 - accuracy: 1.0000 - val_loss: 0.7803 - val_accuracy: 0.5114\n",
      "Epoch 35/65\n",
      "3/3 [==============================] - 1s 330ms/step - loss: 0.1690 - accuracy: 0.9915 - val_loss: 0.7774 - val_accuracy: 0.5114\n",
      "Epoch 36/65\n",
      "3/3 [==============================] - 1s 344ms/step - loss: 0.1557 - accuracy: 0.9943 - val_loss: 0.7740 - val_accuracy: 0.5227\n",
      "Epoch 37/65\n",
      "3/3 [==============================] - 1s 344ms/step - loss: 0.1518 - accuracy: 0.9972 - val_loss: 0.7710 - val_accuracy: 0.5000\n",
      "Epoch 38/65\n",
      "3/3 [==============================] - 1s 342ms/step - loss: 0.1422 - accuracy: 1.0000 - val_loss: 0.7691 - val_accuracy: 0.5227\n",
      "Epoch 39/65\n",
      "3/3 [==============================] - 1s 354ms/step - loss: 0.1378 - accuracy: 1.0000 - val_loss: 0.7648 - val_accuracy: 0.5227\n",
      "Epoch 40/65\n",
      "3/3 [==============================] - 1s 362ms/step - loss: 0.1330 - accuracy: 1.0000 - val_loss: 0.7628 - val_accuracy: 0.5341\n",
      "Epoch 41/65\n",
      "3/3 [==============================] - 1s 368ms/step - loss: 0.1287 - accuracy: 0.9972 - val_loss: 0.7604 - val_accuracy: 0.5227\n",
      "Epoch 42/65\n",
      "3/3 [==============================] - 1s 337ms/step - loss: 0.1284 - accuracy: 1.0000 - val_loss: 0.7573 - val_accuracy: 0.5341\n",
      "Epoch 43/65\n",
      "3/3 [==============================] - 1s 330ms/step - loss: 0.1174 - accuracy: 1.0000 - val_loss: 0.7535 - val_accuracy: 0.5114\n",
      "Epoch 44/65\n",
      "3/3 [==============================] - 1s 330ms/step - loss: 0.1176 - accuracy: 1.0000 - val_loss: 0.7525 - val_accuracy: 0.5114\n",
      "Epoch 45/65\n",
      "3/3 [==============================] - 1s 354ms/step - loss: 0.1091 - accuracy: 1.0000 - val_loss: 0.7495 - val_accuracy: 0.5114\n",
      "Epoch 46/65\n",
      "3/3 [==============================] - 1s 330ms/step - loss: 0.1043 - accuracy: 1.0000 - val_loss: 0.7484 - val_accuracy: 0.5227\n",
      "Epoch 47/65\n",
      "3/3 [==============================] - 1s 328ms/step - loss: 0.0995 - accuracy: 1.0000 - val_loss: 0.7468 - val_accuracy: 0.5227\n",
      "Epoch 48/65\n",
      "3/3 [==============================] - 1s 327ms/step - loss: 0.0989 - accuracy: 1.0000 - val_loss: 0.7422 - val_accuracy: 0.5114\n",
      "Epoch 49/65\n",
      "3/3 [==============================] - 1s 331ms/step - loss: 0.0911 - accuracy: 1.0000 - val_loss: 0.7405 - val_accuracy: 0.5114\n",
      "Epoch 50/65\n",
      "3/3 [==============================] - 1s 374ms/step - loss: 0.0896 - accuracy: 1.0000 - val_loss: 0.7384 - val_accuracy: 0.5114\n",
      "Epoch 51/65\n",
      "3/3 [==============================] - 1s 333ms/step - loss: 0.0844 - accuracy: 1.0000 - val_loss: 0.7354 - val_accuracy: 0.5114\n",
      "Epoch 52/65\n",
      "3/3 [==============================] - 1s 338ms/step - loss: 0.0832 - accuracy: 1.0000 - val_loss: 0.7330 - val_accuracy: 0.5227\n",
      "Epoch 53/65\n",
      "3/3 [==============================] - 1s 330ms/step - loss: 0.0787 - accuracy: 1.0000 - val_loss: 0.7297 - val_accuracy: 0.5227\n",
      "Epoch 54/65\n",
      "3/3 [==============================] - 1s 370ms/step - loss: 0.0758 - accuracy: 1.0000 - val_loss: 0.7288 - val_accuracy: 0.5341\n",
      "Epoch 55/65\n",
      "3/3 [==============================] - 1s 347ms/step - loss: 0.0747 - accuracy: 1.0000 - val_loss: 0.7250 - val_accuracy: 0.5227\n",
      "Epoch 56/65\n",
      "3/3 [==============================] - 1s 345ms/step - loss: 0.0728 - accuracy: 1.0000 - val_loss: 0.7253 - val_accuracy: 0.5341\n",
      "Epoch 57/65\n",
      "3/3 [==============================] - 1s 333ms/step - loss: 0.0682 - accuracy: 1.0000 - val_loss: 0.7208 - val_accuracy: 0.5227\n",
      "Epoch 58/65\n",
      "3/3 [==============================] - 1s 343ms/step - loss: 0.0670 - accuracy: 1.0000 - val_loss: 0.7192 - val_accuracy: 0.5227\n",
      "Epoch 59/65\n",
      "3/3 [==============================] - 1s 348ms/step - loss: 0.0644 - accuracy: 1.0000 - val_loss: 0.7176 - val_accuracy: 0.5227\n",
      "Epoch 60/65\n",
      "3/3 [==============================] - 1s 326ms/step - loss: 0.0607 - accuracy: 1.0000 - val_loss: 0.7151 - val_accuracy: 0.5341\n",
      "Epoch 61/65\n",
      "3/3 [==============================] - 1s 332ms/step - loss: 0.0608 - accuracy: 1.0000 - val_loss: 0.7135 - val_accuracy: 0.5455\n",
      "Epoch 62/65\n",
      "3/3 [==============================] - 1s 348ms/step - loss: 0.0570 - accuracy: 1.0000 - val_loss: 0.7126 - val_accuracy: 0.5568\n",
      "Epoch 63/65\n",
      "3/3 [==============================] - 1s 348ms/step - loss: 0.0582 - accuracy: 1.0000 - val_loss: 0.7107 - val_accuracy: 0.5795\n",
      "Epoch 64/65\n",
      "3/3 [==============================] - 1s 336ms/step - loss: 0.0548 - accuracy: 1.0000 - val_loss: 0.7081 - val_accuracy: 0.5795\n",
      "Epoch 65/65\n",
      "3/3 [==============================] - 1s 329ms/step - loss: 0.0505 - accuracy: 1.0000 - val_loss: 0.7095 - val_accuracy: 0.5341\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x285570d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - 0s 41ms/step\n",
      "auc_roc: 0.6661498708010336\n",
      "auc_pr: 0.6499520312834965\n",
      "f1_score: 0.4723406108855024\n",
      "mcc: 0.0720410127322549\n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "[[0.20646486 0.7935352 ]\n",
      " [0.495732   0.50426805]\n",
      " [0.47765136 0.52234864]\n",
      " [0.37177157 0.6282285 ]\n",
      " [0.49750873 0.5024913 ]\n",
      " [0.5841361  0.41586396]\n",
      " [0.34031743 0.6596826 ]\n",
      " [0.30781642 0.6921836 ]\n",
      " [0.29658657 0.7034135 ]\n",
      " [0.2794064  0.72059363]\n",
      " [0.2959089  0.70409113]\n",
      " [0.18707152 0.81292856]\n",
      " [0.36576873 0.63423127]\n",
      " [0.12270508 0.8772949 ]\n",
      " [0.33147493 0.66852504]\n",
      " [0.20844755 0.7915525 ]\n",
      " [0.25029805 0.749702  ]\n",
      " [0.16154052 0.83845955]\n",
      " [0.47574776 0.52425224]\n",
      " [0.13807666 0.8619234 ]\n",
      " [0.38080665 0.6191934 ]\n",
      " [0.2910866  0.70891345]\n",
      " [0.6267884  0.37321168]\n",
      " [0.34708858 0.65291137]\n",
      " [0.5317153  0.46828482]\n",
      " [0.10811981 0.8918802 ]\n",
      " [0.34095624 0.6590437 ]\n",
      " [0.42786425 0.5721358 ]\n",
      " [0.32954606 0.6704539 ]\n",
      " [0.3708192  0.62918085]\n",
      " [0.6210842  0.37891576]\n",
      " [0.42258975 0.5774103 ]\n",
      " [0.32498667 0.67501336]\n",
      " [0.36259732 0.6374027 ]\n",
      " [0.50220335 0.49779668]\n",
      " [0.06055754 0.9394425 ]\n",
      " [0.28349563 0.71650445]\n",
      " [0.34867957 0.65132046]\n",
      " [0.7670244  0.23297565]\n",
      " [0.51335526 0.48664477]\n",
      " [0.12540929 0.8745907 ]\n",
      " [0.5502375  0.4497625 ]\n",
      " [0.461355   0.5386451 ]\n",
      " [0.12912634 0.8708737 ]\n",
      " [0.31968993 0.6803101 ]\n",
      " [0.05067179 0.94932824]\n",
      " [0.4656812  0.53431886]\n",
      " [0.37264434 0.62735564]\n",
      " [0.23260842 0.7673916 ]\n",
      " [0.4090511  0.590949  ]\n",
      " [0.43706256 0.5629375 ]\n",
      " [0.23296285 0.76703715]\n",
      " [0.5249966  0.4750035 ]\n",
      " [0.34201667 0.65798336]\n",
      " [0.32895735 0.6710426 ]\n",
      " [0.25419688 0.74580306]\n",
      " [0.28173643 0.7182636 ]\n",
      " [0.40637213 0.5936279 ]\n",
      " [0.7176247  0.28237525]\n",
      " [0.57188    0.42811996]\n",
      " [0.41537362 0.58462644]\n",
      " [0.36995035 0.63004965]\n",
      " [0.08749254 0.9125075 ]\n",
      " [0.20071205 0.7992879 ]\n",
      " [0.2465843  0.7534157 ]\n",
      " [0.17226908 0.82773095]\n",
      " [0.3526527  0.64734733]\n",
      " [0.31243926 0.68756074]\n",
      " [0.22415212 0.7758479 ]\n",
      " [0.41141534 0.58858466]\n",
      " [0.11722971 0.88277036]\n",
      " [0.29659936 0.7034006 ]\n",
      " [0.4567336  0.5432664 ]\n",
      " [0.53864294 0.46135712]\n",
      " [0.43235812 0.5676419 ]\n",
      " [0.09388044 0.9061195 ]\n",
      " [0.40711173 0.59288824]\n",
      " [0.18738091 0.81261915]\n",
      " [0.6882607  0.3117393 ]\n",
      " [0.41690257 0.58309746]\n",
      " [0.4843564  0.5156436 ]\n",
      " [0.38427958 0.6157204 ]\n",
      " [0.3407881  0.659212  ]\n",
      " [0.21739149 0.7826085 ]\n",
      " [0.25300702 0.746993  ]\n",
      " [0.4042928  0.5957073 ]\n",
      " [0.53600055 0.46399945]\n",
      " [0.41114488 0.58885515]]\n",
      "Model: \"res\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 16, 396, 1)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " res2a-conv1 (Conv2D)           (None, 16, 396, 64)  576         ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " res2a-batchnorm1 (BatchNormali  (None, 16, 396, 64)  256        ['res2a-conv1[0][0]']            \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 16, 396, 64)  0           ['res2a-batchnorm1[0][0]']       \n",
      "                                                                                                  \n",
      " res2a-conv2 (Conv2D)           (None, 16, 396, 64)  32832       ['activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " res2a-batchnorm2 (BatchNormali  (None, 16, 396, 64)  256        ['res2a-conv2[0][0]']            \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 16, 396, 64)  0           ['res2a-batchnorm2[0][0]',       \n",
      "                                                                  'input_5[0][0]']                \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 16, 396, 64)  0           ['add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " res2b-conv1 (Conv2D)           (None, 16, 396, 64)  32832       ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " res2b-batchnorm1 (BatchNormali  (None, 16, 396, 64)  256        ['res2b-conv1[0][0]']            \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 16, 396, 64)  0           ['res2b-batchnorm1[0][0]']       \n",
      "                                                                                                  \n",
      " res2b-conv2 (Conv2D)           (None, 16, 396, 64)  32832       ['activation_18[0][0]']          \n",
      "                                                                                                  \n",
      " res2b-batchnorm2 (BatchNormali  (None, 16, 396, 64)  256        ['res2b-conv2[0][0]']            \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 16, 396, 64)  0           ['res2b-batchnorm2[0][0]',       \n",
      "                                                                  'activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " activation_19 (Activation)     (None, 16, 396, 64)  0           ['add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2 (Conv2D)                 (None, 16, 396, 1)   65          ['activation_19[0][0]']          \n",
      "                                                                                                  \n",
      " flatten_4 (Flatten)            (None, 6336)         0           ['conv2[0][0]']                  \n",
      "                                                                                                  \n",
      " fc (Dense)                     (None, 100)          633700      ['flatten_4[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 100)          0           ['fc[0][0]']                     \n",
      "                                                                                                  \n",
      " softmax (Dense)                (None, 2)            202         ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 734,063\n",
      "Trainable params: 733,551\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "data_lst = []\n",
    "\n",
    "# for i in range(k):\n",
    "#     x_train1 = x_train[i]\n",
    "#     y_train1 = y_train[i]\n",
    "#     x_test1 = x_test[i]\n",
    "#     y_test1 = y_test[i]\n",
    "\n",
    "#     model = ResNet(height = x_train1.shape[1], width = x_train1.shape[2], channels = 1, classes = 2)\n",
    "#     model.init_model()\n",
    "\n",
    "#     model.train(x_train1, y_train1, x_test1, y_test1, dataset, use_weights = False)\n",
    "#     y_pred = model.predict(x_test1)\n",
    "#     auc_roc, auc_pr, f1, mcc = model.evaluate(y_test1, y_pred)\n",
    "#     data_lst.append([auc_roc, auc_pr, f1, mcc])\n",
    "    \n",
    "#     #model.model.save_weights(path + \"/model_weights.h5\")\n",
    "\n",
    "#     print(y_test1)\n",
    "#     print(y_pred)\n",
    "    \n",
    "# print(model.model.summary())\n",
    "\n",
    "\n",
    "\n",
    "n_values = np.max(labels) + 1\n",
    "labels_oh = np.eye(n_values)[labels]\n",
    "tree_row = my_maps.shape[1]\n",
    "tree_col = my_maps.shape[2]\n",
    "\n",
    "skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "fold = 0\n",
    "for train_index, test_index in skf.split(my_maps, labels):\n",
    "    train_x, test_x = my_maps[train_index,:,:], my_maps[test_index,:,:]\n",
    "    train_y, test_y = labels_oh[train_index,:], labels_oh[test_index,:]\n",
    "        \n",
    "    train_x = np.log(train_x + 1)\n",
    "    test_x = np.log(test_x + 1)\n",
    "        \n",
    "    c_prob = [0] * len(np.unique(labels))\n",
    "    train_weights = []\n",
    "\n",
    "    for l in np.unique(labels):\n",
    "        a = float(len(labels))\n",
    "        b = 2.0 * float((np.sum(labels==l)))\n",
    "        c_prob[int(l)] = a/b\n",
    "\n",
    "    c_prob = np.array(c_prob).reshape(-1)\n",
    "\n",
    "    for l in np.argmax(train_y, 1):\n",
    "        train_weights.append(c_prob[int(l)])\n",
    "    train_weights = np.array(train_weights)\n",
    "        \n",
    "    scaler = MinMaxScaler().fit(train_x.reshape(-1, tree_row * tree_col))\n",
    "    train_x = np.clip(scaler.transform(train_x.reshape(-1, tree_row * tree_col)), 0, 1).reshape(-1, tree_row, tree_col)\n",
    "    test_x = np.clip(scaler.transform(test_x.reshape(-1, tree_row * tree_col)), 0, 1).reshape(-1, tree_row, tree_col)\n",
    "\n",
    "    train = [train_x, train_y]\n",
    "    test = [test_x, test_y]\n",
    "\n",
    "    x_train1 = train_x\n",
    "    y_train1 = train_y\n",
    "    x_test1 = test_x\n",
    "    y_test1 = test_y\n",
    "        \n",
    "#         y_train1 = train_y\n",
    "#         y_test1 = test_y\n",
    "        \n",
    "#         x_train1 = np.zeros(train_x.shape)\n",
    "#         x_train1[train_x != 0] = 1\n",
    "        \n",
    "#         x_test1 = np.zeros(test_x.shape)\n",
    "#         x_test1[test_x != 0] = 1\n",
    "        \n",
    "        # for i in range(len(train_x)):\n",
    "        #     for j in range(len(test_x)):\n",
    "        #         if np.array_equal(train_x[i], test_x[j]):\n",
    "        #             print('train')\n",
    "        #             print(train_x[i])\n",
    "        #             print('test')\n",
    "        #             print(test_x[j])\n",
    "        \n",
    "        \n",
    "    model = ResNet(height = train_x.shape[1], width = train_x.shape[2], channels = 1, classes = 2)\n",
    "    model.init_model()\n",
    "    model.train(train_x, train_y, test_x, y_test1, dataset, use_weights = False)\n",
    "    y_pred = model.predict(test_x)\n",
    "    auc_roc, auc_pr, f1, mcc = model.evaluate(test_y, y_pred)\n",
    "    data_lst.append([auc_roc, auc_pr, f1, mcc])\n",
    "    #model.model.save_weights(path + \"/model_weights.h5\")\n",
    "    print(test_y)\n",
    "    print(y_pred)\n",
    "    print(model.model.summary())\n",
    "    \n",
    "    fold += 1\n",
    "#run += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying Accuracy Metrics and Saving Metrics\n",
    "\n",
    "Option to save results of all k folds and weights of last model into same directy as data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>auc(roc)</th>\n",
       "      <td>0.697314</td>\n",
       "      <td>0.648244</td>\n",
       "      <td>0.521447</td>\n",
       "      <td>0.693540</td>\n",
       "      <td>0.666150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc(pr)</th>\n",
       "      <td>0.693166</td>\n",
       "      <td>0.648700</td>\n",
       "      <td>0.540659</td>\n",
       "      <td>0.672859</td>\n",
       "      <td>0.649952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.620485</td>\n",
       "      <td>0.631606</td>\n",
       "      <td>0.511972</td>\n",
       "      <td>0.454052</td>\n",
       "      <td>0.472341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mcc</th>\n",
       "      <td>0.298881</td>\n",
       "      <td>0.280056</td>\n",
       "      <td>0.092727</td>\n",
       "      <td>0.186530</td>\n",
       "      <td>0.072041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 1         2         3         4         5\n",
       "auc(roc)  0.697314  0.648244  0.521447  0.693540  0.666150\n",
       "auc(pr)   0.693166  0.648700  0.540659  0.672859  0.649952\n",
       "f1        0.620485  0.631606  0.511972  0.454052  0.472341\n",
       "mcc       0.298881  0.280056  0.092727  0.186530  0.072041"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = [str(i) for i in range(1,k+1)]    \n",
    "results_df = pd.DataFrame(data_lst, columns = ['auc(roc)', 'auc(pr)', 'f1', 'mcc'])\n",
    "results_df = results_df.transpose()\n",
    "results_df.columns = col\n",
    "\n",
    "#results_df.to_csv(path + \"/results.csv\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Model Weights\n",
    "\n",
    "Option to save model weights of last model in k fold into same directy as data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.model.save_weights(path + \"/model_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
