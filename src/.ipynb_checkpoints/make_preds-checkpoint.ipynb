{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from os.path import abspath\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils.generate_network import generate_network\n",
    "from utils.prepare_data import prepare_data\n",
    "from utils.popphy_io import save_params, load_params\n",
    "from utils.popphy_io import get_stat, get_stat_dict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from models.PopPhy import PopPhyCNN\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from utils.combine_abundance import main\n",
    "\n",
    "import tensorflow as tf\n",
    "#from models.PopPhy2 import ResNet\n",
    "from models.PopPhy2 import ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "### Reading Configuration\n",
    "Configuring which data to read in, minimun threshold needed in an OTU (individual sample must have at least set threshold relative abundance), and how many k folds for k fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'skg-wt-t1'\n",
    "training_dataset = 'skg2-mtx-wt-t1'\n",
    "threshold = 0\n",
    "k = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Features\n",
    "Reduce amount of OTU features by filtering out OTUs that contain no individual sample with a relative abundance greater than the set threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, pred_data = main('skg-wt-t1', 'skg2-wt-t14', 0)\n",
    "\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of data rows\n",
    "#get list of data2 rows\n",
    "pred_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 2d Matrix Representing OTU Data\n",
    "Dai et al. PopPhy-CNN's (2019) algorithm creates Phylogenetic tree from OTUs and populates tree based on OTU abundances. This tree graph structure is then converted to a 2d Matrix by taking each parent node in the tree graph and pushing them all to the left and childrens' nodes in the same order from left to right the parents were ordered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_maps, raw_x, tree_x, raw_features, tree_features, labels, label_set, g, feature_df = prepare_data(path, data)\n",
    "\n",
    "# norms = np.linalg.norm(my_maps, axis=2, keepdims=True)\n",
    "# my_maps = my_maps / norms\n",
    "pd.DataFrame(my_maps[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating training and test sets\n",
    "Splitting data into k training and k test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding\n",
    "input = my_maps\n",
    "target = tf.keras.utils.to_categorical(labels, 2, dtype='int64')\n",
    "    \n",
    "\n",
    "# #shuffle dataset\n",
    "# seed = np.random.randint(100)\n",
    "# # np.random.seed(seed)\n",
    "# # np.random.shuffle(input)\n",
    "# np.random.seed(seed)\n",
    "# np.random.shuffle(target)\n",
    "\n",
    "# np.random.seed(seed)\n",
    "# np.random.shuffle(my_maps)\n",
    "# np.random.seed(seed)\n",
    "# np.random.shuffle(raw_x)\n",
    "# np.random.seed(seed)\n",
    "# np.random.shuffle(tree_x)\n",
    "# np.random.seed(seed)\n",
    "# np.random.shuffle(labels)\n",
    "\n",
    "\n",
    "#create k training and k test sets\n",
    "# groups_input = []\n",
    "# groups_target = []\n",
    "# k_size = len(input)//k\n",
    "# start, end = 0, k_size\n",
    "# for i in range(k):\n",
    "#     if i == k-1:\n",
    "#         group_input = input[start:]\n",
    "#         group_target = target[start:]\n",
    "#     else:\n",
    "#         group_input = input[start:end]\n",
    "#         group_target = target[start:end]\n",
    "#     start += k_size\n",
    "#     end += k_size\n",
    "#     groups_input.append(group_input)\n",
    "#     groups_target.append(group_target)\n",
    "\n",
    "# x_train = []\n",
    "# y_train = []\n",
    "# x_test = []\n",
    "# y_test = []\n",
    "# for i in range(k-1, -1, -1):\n",
    "#     x_train.append(np.concatenate((groups_input[i-1], groups_input[i-2], groups_input[i-3], groups_input[i-4])))\n",
    "#     y_train.append(np.concatenate((groups_target[i-1], groups_target[i-2], groups_target[i-3], groups_target[i-4])))\n",
    "\n",
    "#     x_test.append(groups_input[i])\n",
    "#     y_test.append(groups_target[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "### Training model\n",
    "Data is log transformed and then a MinMax transformation. Uses CNN that employs skipped residual identity blocks borrowed from the classic ResNet model then a FC Neural Network to make phenotype prediction. Model dimensions printed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lst = []\n",
    "\n",
    "# for i in range(k):\n",
    "#     x_train1 = x_train[i]\n",
    "#     y_train1 = y_train[i]\n",
    "#     x_test1 = x_test[i]\n",
    "#     y_test1 = y_test[i]\n",
    "\n",
    "#     model = ResNet(height = x_train1.shape[1], width = x_train1.shape[2], channels = 1, classes = 2)\n",
    "#     model.init_model()\n",
    "\n",
    "#     model.train(x_train1, y_train1, x_test1, y_test1, dataset, use_weights = False)\n",
    "#     y_pred = model.predict(x_test1)\n",
    "#     auc_roc, auc_pr, f1, mcc = model.evaluate(y_test1, y_pred)\n",
    "#     data_lst.append([auc_roc, auc_pr, f1, mcc])\n",
    "    \n",
    "#     #model.model.save_weights(path + \"/model_weights.h5\")\n",
    "\n",
    "#     print(y_test1)\n",
    "#     print(y_pred)\n",
    "    \n",
    "# print(model.model.summary())\n",
    "\n",
    "\n",
    "\n",
    "# n_values = np.max(labels) + 1\n",
    "# labels_oh = np.eye(n_values)[labels]\n",
    "# tree_row = my_maps.shape[1]\n",
    "# tree_col = my_maps.shape[2]\n",
    "\n",
    "# skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "# fold = 0\n",
    "# for train_index, test_index in skf.split(my_maps, labels):\n",
    "#     train_x, test_x = my_maps[train_index,:,:], my_maps[test_index,:,:]\n",
    "#     train_y, test_y = labels_oh[train_index,:], labels_oh[test_index,:]\n",
    "        \n",
    "#     train_x = np.log(train_x + 1)\n",
    "#     test_x = np.log(test_x + 1)\n",
    "        \n",
    "#     c_prob = [0] * len(np.unique(labels))\n",
    "#     train_weights = []\n",
    "\n",
    "#     for l in np.unique(labels):\n",
    "#         a = float(len(labels))\n",
    "#         b = 2.0 * float((np.sum(labels==l)))\n",
    "#         c_prob[int(l)] = a/b\n",
    "\n",
    "#     c_prob = np.array(c_prob).reshape(-1)\n",
    "\n",
    "#     for l in np.argmax(train_y, 1):\n",
    "#         train_weights.append(c_prob[int(l)])\n",
    "#     train_weights = np.array(train_weights)\n",
    "        \n",
    "#     scaler = MinMaxScaler().fit(train_x.reshape(-1, tree_row * tree_col))\n",
    "#     train_x = np.clip(scaler.transform(train_x.reshape(-1, tree_row * tree_col)), 0, 1).reshape(-1, tree_row, tree_col)\n",
    "#     test_x = np.clip(scaler.transform(test_x.reshape(-1, tree_row * tree_col)), 0, 1).reshape(-1, tree_row, tree_col)\n",
    "\n",
    "#     train = [train_x, train_y]\n",
    "#     test = [test_x, test_y]\n",
    "\n",
    "#     x_train1 = train_x\n",
    "#     y_train1 = train_y\n",
    "#     x_test1 = test_x\n",
    "#     y_test1 = test_y\n",
    "        \n",
    "#         y_train1 = train_y\n",
    "#         y_test1 = test_y\n",
    "        \n",
    "#         x_train1 = np.zeros(train_x.shape)\n",
    "#         x_train1[train_x != 0] = 1\n",
    "        \n",
    "#         x_test1 = np.zeros(test_x.shape)\n",
    "#         x_test1[test_x != 0] = 1\n",
    "        \n",
    "        # for i in range(len(train_x)):\n",
    "        #     for j in range(len(test_x)):\n",
    "        #         if np.array_equal(train_x[i], test_x[j]):\n",
    "        #             print('train')\n",
    "        #             print(train_x[i])\n",
    "        #             print('test')\n",
    "        #             print(test_x[j])\n",
    "        \n",
    "        \n",
    "#     model = ResNet(height = train_x.shape[1], width = train_x.shape[2], channels = 1, classes = 2)\n",
    "#     model.init_model()\n",
    "#     model.train(train_x, train_y, test_x, y_test1, dataset, use_weights = False)\n",
    "#     y_pred = model.predict(test_x)\n",
    "#     auc_roc, auc_pr, f1, mcc = model.evaluate(test_y, y_pred)\n",
    "#     data_lst.append([auc_roc, auc_pr, f1, mcc])\n",
    "#     #model.model.save_weights(path + \"/model_weights.h5\")\n",
    "#     print(test_y)\n",
    "#     print(y_pred)\n",
    "#     print(model.model.summary())\n",
    "    \n",
    "#     fold += 1\n",
    "#run += 1\n",
    "\n",
    "model = ResNet(height = input.shape[1], width = input.shape[2], channels = 1, classes = 2)\n",
    "model.init_model()\n",
    "model.train(input, target, input, target, dataset, use_weights = False)\n",
    "y_pred = model.predict(input)\n",
    "auc_roc, auc_pr, f1, mcc = model.evaluate(target, y_pred)\n",
    "data_lst.append([auc_roc, auc_pr, f1, mcc])\n",
    "#model.model.save_weights(path + \"/model_weights.h5\")\n",
    "print(target)\n",
    "print(y_pred)\n",
    "print(model.model.summary())\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying Accuracy Metrics and Saving Metrics\n",
    "\n",
    "Option to save results of all k folds and weights of last model into same directy as data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = [str(i) for i in range(1,k+1)]    \n",
    "results_df = pd.DataFrame(data_lst, columns = ['auc(roc)', 'auc(pr)', 'f1', 'mcc'])\n",
    "results_df = results_df.transpose()\n",
    "results_df.columns = col\n",
    "\n",
    "#results_df.to_csv(path + \"/results.csv\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Model Weights\n",
    "\n",
    "Option to save model weights of last model in k fold into same directy as data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.model.save_weights(path + \"/model_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
