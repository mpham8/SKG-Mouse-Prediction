{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from os.path import abspath\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils.generate_network import generate_network\n",
    "from utils.prepare_data import prepare_data\n",
    "from utils.popphy_io import save_params, load_params\n",
    "from utils.popphy_io import get_stat, get_stat_dict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from models.PopPhy import PopPhyCNN\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import tensorflow as tf\n",
    "#from models.PopPhy2 import ResNet\n",
    "from models.PopPhy2 import ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "### Reading Configuration\n",
    "Configuring which data to read in, minimun threshold needed in an OTU (individual sample must have at least set threshold relative abundance), and how many k folds for k fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'CDr'\n",
    "threshold = 0\n",
    "k = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Features\n",
    "Reduce amount of OTU features by filtering out OTUs that contain no individual sample with a relative abundance greater than the set threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>k__Archaea|p__Euryarchaeota|c__Methanobacteria|o__Methanobacteriales|f__Methanobacteriaceae|g__Methanobrevibacter</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k__Archaea|p__Euryarchaeota|c__Methanobacteria|o__Methanobacteriales|f__Methanobacteriaceae|g__Methanosphaera</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k__Bacteria.....</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k__Bacteria|p__Actinobacteria|c__Actinobacteria|o__Actinomycetales|f__Actinomycetaceae|g__</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k__Bacteria|p__Actinobacteria|c__Actinobacteria|o__Actinomycetales|f__Actinomycetaceae|g__Actinomyces</th>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k__Bacteria|p__Tenericutes|c__Mollicutes|o__Anaeroplasmatales|f__Anaeroplasmataceae|g__</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k__Bacteria|p__Tenericutes|c__Mollicutes|o__RF39|f__|g__</th>\n",
       "      <td>0.004988</td>\n",
       "      <td>0.000921</td>\n",
       "      <td>0.014342</td>\n",
       "      <td>0.002366</td>\n",
       "      <td>0.015264</td>\n",
       "      <td>0.009125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>0.000989</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>0.000849</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k__Bacteria|p__Tenericutes|c__RF3|o__ML615J.28|f__|g__</th>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k__Bacteria|p__Verrucomicrobia|c__Opitutae|o__.Cerasicoccales.|f__.Cerasicoccaceae.|g__</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k__Bacteria|p__Verrucomicrobia|c__Verrucomicrobiae|o__Verrucomicrobiales|f__Verrucomicrobiaceae|g__Akkermansia</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>257 rows × 115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         1         2    \\\n",
       "0                                                                        \n",
       "k__Archaea|p__Euryarchaeota|c__Methanobacteria|...  0.000000  0.000000   \n",
       "k__Archaea|p__Euryarchaeota|c__Methanobacteria|...  0.000000  0.000000   \n",
       "k__Bacteria.....                                    0.000000  0.000000   \n",
       "k__Bacteria|p__Actinobacteria|c__Actinobacteria...  0.000000  0.000000   \n",
       "k__Bacteria|p__Actinobacteria|c__Actinobacteria...  0.000068  0.000000   \n",
       "...                                                      ...       ...   \n",
       "k__Bacteria|p__Tenericutes|c__Mollicutes|o__Ana...  0.000000  0.000000   \n",
       "k__Bacteria|p__Tenericutes|c__Mollicutes|o__RF3...  0.004988  0.000921   \n",
       "k__Bacteria|p__Tenericutes|c__RF3|o__ML615J.28|...  0.000046  0.000000   \n",
       "k__Bacteria|p__Verrucomicrobia|c__Opitutae|o__....  0.000000  0.000000   \n",
       "k__Bacteria|p__Verrucomicrobia|c__Verrucomicrob...  0.000000  0.000000   \n",
       "\n",
       "                                                         3         4    \\\n",
       "0                                                                        \n",
       "k__Archaea|p__Euryarchaeota|c__Methanobacteria|...  0.000035  0.000084   \n",
       "k__Archaea|p__Euryarchaeota|c__Methanobacteria|...  0.000000  0.000000   \n",
       "k__Bacteria.....                                    0.000313  0.000000   \n",
       "k__Bacteria|p__Actinobacteria|c__Actinobacteria...  0.000000  0.000000   \n",
       "k__Bacteria|p__Actinobacteria|c__Actinobacteria...  0.000000  0.000042   \n",
       "...                                                      ...       ...   \n",
       "k__Bacteria|p__Tenericutes|c__Mollicutes|o__Ana...  0.000000  0.000000   \n",
       "k__Bacteria|p__Tenericutes|c__Mollicutes|o__RF3...  0.014342  0.002366   \n",
       "k__Bacteria|p__Tenericutes|c__RF3|o__ML615J.28|...  0.000000  0.000000   \n",
       "k__Bacteria|p__Verrucomicrobia|c__Opitutae|o__....  0.000000  0.000000   \n",
       "k__Bacteria|p__Verrucomicrobia|c__Verrucomicrob...  0.000000  0.000000   \n",
       "\n",
       "                                                         5         6    7    \\\n",
       "0                                                                             \n",
       "k__Archaea|p__Euryarchaeota|c__Methanobacteria|...  0.000943  0.000000  0.0   \n",
       "k__Archaea|p__Euryarchaeota|c__Methanobacteria|...  0.000000  0.000000  0.0   \n",
       "k__Bacteria.....                                    0.000021  0.000034  0.0   \n",
       "k__Bacteria|p__Actinobacteria|c__Actinobacteria...  0.000000  0.000000  0.0   \n",
       "k__Bacteria|p__Actinobacteria|c__Actinobacteria...  0.000172  0.000000  0.0   \n",
       "...                                                      ...       ...  ...   \n",
       "k__Bacteria|p__Tenericutes|c__Mollicutes|o__Ana...  0.000000  0.000000  0.0   \n",
       "k__Bacteria|p__Tenericutes|c__Mollicutes|o__RF3...  0.015264  0.009125  0.0   \n",
       "k__Bacteria|p__Tenericutes|c__RF3|o__ML615J.28|...  0.000064  0.000000  0.0   \n",
       "k__Bacteria|p__Verrucomicrobia|c__Opitutae|o__....  0.000000  0.000000  0.0   \n",
       "k__Bacteria|p__Verrucomicrobia|c__Verrucomicrob...  0.000515  0.000000  0.0   \n",
       "\n",
       "                                                         8         9    \\\n",
       "0                                                                        \n",
       "k__Archaea|p__Euryarchaeota|c__Methanobacteria|...  0.000000  0.000000   \n",
       "k__Archaea|p__Euryarchaeota|c__Methanobacteria|...  0.000000  0.000000   \n",
       "k__Bacteria.....                                    0.000072  0.000000   \n",
       "k__Bacteria|p__Actinobacteria|c__Actinobacteria...  0.000000  0.000000   \n",
       "k__Bacteria|p__Actinobacteria|c__Actinobacteria...  0.000024  0.000000   \n",
       "...                                                      ...       ...   \n",
       "k__Bacteria|p__Tenericutes|c__Mollicutes|o__Ana...  0.000000  0.000000   \n",
       "k__Bacteria|p__Tenericutes|c__Mollicutes|o__RF3...  0.000361  0.000989   \n",
       "k__Bacteria|p__Tenericutes|c__RF3|o__ML615J.28|...  0.000000  0.000000   \n",
       "k__Bacteria|p__Verrucomicrobia|c__Opitutae|o__....  0.000000  0.000000   \n",
       "k__Bacteria|p__Verrucomicrobia|c__Verrucomicrob...  0.000000  0.000000   \n",
       "\n",
       "                                                         10   ...       106  \\\n",
       "0                                                             ...             \n",
       "k__Archaea|p__Euryarchaeota|c__Methanobacteria|...  0.000000  ...  0.000000   \n",
       "k__Archaea|p__Euryarchaeota|c__Methanobacteria|...  0.000000  ...  0.000000   \n",
       "k__Bacteria.....                                    0.000000  ...  0.000000   \n",
       "k__Bacteria|p__Actinobacteria|c__Actinobacteria...  0.000000  ...  0.000000   \n",
       "k__Bacteria|p__Actinobacteria|c__Actinobacteria...  0.000000  ...  0.000021   \n",
       "...                                                      ...  ...       ...   \n",
       "k__Bacteria|p__Tenericutes|c__Mollicutes|o__Ana...  0.000000  ...  0.000000   \n",
       "k__Bacteria|p__Tenericutes|c__Mollicutes|o__RF3...  0.000334  ...  0.000211   \n",
       "k__Bacteria|p__Tenericutes|c__RF3|o__ML615J.28|...  0.000000  ...  0.000000   \n",
       "k__Bacteria|p__Verrucomicrobia|c__Opitutae|o__....  0.000000  ...  0.000000   \n",
       "k__Bacteria|p__Verrucomicrobia|c__Verrucomicrob...  0.000000  ...  0.000000   \n",
       "\n",
       "                                                         107       108  \\\n",
       "0                                                                        \n",
       "k__Archaea|p__Euryarchaeota|c__Methanobacteria|...  0.000000  0.000000   \n",
       "k__Archaea|p__Euryarchaeota|c__Methanobacteria|...  0.000000  0.000000   \n",
       "k__Bacteria.....                                    0.000000  0.000035   \n",
       "k__Bacteria|p__Actinobacteria|c__Actinobacteria...  0.000000  0.000000   \n",
       "k__Bacteria|p__Actinobacteria|c__Actinobacteria...  0.000239  0.000035   \n",
       "...                                                      ...       ...   \n",
       "k__Bacteria|p__Tenericutes|c__Mollicutes|o__Ana...  0.000000  0.000000   \n",
       "k__Bacteria|p__Tenericutes|c__Mollicutes|o__RF3...  0.000040  0.000565   \n",
       "k__Bacteria|p__Tenericutes|c__RF3|o__ML615J.28|...  0.000000  0.000000   \n",
       "k__Bacteria|p__Verrucomicrobia|c__Opitutae|o__....  0.000000  0.000000   \n",
       "k__Bacteria|p__Verrucomicrobia|c__Verrucomicrob...  0.000040  0.000000   \n",
       "\n",
       "                                                         109       110  \\\n",
       "0                                                                        \n",
       "k__Archaea|p__Euryarchaeota|c__Methanobacteria|...  0.000000  0.000000   \n",
       "k__Archaea|p__Euryarchaeota|c__Methanobacteria|...  0.000000  0.000000   \n",
       "k__Bacteria.....                                    0.000000  0.000000   \n",
       "k__Bacteria|p__Actinobacteria|c__Actinobacteria...  0.000000  0.000000   \n",
       "k__Bacteria|p__Actinobacteria|c__Actinobacteria...  0.000000  0.000101   \n",
       "...                                                      ...       ...   \n",
       "k__Bacteria|p__Tenericutes|c__Mollicutes|o__Ana...  0.000000  0.000000   \n",
       "k__Bacteria|p__Tenericutes|c__Mollicutes|o__RF3...  0.000849  0.000371   \n",
       "k__Bacteria|p__Tenericutes|c__RF3|o__ML615J.28|...  0.000000  0.000000   \n",
       "k__Bacteria|p__Verrucomicrobia|c__Opitutae|o__....  0.000000  0.000000   \n",
       "k__Bacteria|p__Verrucomicrobia|c__Verrucomicrob...  0.000000  0.000000   \n",
       "\n",
       "                                                         111       112  \\\n",
       "0                                                                        \n",
       "k__Archaea|p__Euryarchaeota|c__Methanobacteria|...  0.000000  0.000000   \n",
       "k__Archaea|p__Euryarchaeota|c__Methanobacteria|...  0.000000  0.000000   \n",
       "k__Bacteria.....                                    0.000000  0.000127   \n",
       "k__Bacteria|p__Actinobacteria|c__Actinobacteria...  0.000000  0.000000   \n",
       "k__Bacteria|p__Actinobacteria|c__Actinobacteria...  0.000269  0.000042   \n",
       "...                                                      ...       ...   \n",
       "k__Bacteria|p__Tenericutes|c__Mollicutes|o__Ana...  0.000000  0.000000   \n",
       "k__Bacteria|p__Tenericutes|c__Mollicutes|o__RF3...  0.000067  0.000423   \n",
       "k__Bacteria|p__Tenericutes|c__RF3|o__ML615J.28|...  0.000000  0.000000   \n",
       "k__Bacteria|p__Verrucomicrobia|c__Opitutae|o__....  0.000000  0.000000   \n",
       "k__Bacteria|p__Verrucomicrobia|c__Verrucomicrob...  0.000000  0.000000   \n",
       "\n",
       "                                                         113       114  \\\n",
       "0                                                                        \n",
       "k__Archaea|p__Euryarchaeota|c__Methanobacteria|...  0.000000  0.000000   \n",
       "k__Archaea|p__Euryarchaeota|c__Methanobacteria|...  0.000000  0.000000   \n",
       "k__Bacteria.....                                    0.000000  0.000000   \n",
       "k__Bacteria|p__Actinobacteria|c__Actinobacteria...  0.000000  0.000000   \n",
       "k__Bacteria|p__Actinobacteria|c__Actinobacteria...  0.000246  0.000000   \n",
       "...                                                      ...       ...   \n",
       "k__Bacteria|p__Tenericutes|c__Mollicutes|o__Ana...  0.000000  0.000000   \n",
       "k__Bacteria|p__Tenericutes|c__Mollicutes|o__RF3...  0.000443  0.000162   \n",
       "k__Bacteria|p__Tenericutes|c__RF3|o__ML615J.28|...  0.000000  0.000000   \n",
       "k__Bacteria|p__Verrucomicrobia|c__Opitutae|o__....  0.000000  0.000000   \n",
       "k__Bacteria|p__Verrucomicrobia|c__Verrucomicrob...  0.000000  0.000000   \n",
       "\n",
       "                                                         115  \n",
       "0                                                             \n",
       "k__Archaea|p__Euryarchaeota|c__Methanobacteria|...  0.000000  \n",
       "k__Archaea|p__Euryarchaeota|c__Methanobacteria|...  0.000000  \n",
       "k__Bacteria.....                                    0.000000  \n",
       "k__Bacteria|p__Actinobacteria|c__Actinobacteria...  0.000000  \n",
       "k__Bacteria|p__Actinobacteria|c__Actinobacteria...  0.000164  \n",
       "...                                                      ...  \n",
       "k__Bacteria|p__Tenericutes|c__Mollicutes|o__Ana...  0.000000  \n",
       "k__Bacteria|p__Tenericutes|c__Mollicutes|o__RF3...  0.000023  \n",
       "k__Bacteria|p__Tenericutes|c__RF3|o__ML615J.28|...  0.000000  \n",
       "k__Bacteria|p__Verrucomicrobia|c__Opitutae|o__....  0.000000  \n",
       "k__Bacteria|p__Verrucomicrobia|c__Verrucomicrob...  0.000000  \n",
       "\n",
       "[257 rows x 115 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../data/\" + dataset\n",
    "data = pd.read_csv(path + '/abundance.tsv', index_col=0, sep='\\t', header=None)\n",
    "to_drop = data.loc[(data < threshold).all(axis=1)]\n",
    "data = data.drop(to_drop.index)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 2d Matrix Representing OTU Data\n",
    "Dai et al. PopPhy-CNN's (2019) algorithm creates Phylogenetic tree from OTUs and populates tree based on OTU abundances. This tree graph structure is then converted to a 2d Matrix by taking each parent node in the tree graph and pushing them all to the left and childrens' nodes in the same order from left to right the parents were ordered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(257, 115)\n",
      "There are 257 raw features...\n",
      "Building tree structure...\n",
      "Found tree file...\n",
      "Populating trees...\n",
      "There are 359 tree features...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002694</td>\n",
       "      <td>0.007485</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.120929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.868891</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002694</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>0.001497</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.005412</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.120929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002694</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001474</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.001428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005297</td>\n",
       "      <td>0.088204</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>0.001175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows × 110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0         1         2         3         4         5         6         7    \\\n",
       "0  1.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.0  1.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.0  0.002694  0.007485  0.000000  0.120929  0.000000  0.868891  0.000000   \n",
       "3  0.0  0.002694  0.000392  0.001497  0.000184  0.005412  0.000000  0.120929   \n",
       "4  0.0  0.002694  0.000392  0.000000  0.001474  0.000023  0.000046  0.000000   \n",
       "5  0.0  0.002694  0.000000  0.000000  0.000392  0.000000  0.000000  0.000000   \n",
       "6  0.0  0.002694  0.000000  0.000000  0.000000  0.000392  0.000000  0.000000   \n",
       "7  0.0  0.000000  0.000000  0.000392  0.000000  0.000000  0.000000  0.000000   \n",
       "8  0.0  0.005297  0.088204  0.000000  0.000000  0.000092  0.000138  0.000000   \n",
       "\n",
       "        8         9    ...  100  101       102  103  104       105  106  107  \\\n",
       "0  0.000000  0.000000  ...  0.0  0.0  0.000000  0.0  0.0  0.000000  0.0  0.0   \n",
       "1  0.000000  0.000000  ...  0.0  0.0  0.000000  0.0  0.0  0.000000  0.0  0.0   \n",
       "2  0.000000  0.000000  ...  0.0  0.0  0.000000  0.0  0.0  0.000000  0.0  0.0   \n",
       "3  0.000000  0.000000  ...  0.0  0.0  0.000000  0.0  0.0  0.000000  0.0  0.0   \n",
       "4  0.000000  0.000000  ...  0.0  0.0  0.000000  0.0  0.0  0.000000  0.0  0.0   \n",
       "5  0.000046  0.001428  ...  0.0  0.0  0.000000  0.0  0.0  0.000000  0.0  0.0   \n",
       "6  0.000046  0.000000  ...  0.0  0.0  0.000000  0.0  0.0  0.000000  0.0  0.0   \n",
       "7  0.000000  0.005297  ...  0.0  0.0  0.000092  0.0  0.0  0.000069  0.0  0.0   \n",
       "8  0.000760  0.001175  ...  0.0  0.0  0.000000  0.0  0.0  0.000000  0.0  0.0   \n",
       "\n",
       "   108  109  \n",
       "0  0.0  0.0  \n",
       "1  0.0  0.0  \n",
       "2  0.0  0.0  \n",
       "3  0.0  0.0  \n",
       "4  0.0  0.0  \n",
       "5  0.0  0.0  \n",
       "6  0.0  0.0  \n",
       "7  0.0  0.0  \n",
       "8  0.0  0.0  \n",
       "\n",
       "[9 rows x 110 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_maps, raw_x, tree_x, raw_features, tree_features, labels, label_set, g, feature_df = prepare_data(path, data)\n",
    "\n",
    "# norms = np.linalg.norm(my_maps, axis=2, keepdims=True)\n",
    "# my_maps = my_maps / norms\n",
    "pd.DataFrame(my_maps[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating training and test sets\n",
    "Splitting data into k training and k test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding\n",
    "input = my_maps\n",
    "target = tf.keras.utils.to_categorical(labels, 2, dtype='int64')\n",
    "    \n",
    "\n",
    "#shuffle dataset\n",
    "seed = np.random.randint(100)\n",
    "# np.random.seed(seed)\n",
    "# np.random.shuffle(input)\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(target)\n",
    "\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(my_maps)\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(raw_x)\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(tree_x)\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(labels)\n",
    "\n",
    "\n",
    "#create k training and k test sets\n",
    "groups_input = []\n",
    "groups_target = []\n",
    "k_size = len(input)//k\n",
    "start, end = 0, k_size\n",
    "for i in range(k):\n",
    "    if i == k-1:\n",
    "        group_input = input[start:]\n",
    "        group_target = target[start:]\n",
    "    else:\n",
    "        group_input = input[start:end]\n",
    "        group_target = target[start:end]\n",
    "    start += k_size\n",
    "    end += k_size\n",
    "    groups_input.append(group_input)\n",
    "    groups_target.append(group_target)\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "for i in range(k-1, -1, -1):\n",
    "    x_train.append(np.concatenate((groups_input[i-1], groups_input[i-2], groups_input[i-3], groups_input[i-4])))\n",
    "    y_train.append(np.concatenate((groups_target[i-1], groups_target[i-2], groups_target[i-3], groups_target[i-4])))\n",
    "\n",
    "    x_test.append(groups_input[i])\n",
    "    y_test.append(groups_target[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "### Training model\n",
    "Data is log transformed and then a MinMax transformation. Uses CNN that employs skipped residual identity blocks borrowed from the classic ResNet model then a FC Neural Network to make phenotype prediction. Model dimensions printed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "Epoch 1/150\n",
      "16/16 [==============================] - 3s 93ms/step - loss: 2.6241 - accuracy: 0.6522 - val_loss: 2.2536 - val_accuracy: 0.6522\n",
      "Epoch 2/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 1.9390 - accuracy: 0.6739 - val_loss: 1.7595 - val_accuracy: 0.6522\n",
      "Epoch 3/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 1.4873 - accuracy: 0.7391 - val_loss: 1.4568 - val_accuracy: 0.6522\n",
      "Epoch 4/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 1.1878 - accuracy: 0.8043 - val_loss: 1.2778 - val_accuracy: 0.6522\n",
      "Epoch 5/150\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.9657 - accuracy: 0.8913 - val_loss: 1.1599 - val_accuracy: 0.6522\n",
      "Epoch 6/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.8014 - accuracy: 0.9348 - val_loss: 1.0893 - val_accuracy: 0.6522\n",
      "Epoch 7/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.6779 - accuracy: 0.9783 - val_loss: 1.0137 - val_accuracy: 0.6522\n",
      "Epoch 8/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.5817 - accuracy: 0.9891 - val_loss: 0.9499 - val_accuracy: 0.6522\n",
      "Epoch 9/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.4981 - accuracy: 1.0000 - val_loss: 0.8692 - val_accuracy: 0.8261\n",
      "Epoch 10/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.4338 - accuracy: 1.0000 - val_loss: 0.8256 - val_accuracy: 0.6957\n",
      "Epoch 11/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.3744 - accuracy: 1.0000 - val_loss: 0.7756 - val_accuracy: 0.8261\n",
      "Epoch 12/150\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.3316 - accuracy: 1.0000 - val_loss: 0.7268 - val_accuracy: 0.7391\n",
      "Epoch 13/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.2949 - accuracy: 1.0000 - val_loss: 0.6822 - val_accuracy: 0.7826\n",
      "Epoch 14/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.2658 - accuracy: 1.0000 - val_loss: 0.6438 - val_accuracy: 0.7826\n",
      "Epoch 15/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.2392 - accuracy: 1.0000 - val_loss: 0.6162 - val_accuracy: 0.7826\n",
      "Epoch 16/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.2177 - accuracy: 1.0000 - val_loss: 0.5907 - val_accuracy: 0.7826\n",
      "Epoch 17/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.1999 - accuracy: 1.0000 - val_loss: 0.5613 - val_accuracy: 0.8261\n",
      "Epoch 18/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.1826 - accuracy: 1.0000 - val_loss: 0.5449 - val_accuracy: 0.8261\n",
      "Epoch 19/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.1680 - accuracy: 1.0000 - val_loss: 0.5219 - val_accuracy: 0.8261\n",
      "Epoch 20/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.1565 - accuracy: 1.0000 - val_loss: 0.5228 - val_accuracy: 0.8261\n",
      "Epoch 21/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.1450 - accuracy: 1.0000 - val_loss: 0.4835 - val_accuracy: 0.8261\n",
      "Epoch 22/150\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.1353 - accuracy: 1.0000 - val_loss: 0.4937 - val_accuracy: 0.8261\n",
      "Epoch 23/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.1271 - accuracy: 1.0000 - val_loss: 0.4834 - val_accuracy: 0.8261\n",
      "Epoch 24/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.1192 - accuracy: 1.0000 - val_loss: 0.4627 - val_accuracy: 0.8261\n",
      "Epoch 25/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.1122 - accuracy: 1.0000 - val_loss: 0.4790 - val_accuracy: 0.8261\n",
      "Epoch 26/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.1058 - accuracy: 1.0000 - val_loss: 0.4693 - val_accuracy: 0.7826\n",
      "Epoch 27/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.1005 - accuracy: 1.0000 - val_loss: 0.4597 - val_accuracy: 0.8261\n",
      "Epoch 28/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0964 - accuracy: 1.0000 - val_loss: 0.4617 - val_accuracy: 0.8261\n",
      "Epoch 29/150\n",
      "16/16 [==============================] - 1s 29ms/step - loss: 0.0916 - accuracy: 1.0000 - val_loss: 0.4619 - val_accuracy: 0.7826\n",
      "Epoch 30/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0880 - accuracy: 1.0000 - val_loss: 0.4652 - val_accuracy: 0.8261\n",
      "Epoch 31/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0836 - accuracy: 1.0000 - val_loss: 0.4758 - val_accuracy: 0.8261\n",
      "Epoch 32/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0810 - accuracy: 1.0000 - val_loss: 0.4889 - val_accuracy: 0.8261\n",
      "Epoch 33/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0786 - accuracy: 1.0000 - val_loss: 0.4585 - val_accuracy: 0.7826\n",
      "Epoch 34/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0760 - accuracy: 1.0000 - val_loss: 0.5286 - val_accuracy: 0.8261\n",
      "Epoch 35/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0739 - accuracy: 1.0000 - val_loss: 0.4627 - val_accuracy: 0.7826\n",
      "Epoch 36/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0708 - accuracy: 1.0000 - val_loss: 0.4961 - val_accuracy: 0.7826\n",
      "Epoch 37/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0691 - accuracy: 1.0000 - val_loss: 0.5155 - val_accuracy: 0.7391\n",
      "Epoch 38/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0666 - accuracy: 1.0000 - val_loss: 0.5207 - val_accuracy: 0.7391\n",
      "Epoch 39/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0649 - accuracy: 1.0000 - val_loss: 0.5535 - val_accuracy: 0.7391\n",
      "Epoch 40/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0638 - accuracy: 1.0000 - val_loss: 0.5365 - val_accuracy: 0.7826\n",
      "Epoch 41/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0616 - accuracy: 1.0000 - val_loss: 0.5493 - val_accuracy: 0.7826\n",
      "Epoch 42/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0605 - accuracy: 1.0000 - val_loss: 0.5586 - val_accuracy: 0.7826\n",
      "Epoch 43/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0596 - accuracy: 1.0000 - val_loss: 0.5959 - val_accuracy: 0.7391\n",
      "Epoch 44/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0586 - accuracy: 1.0000 - val_loss: 0.5747 - val_accuracy: 0.7826\n",
      "Epoch 45/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0568 - accuracy: 1.0000 - val_loss: 0.6254 - val_accuracy: 0.8261\n",
      "Epoch 46/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0559 - accuracy: 1.0000 - val_loss: 0.5918 - val_accuracy: 0.7826\n",
      "Epoch 47/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0547 - accuracy: 1.0000 - val_loss: 0.6041 - val_accuracy: 0.7826\n",
      "Epoch 48/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0536 - accuracy: 1.0000 - val_loss: 0.6251 - val_accuracy: 0.7391\n",
      "Epoch 49/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0524 - accuracy: 1.0000 - val_loss: 0.6367 - val_accuracy: 0.7391\n",
      "Epoch 50/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0517 - accuracy: 1.0000 - val_loss: 0.6336 - val_accuracy: 0.7826\n",
      "Epoch 51/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0512 - accuracy: 1.0000 - val_loss: 0.6451 - val_accuracy: 0.7391\n",
      "Epoch 52/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0502 - accuracy: 1.0000 - val_loss: 0.6490 - val_accuracy: 0.7391\n",
      "Epoch 53/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0495 - accuracy: 1.0000 - val_loss: 0.6849 - val_accuracy: 0.7391\n",
      "Epoch 54/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0486 - accuracy: 1.0000 - val_loss: 0.6700 - val_accuracy: 0.7391\n",
      "Epoch 55/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0480 - accuracy: 1.0000 - val_loss: 0.6834 - val_accuracy: 0.7391\n",
      "Epoch 56/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0468 - accuracy: 1.0000 - val_loss: 0.7268 - val_accuracy: 0.7826\n",
      "Epoch 57/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0468 - accuracy: 1.0000 - val_loss: 0.6861 - val_accuracy: 0.7391\n",
      "Epoch 58/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0460 - accuracy: 1.0000 - val_loss: 0.7131 - val_accuracy: 0.7826\n",
      "Epoch 59/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0452 - accuracy: 1.0000 - val_loss: 0.6940 - val_accuracy: 0.7826\n",
      "Epoch 60/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0445 - accuracy: 1.0000 - val_loss: 0.6786 - val_accuracy: 0.7391\n",
      "Epoch 61/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0442 - accuracy: 1.0000 - val_loss: 0.6802 - val_accuracy: 0.7391\n",
      "Epoch 62/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0436 - accuracy: 1.0000 - val_loss: 0.7033 - val_accuracy: 0.7391\n",
      "Epoch 63/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0426 - accuracy: 1.0000 - val_loss: 0.7001 - val_accuracy: 0.7391\n",
      "Epoch 64/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0422 - accuracy: 1.0000 - val_loss: 0.7106 - val_accuracy: 0.7826\n",
      "Epoch 65/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0418 - accuracy: 1.0000 - val_loss: 0.7674 - val_accuracy: 0.7391\n",
      "Epoch 66/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0410 - accuracy: 1.0000 - val_loss: 0.7538 - val_accuracy: 0.7826\n",
      "Epoch 67/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0407 - accuracy: 1.0000 - val_loss: 0.7113 - val_accuracy: 0.7391\n",
      "Epoch 68/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0403 - accuracy: 1.0000 - val_loss: 0.7558 - val_accuracy: 0.7391\n",
      "Epoch 69/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0398 - accuracy: 1.0000 - val_loss: 0.7425 - val_accuracy: 0.7391\n",
      "Epoch 70/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0392 - accuracy: 1.0000 - val_loss: 0.7564 - val_accuracy: 0.7391\n",
      "Epoch 71/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0389 - accuracy: 1.0000 - val_loss: 0.7506 - val_accuracy: 0.7391\n",
      "Epoch 72/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0383 - accuracy: 1.0000 - val_loss: 0.7817 - val_accuracy: 0.7826\n",
      "Epoch 73/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0378 - accuracy: 1.0000 - val_loss: 0.7878 - val_accuracy: 0.7391\n",
      "Epoch 74/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0374 - accuracy: 1.0000 - val_loss: 0.7793 - val_accuracy: 0.7391\n",
      "Epoch 75/150\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.0373 - accuracy: 1.0000 - val_loss: 0.7660 - val_accuracy: 0.7826\n",
      "Epoch 76/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0365 - accuracy: 1.0000 - val_loss: 0.7764 - val_accuracy: 0.7391\n",
      "Epoch 77/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0364 - accuracy: 1.0000 - val_loss: 0.7856 - val_accuracy: 0.7391\n",
      "Epoch 78/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0358 - accuracy: 1.0000 - val_loss: 0.8078 - val_accuracy: 0.7826\n",
      "Epoch 79/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0358 - accuracy: 1.0000 - val_loss: 0.7531 - val_accuracy: 0.7391\n",
      "Epoch 80/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0353 - accuracy: 1.0000 - val_loss: 0.7877 - val_accuracy: 0.7391\n",
      "Epoch 81/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0350 - accuracy: 1.0000 - val_loss: 0.7828 - val_accuracy: 0.7391\n",
      "Epoch 82/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0347 - accuracy: 1.0000 - val_loss: 0.8209 - val_accuracy: 0.7391\n",
      "Epoch 83/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0345 - accuracy: 1.0000 - val_loss: 0.8342 - val_accuracy: 0.7391\n",
      "Epoch 84/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0335 - accuracy: 1.0000 - val_loss: 0.8396 - val_accuracy: 0.7391\n",
      "Epoch 85/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0334 - accuracy: 1.0000 - val_loss: 0.7834 - val_accuracy: 0.7391\n",
      "Epoch 86/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0333 - accuracy: 1.0000 - val_loss: 0.8413 - val_accuracy: 0.7391\n",
      "Epoch 87/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0331 - accuracy: 1.0000 - val_loss: 0.8323 - val_accuracy: 0.7826\n",
      "Epoch 88/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0326 - accuracy: 1.0000 - val_loss: 0.8554 - val_accuracy: 0.7391\n",
      "Epoch 89/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0323 - accuracy: 1.0000 - val_loss: 0.8486 - val_accuracy: 0.7391\n",
      "Epoch 90/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0321 - accuracy: 1.0000 - val_loss: 0.8682 - val_accuracy: 0.7391\n",
      "Epoch 91/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0316 - accuracy: 1.0000 - val_loss: 0.8783 - val_accuracy: 0.7391\n",
      "Epoch 92/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0315 - accuracy: 1.0000 - val_loss: 0.8639 - val_accuracy: 0.6957\n",
      "Epoch 93/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0312 - accuracy: 1.0000 - val_loss: 0.8491 - val_accuracy: 0.7391\n",
      "Epoch 94/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0311 - accuracy: 1.0000 - val_loss: 0.8728 - val_accuracy: 0.7391\n",
      "Epoch 95/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0306 - accuracy: 1.0000 - val_loss: 0.8332 - val_accuracy: 0.7391\n",
      "Epoch 96/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0303 - accuracy: 1.0000 - val_loss: 0.8680 - val_accuracy: 0.7391\n",
      "Epoch 97/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0302 - accuracy: 1.0000 - val_loss: 0.8715 - val_accuracy: 0.7391\n",
      "Epoch 98/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0301 - accuracy: 1.0000 - val_loss: 0.9191 - val_accuracy: 0.6957\n",
      "Epoch 99/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0297 - accuracy: 1.0000 - val_loss: 0.9235 - val_accuracy: 0.7391\n",
      "Epoch 100/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0294 - accuracy: 1.0000 - val_loss: 0.9426 - val_accuracy: 0.6957\n",
      "Epoch 101/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0294 - accuracy: 1.0000 - val_loss: 0.9164 - val_accuracy: 0.7391\n",
      "Epoch 102/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0287 - accuracy: 1.0000 - val_loss: 0.8941 - val_accuracy: 0.7391\n",
      "Epoch 103/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0287 - accuracy: 1.0000 - val_loss: 0.8914 - val_accuracy: 0.7391\n",
      "Epoch 104/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0285 - accuracy: 1.0000 - val_loss: 0.9034 - val_accuracy: 0.7391\n",
      "Epoch 105/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0283 - accuracy: 1.0000 - val_loss: 0.9123 - val_accuracy: 0.6957\n",
      "Epoch 106/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0280 - accuracy: 1.0000 - val_loss: 0.9500 - val_accuracy: 0.7391\n",
      "Epoch 107/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0281 - accuracy: 1.0000 - val_loss: 0.9342 - val_accuracy: 0.7391\n",
      "Epoch 108/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0274 - accuracy: 1.0000 - val_loss: 0.9889 - val_accuracy: 0.6957\n",
      "Epoch 109/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0273 - accuracy: 1.0000 - val_loss: 0.9580 - val_accuracy: 0.6957\n",
      "Epoch 110/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0273 - accuracy: 1.0000 - val_loss: 0.9087 - val_accuracy: 0.7826\n",
      "Epoch 111/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0270 - accuracy: 1.0000 - val_loss: 0.9393 - val_accuracy: 0.6957\n",
      "Epoch 112/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0267 - accuracy: 1.0000 - val_loss: 0.9050 - val_accuracy: 0.7391\n",
      "Epoch 113/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0265 - accuracy: 1.0000 - val_loss: 0.8818 - val_accuracy: 0.7391\n",
      "Epoch 114/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0264 - accuracy: 1.0000 - val_loss: 0.9363 - val_accuracy: 0.6957\n",
      "Epoch 115/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0262 - accuracy: 1.0000 - val_loss: 0.9244 - val_accuracy: 0.7391\n",
      "Epoch 116/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0260 - accuracy: 1.0000 - val_loss: 0.9706 - val_accuracy: 0.6957\n",
      "Epoch 117/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0259 - accuracy: 1.0000 - val_loss: 0.9441 - val_accuracy: 0.7391\n",
      "Epoch 118/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0257 - accuracy: 1.0000 - val_loss: 0.9569 - val_accuracy: 0.6957\n",
      "Epoch 119/150\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.0254 - accuracy: 1.0000 - val_loss: 0.9873 - val_accuracy: 0.6957\n",
      "Epoch 120/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0254 - accuracy: 1.0000 - val_loss: 0.9391 - val_accuracy: 0.7391\n",
      "Epoch 121/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0249 - accuracy: 1.0000 - val_loss: 1.0231 - val_accuracy: 0.6957\n",
      "Epoch 122/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0249 - accuracy: 1.0000 - val_loss: 0.9895 - val_accuracy: 0.6957\n",
      "Epoch 123/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0250 - accuracy: 1.0000 - val_loss: 0.9264 - val_accuracy: 0.7391\n",
      "Epoch 124/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0245 - accuracy: 1.0000 - val_loss: 1.0460 - val_accuracy: 0.6522\n",
      "Epoch 125/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0245 - accuracy: 1.0000 - val_loss: 0.9465 - val_accuracy: 0.6957\n",
      "Epoch 126/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0242 - accuracy: 1.0000 - val_loss: 0.9609 - val_accuracy: 0.6957\n",
      "Epoch 127/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0241 - accuracy: 1.0000 - val_loss: 1.1098 - val_accuracy: 0.6957\n",
      "Epoch 128/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0241 - accuracy: 1.0000 - val_loss: 0.9512 - val_accuracy: 0.7391\n",
      "Epoch 129/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0240 - accuracy: 1.0000 - val_loss: 0.9902 - val_accuracy: 0.7391\n",
      "Epoch 130/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0236 - accuracy: 1.0000 - val_loss: 0.9919 - val_accuracy: 0.6957\n",
      "Epoch 131/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0233 - accuracy: 1.0000 - val_loss: 0.9634 - val_accuracy: 0.6957\n",
      "Epoch 132/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0233 - accuracy: 1.0000 - val_loss: 1.0620 - val_accuracy: 0.6957\n",
      "Epoch 133/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0232 - accuracy: 1.0000 - val_loss: 1.0104 - val_accuracy: 0.6957\n",
      "Epoch 134/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0231 - accuracy: 1.0000 - val_loss: 1.0206 - val_accuracy: 0.6957\n",
      "Epoch 135/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0228 - accuracy: 1.0000 - val_loss: 0.9371 - val_accuracy: 0.7826\n",
      "Epoch 136/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0228 - accuracy: 1.0000 - val_loss: 1.0296 - val_accuracy: 0.6957\n",
      "Epoch 137/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0225 - accuracy: 1.0000 - val_loss: 1.0110 - val_accuracy: 0.6957\n",
      "Epoch 138/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0224 - accuracy: 1.0000 - val_loss: 1.0277 - val_accuracy: 0.6957\n",
      "Epoch 139/150\n",
      "16/16 [==============================] - 1s 38ms/step - loss: 0.0222 - accuracy: 1.0000 - val_loss: 1.0357 - val_accuracy: 0.6957\n",
      "Epoch 140/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0222 - accuracy: 1.0000 - val_loss: 1.0565 - val_accuracy: 0.6522\n",
      "Epoch 141/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0221 - accuracy: 1.0000 - val_loss: 1.0521 - val_accuracy: 0.6522\n",
      "Epoch 142/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0218 - accuracy: 1.0000 - val_loss: 1.0329 - val_accuracy: 0.6522\n",
      "Epoch 143/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0219 - accuracy: 1.0000 - val_loss: 1.0750 - val_accuracy: 0.6957\n",
      "Epoch 144/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0216 - accuracy: 1.0000 - val_loss: 1.1212 - val_accuracy: 0.6522\n",
      "Epoch 145/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0215 - accuracy: 1.0000 - val_loss: 1.0266 - val_accuracy: 0.6957\n",
      "Epoch 146/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0213 - accuracy: 1.0000 - val_loss: 1.0296 - val_accuracy: 0.6522\n",
      "Epoch 147/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0213 - accuracy: 1.0000 - val_loss: 1.0881 - val_accuracy: 0.6522\n",
      "Epoch 148/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0211 - accuracy: 1.0000 - val_loss: 1.0995 - val_accuracy: 0.6957\n",
      "Epoch 149/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0211 - accuracy: 1.0000 - val_loss: 1.1357 - val_accuracy: 0.6957\n",
      "Epoch 150/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0208 - accuracy: 1.0000 - val_loss: 1.0536 - val_accuracy: 0.6957\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "auc_roc: 0.7333333333333334\n",
      "auc_pr: 0.7550499065204948\n",
      "f1_score: 0.69041608228144\n",
      "mcc: 0.3105295017040594\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "[[9.9667633e-01 3.3236681e-03]\n",
      " [1.6555088e-02 9.8344487e-01]\n",
      " [1.1856496e-04 9.9988139e-01]\n",
      " [1.5948202e-02 9.8405182e-01]\n",
      " [9.9969029e-01 3.0969651e-04]\n",
      " [9.9412529e-05 9.9990058e-01]\n",
      " [9.9962109e-01 3.7889462e-04]\n",
      " [6.0707138e-05 9.9993932e-01]\n",
      " [6.3490275e-05 9.9993646e-01]\n",
      " [9.9973649e-01 2.6348064e-04]\n",
      " [9.9976164e-01 2.3840787e-04]\n",
      " [1.2249084e-01 8.7750912e-01]\n",
      " [7.3273441e-05 9.9992669e-01]\n",
      " [8.2120560e-02 9.1787940e-01]\n",
      " [8.4553132e-05 9.9991548e-01]\n",
      " [9.7097719e-01 2.9022766e-02]\n",
      " [4.1214251e-03 9.9587858e-01]\n",
      " [5.1379851e-05 9.9994862e-01]\n",
      " [4.9878410e-05 9.9995017e-01]\n",
      " [3.0981803e-03 9.9690187e-01]\n",
      " [9.9896324e-01 1.0367936e-03]\n",
      " [4.8531678e-05 9.9995148e-01]\n",
      " [5.2103784e-05 9.9994791e-01]]\n",
      "Model: \"res\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 9, 110, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " res2a-conv1 (Conv2D)           (None, 9, 110, 64)   576         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " res2a-batchnorm1 (BatchNormali  (None, 9, 110, 64)  256         ['res2a-conv1[0][0]']            \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 9, 110, 64)   0           ['res2a-batchnorm1[0][0]']       \n",
      "                                                                                                  \n",
      " res2a-conv2 (Conv2D)           (None, 9, 110, 64)   32832       ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " res2a-batchnorm2 (BatchNormali  (None, 9, 110, 64)  256         ['res2a-conv2[0][0]']            \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 9, 110, 64)   0           ['res2a-batchnorm2[0][0]',       \n",
      "                                                                  'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 9, 110, 64)   0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " res2b-conv1 (Conv2D)           (None, 9, 110, 64)   32832       ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " res2b-batchnorm1 (BatchNormali  (None, 9, 110, 64)  256         ['res2b-conv1[0][0]']            \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 9, 110, 64)   0           ['res2b-batchnorm1[0][0]']       \n",
      "                                                                                                  \n",
      " res2b-conv2 (Conv2D)           (None, 9, 110, 64)   32832       ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " res2b-batchnorm2 (BatchNormali  (None, 9, 110, 64)  256         ['res2b-conv2[0][0]']            \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 9, 110, 64)   0           ['res2b-batchnorm2[0][0]',       \n",
      "                                                                  'activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 9, 110, 64)   0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2 (Conv2D)                 (None, 9, 110, 1)    65          ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 990)          0           ['conv2[0][0]']                  \n",
      "                                                                                                  \n",
      " fc (Dense)                     (None, 100)          99100       ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 100)          0           ['fc[0][0]']                     \n",
      "                                                                                                  \n",
      " fc2 (Dense)                    (None, 32)           3232        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " softmax (Dense)                (None, 2)            66          ['fc2[0][0]']                    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 202,559\n",
      "Trainable params: 202,047\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "16/16 [==============================] - 2s 55ms/step - loss: 2.6124 - accuracy: 0.6413 - val_loss: 2.2045 - val_accuracy: 0.6522\n",
      "Epoch 2/150\n",
      "16/16 [==============================] - 1s 37ms/step - loss: 1.9151 - accuracy: 0.6739 - val_loss: 1.6953 - val_accuracy: 0.6522\n",
      "Epoch 3/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 1.4659 - accuracy: 0.6739 - val_loss: 1.3898 - val_accuracy: 0.6522\n",
      "Epoch 4/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 1.1602 - accuracy: 0.8152 - val_loss: 1.2238 - val_accuracy: 0.6522\n",
      "Epoch 5/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.9391 - accuracy: 0.8913 - val_loss: 1.1198 - val_accuracy: 0.6522\n",
      "Epoch 6/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.7476 - accuracy: 0.9457 - val_loss: 1.0362 - val_accuracy: 0.6522\n",
      "Epoch 7/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.6384 - accuracy: 0.9783 - val_loss: 0.9975 - val_accuracy: 0.7391\n",
      "Epoch 8/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.5270 - accuracy: 0.9891 - val_loss: 1.0136 - val_accuracy: 0.6087\n",
      "Epoch 9/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.4487 - accuracy: 0.9891 - val_loss: 0.9345 - val_accuracy: 0.7391\n",
      "Epoch 10/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.3922 - accuracy: 0.9891 - val_loss: 0.9404 - val_accuracy: 0.5217\n",
      "Epoch 11/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.3462 - accuracy: 0.9891 - val_loss: 0.9425 - val_accuracy: 0.4348\n",
      "Epoch 12/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.3055 - accuracy: 1.0000 - val_loss: 0.9640 - val_accuracy: 0.3478\n",
      "Epoch 13/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.2736 - accuracy: 1.0000 - val_loss: 1.0276 - val_accuracy: 0.3478\n",
      "Epoch 14/150\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.2439 - accuracy: 1.0000 - val_loss: 1.0863 - val_accuracy: 0.3478\n",
      "Epoch 15/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.2213 - accuracy: 1.0000 - val_loss: 1.1287 - val_accuracy: 0.3478\n",
      "Epoch 16/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.2010 - accuracy: 1.0000 - val_loss: 1.0260 - val_accuracy: 0.3478\n",
      "Epoch 17/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.1831 - accuracy: 1.0000 - val_loss: 1.0290 - val_accuracy: 0.3478\n",
      "Epoch 18/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.1664 - accuracy: 1.0000 - val_loss: 1.1169 - val_accuracy: 0.3478\n",
      "Epoch 19/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.1556 - accuracy: 1.0000 - val_loss: 1.0951 - val_accuracy: 0.3478\n",
      "Epoch 20/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.1453 - accuracy: 1.0000 - val_loss: 1.0363 - val_accuracy: 0.3478\n",
      "Epoch 21/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.1349 - accuracy: 1.0000 - val_loss: 1.1056 - val_accuracy: 0.3478\n",
      "Epoch 22/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.1263 - accuracy: 1.0000 - val_loss: 1.1652 - val_accuracy: 0.3478\n",
      "Epoch 23/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.1198 - accuracy: 1.0000 - val_loss: 1.1406 - val_accuracy: 0.3478\n",
      "Epoch 24/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.1113 - accuracy: 1.0000 - val_loss: 1.1171 - val_accuracy: 0.3913\n",
      "Epoch 25/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.1056 - accuracy: 1.0000 - val_loss: 0.8605 - val_accuracy: 0.5217\n",
      "Epoch 26/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0997 - accuracy: 1.0000 - val_loss: 0.9212 - val_accuracy: 0.5217\n",
      "Epoch 27/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0952 - accuracy: 1.0000 - val_loss: 0.9299 - val_accuracy: 0.4783\n",
      "Epoch 28/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0898 - accuracy: 1.0000 - val_loss: 0.8484 - val_accuracy: 0.5652\n",
      "Epoch 29/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0877 - accuracy: 1.0000 - val_loss: 0.6834 - val_accuracy: 0.6957\n",
      "Epoch 30/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0844 - accuracy: 1.0000 - val_loss: 0.7491 - val_accuracy: 0.6957\n",
      "Epoch 31/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0811 - accuracy: 1.0000 - val_loss: 0.6059 - val_accuracy: 0.7826\n",
      "Epoch 32/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0788 - accuracy: 1.0000 - val_loss: 0.6069 - val_accuracy: 0.7826\n",
      "Epoch 33/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0753 - accuracy: 1.0000 - val_loss: 0.5010 - val_accuracy: 0.7826\n",
      "Epoch 34/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0735 - accuracy: 1.0000 - val_loss: 0.4754 - val_accuracy: 0.8261\n",
      "Epoch 35/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0709 - accuracy: 1.0000 - val_loss: 0.5109 - val_accuracy: 0.8261\n",
      "Epoch 36/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0686 - accuracy: 1.0000 - val_loss: 0.4794 - val_accuracy: 0.8261\n",
      "Epoch 37/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0672 - accuracy: 1.0000 - val_loss: 0.4318 - val_accuracy: 0.8696\n",
      "Epoch 38/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0660 - accuracy: 1.0000 - val_loss: 0.4539 - val_accuracy: 0.7826\n",
      "Epoch 39/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0636 - accuracy: 1.0000 - val_loss: 0.4051 - val_accuracy: 0.8261\n",
      "Epoch 40/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0627 - accuracy: 1.0000 - val_loss: 0.4331 - val_accuracy: 0.7391\n",
      "Epoch 41/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0609 - accuracy: 1.0000 - val_loss: 0.4069 - val_accuracy: 0.8696\n",
      "Epoch 42/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0597 - accuracy: 1.0000 - val_loss: 0.3937 - val_accuracy: 0.8261\n",
      "Epoch 43/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0583 - accuracy: 1.0000 - val_loss: 0.4082 - val_accuracy: 0.8261\n",
      "Epoch 44/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0581 - accuracy: 1.0000 - val_loss: 0.4734 - val_accuracy: 0.7826\n",
      "Epoch 45/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0581 - accuracy: 1.0000 - val_loss: 0.4151 - val_accuracy: 0.8261\n",
      "Epoch 46/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0586 - accuracy: 1.0000 - val_loss: 0.4449 - val_accuracy: 0.8261\n",
      "Epoch 47/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0574 - accuracy: 1.0000 - val_loss: 0.4532 - val_accuracy: 0.8261\n",
      "Epoch 48/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0549 - accuracy: 1.0000 - val_loss: 0.3963 - val_accuracy: 0.8696\n",
      "Epoch 49/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0532 - accuracy: 1.0000 - val_loss: 0.4482 - val_accuracy: 0.8261\n",
      "Epoch 50/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0538 - accuracy: 1.0000 - val_loss: 0.5259 - val_accuracy: 0.8261\n",
      "Epoch 51/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0535 - accuracy: 1.0000 - val_loss: 0.4432 - val_accuracy: 0.8261\n",
      "Epoch 52/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0568 - accuracy: 1.0000 - val_loss: 0.4943 - val_accuracy: 0.8696\n",
      "Epoch 53/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.1994 - accuracy: 0.9565 - val_loss: 1.1858 - val_accuracy: 0.6522\n",
      "Epoch 54/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.6160 - accuracy: 0.8370 - val_loss: 0.7553 - val_accuracy: 0.7391\n",
      "Epoch 55/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.5415 - accuracy: 0.8804 - val_loss: 0.6883 - val_accuracy: 0.7826\n",
      "Epoch 56/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.4316 - accuracy: 0.9457 - val_loss: 0.7062 - val_accuracy: 0.7826\n",
      "Epoch 57/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.3584 - accuracy: 0.9674 - val_loss: 0.6895 - val_accuracy: 0.8261\n",
      "Epoch 58/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.2832 - accuracy: 1.0000 - val_loss: 0.7367 - val_accuracy: 0.8261\n",
      "Epoch 59/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.2395 - accuracy: 1.0000 - val_loss: 0.6500 - val_accuracy: 0.7391\n",
      "Epoch 60/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.2049 - accuracy: 1.0000 - val_loss: 0.6385 - val_accuracy: 0.7391\n",
      "Epoch 61/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.1766 - accuracy: 1.0000 - val_loss: 0.6313 - val_accuracy: 0.7826\n",
      "Epoch 62/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.1546 - accuracy: 1.0000 - val_loss: 0.6158 - val_accuracy: 0.8261\n",
      "Epoch 63/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.1374 - accuracy: 1.0000 - val_loss: 0.5952 - val_accuracy: 0.8261\n",
      "Epoch 64/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.1234 - accuracy: 1.0000 - val_loss: 0.5684 - val_accuracy: 0.8261\n",
      "Epoch 65/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.1114 - accuracy: 1.0000 - val_loss: 0.5614 - val_accuracy: 0.8261\n",
      "Epoch 66/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.1019 - accuracy: 1.0000 - val_loss: 0.5511 - val_accuracy: 0.8261\n",
      "Epoch 67/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0933 - accuracy: 1.0000 - val_loss: 0.5472 - val_accuracy: 0.8261\n",
      "Epoch 68/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0867 - accuracy: 1.0000 - val_loss: 0.5325 - val_accuracy: 0.8261\n",
      "Epoch 69/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0804 - accuracy: 1.0000 - val_loss: 0.5278 - val_accuracy: 0.8261\n",
      "Epoch 70/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0759 - accuracy: 1.0000 - val_loss: 0.5252 - val_accuracy: 0.8261\n",
      "Epoch 71/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0709 - accuracy: 1.0000 - val_loss: 0.5167 - val_accuracy: 0.8261\n",
      "Epoch 72/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0675 - accuracy: 1.0000 - val_loss: 0.5115 - val_accuracy: 0.8261\n",
      "Epoch 73/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0642 - accuracy: 1.0000 - val_loss: 0.5218 - val_accuracy: 0.8261\n",
      "Epoch 74/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0607 - accuracy: 1.0000 - val_loss: 0.5075 - val_accuracy: 0.8261\n",
      "Epoch 75/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0583 - accuracy: 1.0000 - val_loss: 0.5201 - val_accuracy: 0.8261\n",
      "Epoch 76/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0558 - accuracy: 1.0000 - val_loss: 0.5128 - val_accuracy: 0.8261\n",
      "Epoch 77/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0540 - accuracy: 1.0000 - val_loss: 0.5216 - val_accuracy: 0.8261\n",
      "Epoch 78/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0521 - accuracy: 1.0000 - val_loss: 0.5209 - val_accuracy: 0.8261\n",
      "Epoch 79/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0502 - accuracy: 1.0000 - val_loss: 0.5262 - val_accuracy: 0.8261\n",
      "Epoch 80/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0490 - accuracy: 1.0000 - val_loss: 0.5162 - val_accuracy: 0.8261\n",
      "Epoch 81/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0473 - accuracy: 1.0000 - val_loss: 0.5217 - val_accuracy: 0.8261\n",
      "Epoch 82/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0463 - accuracy: 1.0000 - val_loss: 0.5114 - val_accuracy: 0.8261\n",
      "Epoch 83/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0452 - accuracy: 1.0000 - val_loss: 0.5085 - val_accuracy: 0.8261\n",
      "Epoch 84/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0438 - accuracy: 1.0000 - val_loss: 0.5102 - val_accuracy: 0.8261\n",
      "Epoch 85/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0434 - accuracy: 1.0000 - val_loss: 0.5193 - val_accuracy: 0.8261\n",
      "Epoch 86/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0424 - accuracy: 1.0000 - val_loss: 0.5316 - val_accuracy: 0.8261\n",
      "Epoch 87/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0416 - accuracy: 1.0000 - val_loss: 0.5151 - val_accuracy: 0.8261\n",
      "Epoch 88/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0413 - accuracy: 1.0000 - val_loss: 0.5346 - val_accuracy: 0.8261\n",
      "Epoch 89/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0402 - accuracy: 1.0000 - val_loss: 0.5360 - val_accuracy: 0.8261\n",
      "Epoch 90/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0397 - accuracy: 1.0000 - val_loss: 0.5307 - val_accuracy: 0.8261\n",
      "Epoch 91/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0390 - accuracy: 1.0000 - val_loss: 0.5237 - val_accuracy: 0.8261\n",
      "Epoch 92/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0384 - accuracy: 1.0000 - val_loss: 0.5199 - val_accuracy: 0.8261\n",
      "Epoch 93/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0383 - accuracy: 1.0000 - val_loss: 0.5165 - val_accuracy: 0.8261\n",
      "Epoch 94/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0375 - accuracy: 1.0000 - val_loss: 0.5294 - val_accuracy: 0.8261\n",
      "Epoch 95/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0367 - accuracy: 1.0000 - val_loss: 0.5243 - val_accuracy: 0.8261\n",
      "Epoch 96/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0366 - accuracy: 1.0000 - val_loss: 0.5074 - val_accuracy: 0.8261\n",
      "Epoch 97/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0357 - accuracy: 1.0000 - val_loss: 0.5245 - val_accuracy: 0.8261\n",
      "Epoch 98/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0357 - accuracy: 1.0000 - val_loss: 0.5115 - val_accuracy: 0.8261\n",
      "Epoch 99/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.0351 - accuracy: 1.0000 - val_loss: 0.5195 - val_accuracy: 0.8261\n",
      "Epoch 100/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0350 - accuracy: 1.0000 - val_loss: 0.5187 - val_accuracy: 0.8261\n",
      "Epoch 101/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0344 - accuracy: 1.0000 - val_loss: 0.5135 - val_accuracy: 0.8261\n",
      "Epoch 102/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0342 - accuracy: 1.0000 - val_loss: 0.5078 - val_accuracy: 0.8261\n",
      "Epoch 103/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.0338 - accuracy: 1.0000 - val_loss: 0.5051 - val_accuracy: 0.8261\n",
      "Epoch 104/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0337 - accuracy: 1.0000 - val_loss: 0.5290 - val_accuracy: 0.8261\n",
      "Epoch 105/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0333 - accuracy: 1.0000 - val_loss: 0.5137 - val_accuracy: 0.8261\n",
      "Epoch 106/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0333 - accuracy: 1.0000 - val_loss: 0.4904 - val_accuracy: 0.8696\n",
      "Epoch 107/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0325 - accuracy: 1.0000 - val_loss: 0.5011 - val_accuracy: 0.8696\n",
      "Epoch 108/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0324 - accuracy: 1.0000 - val_loss: 0.5055 - val_accuracy: 0.8696\n",
      "Epoch 109/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0318 - accuracy: 1.0000 - val_loss: 0.5068 - val_accuracy: 0.8261\n",
      "Epoch 110/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0319 - accuracy: 1.0000 - val_loss: 0.5109 - val_accuracy: 0.8696\n",
      "Epoch 111/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0314 - accuracy: 1.0000 - val_loss: 0.5205 - val_accuracy: 0.8261\n",
      "Epoch 112/150\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.0312 - accuracy: 1.0000 - val_loss: 0.5187 - val_accuracy: 0.8261\n",
      "Epoch 113/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0312 - accuracy: 1.0000 - val_loss: 0.5313 - val_accuracy: 0.8261\n",
      "Epoch 114/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0302 - accuracy: 1.0000 - val_loss: 0.5229 - val_accuracy: 0.8261\n",
      "Epoch 115/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0303 - accuracy: 1.0000 - val_loss: 0.5402 - val_accuracy: 0.8261\n",
      "Epoch 116/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0309 - accuracy: 1.0000 - val_loss: 0.5182 - val_accuracy: 0.8696\n",
      "Epoch 117/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0304 - accuracy: 1.0000 - val_loss: 0.5171 - val_accuracy: 0.8696\n",
      "Epoch 118/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0299 - accuracy: 1.0000 - val_loss: 0.5254 - val_accuracy: 0.8261\n",
      "Epoch 119/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0301 - accuracy: 1.0000 - val_loss: 0.5199 - val_accuracy: 0.8261\n",
      "Epoch 120/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0295 - accuracy: 1.0000 - val_loss: 0.5085 - val_accuracy: 0.8261\n",
      "Epoch 121/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0293 - accuracy: 1.0000 - val_loss: 0.5137 - val_accuracy: 0.8261\n",
      "Epoch 122/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0294 - accuracy: 1.0000 - val_loss: 0.5115 - val_accuracy: 0.8261\n",
      "Epoch 123/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0288 - accuracy: 1.0000 - val_loss: 0.5185 - val_accuracy: 0.8261\n",
      "Epoch 124/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0289 - accuracy: 1.0000 - val_loss: 0.5089 - val_accuracy: 0.8696\n",
      "Epoch 125/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0285 - accuracy: 1.0000 - val_loss: 0.5159 - val_accuracy: 0.8261\n",
      "Epoch 126/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0285 - accuracy: 1.0000 - val_loss: 0.5065 - val_accuracy: 0.8696\n",
      "Epoch 127/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0283 - accuracy: 1.0000 - val_loss: 0.5038 - val_accuracy: 0.8696\n",
      "Epoch 128/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0278 - accuracy: 1.0000 - val_loss: 0.4987 - val_accuracy: 0.8261\n",
      "Epoch 129/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0274 - accuracy: 1.0000 - val_loss: 0.5015 - val_accuracy: 0.8696\n",
      "Epoch 130/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0278 - accuracy: 1.0000 - val_loss: 0.5104 - val_accuracy: 0.8696\n",
      "Epoch 131/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0270 - accuracy: 1.0000 - val_loss: 0.5078 - val_accuracy: 0.8696\n",
      "Epoch 132/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0273 - accuracy: 1.0000 - val_loss: 0.5040 - val_accuracy: 0.8696\n",
      "Epoch 133/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0271 - accuracy: 1.0000 - val_loss: 0.5033 - val_accuracy: 0.8696\n",
      "Epoch 134/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0276 - accuracy: 1.0000 - val_loss: 0.5470 - val_accuracy: 0.8261\n",
      "Epoch 135/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0273 - accuracy: 1.0000 - val_loss: 0.5329 - val_accuracy: 0.8696\n",
      "Epoch 136/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0268 - accuracy: 1.0000 - val_loss: 0.5232 - val_accuracy: 0.8696\n",
      "Epoch 137/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0263 - accuracy: 1.0000 - val_loss: 0.5351 - val_accuracy: 0.8261\n",
      "Epoch 138/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0258 - accuracy: 1.0000 - val_loss: 0.5276 - val_accuracy: 0.8261\n",
      "Epoch 139/150\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.0258 - accuracy: 1.0000 - val_loss: 0.5224 - val_accuracy: 0.8696\n",
      "Epoch 140/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0259 - accuracy: 1.0000 - val_loss: 0.5391 - val_accuracy: 0.8261\n",
      "Epoch 141/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0256 - accuracy: 1.0000 - val_loss: 0.5369 - val_accuracy: 0.8261\n",
      "Epoch 142/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0252 - accuracy: 1.0000 - val_loss: 0.5318 - val_accuracy: 0.8261\n",
      "Epoch 143/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0253 - accuracy: 1.0000 - val_loss: 0.5373 - val_accuracy: 0.8261\n",
      "Epoch 144/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0252 - accuracy: 1.0000 - val_loss: 0.5408 - val_accuracy: 0.8261\n",
      "Epoch 145/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.0247 - accuracy: 1.0000 - val_loss: 0.5185 - val_accuracy: 0.8696\n",
      "Epoch 146/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0246 - accuracy: 1.0000 - val_loss: 0.5318 - val_accuracy: 0.8261\n",
      "Epoch 147/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0244 - accuracy: 1.0000 - val_loss: 0.5366 - val_accuracy: 0.8261\n",
      "Epoch 148/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0246 - accuracy: 1.0000 - val_loss: 0.5129 - val_accuracy: 0.8696\n",
      "Epoch 149/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0243 - accuracy: 1.0000 - val_loss: 0.5378 - val_accuracy: 0.8261\n",
      "Epoch 150/150\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0243 - accuracy: 1.0000 - val_loss: 0.5205 - val_accuracy: 0.8261\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "auc_roc: 0.9333333333333333\n",
      "auc_pr: 0.9292306523885471\n",
      "f1_score: 0.8190993788819875\n",
      "mcc: 0.6055975280770818\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "[[7.8263496e-05 9.9992168e-01]\n",
      " [6.5998989e-05 9.9993396e-01]\n",
      " [4.7812206e-01 5.2187800e-01]\n",
      " [6.4355976e-05 9.9993563e-01]\n",
      " [9.9970067e-01 2.9941113e-04]\n",
      " [4.4200365e-03 9.9557990e-01]\n",
      " [9.9966908e-01 3.3095072e-04]\n",
      " [5.7225343e-04 9.9942780e-01]\n",
      " [2.0181986e-04 9.9979824e-01]\n",
      " [6.1389415e-05 9.9993861e-01]\n",
      " [6.1785147e-05 9.9993825e-01]\n",
      " [4.2726539e-04 9.9957269e-01]\n",
      " [5.1453238e-04 9.9948555e-01]\n",
      " [9.9404836e-01 5.9516435e-03]\n",
      " [6.1818144e-05 9.9993813e-01]\n",
      " [9.9943399e-01 5.6606502e-04]\n",
      " [8.2857012e-05 9.9991715e-01]\n",
      " [6.3962209e-05 9.9993598e-01]\n",
      " [2.2919622e-04 9.9977082e-01]\n",
      " [9.9974018e-01 2.5975073e-04]\n",
      " [9.5011842e-01 4.9881592e-02]\n",
      " [7.9043486e-05 9.9992096e-01]\n",
      " [5.2434321e-02 9.4756573e-01]]\n",
      "Model: \"res\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 9, 110, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " res2a-conv1 (Conv2D)           (None, 9, 110, 64)   576         ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " res2a-batchnorm1 (BatchNormali  (None, 9, 110, 64)  256         ['res2a-conv1[0][0]']            \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 9, 110, 64)   0           ['res2a-batchnorm1[0][0]']       \n",
      "                                                                                                  \n",
      " res2a-conv2 (Conv2D)           (None, 9, 110, 64)   32832       ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " res2a-batchnorm2 (BatchNormali  (None, 9, 110, 64)  256         ['res2a-conv2[0][0]']            \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 9, 110, 64)   0           ['res2a-batchnorm2[0][0]',       \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 9, 110, 64)   0           ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " res2b-conv1 (Conv2D)           (None, 9, 110, 64)   32832       ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " res2b-batchnorm1 (BatchNormali  (None, 9, 110, 64)  256         ['res2b-conv1[0][0]']            \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 9, 110, 64)   0           ['res2b-batchnorm1[0][0]']       \n",
      "                                                                                                  \n",
      " res2b-conv2 (Conv2D)           (None, 9, 110, 64)   32832       ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " res2b-batchnorm2 (BatchNormali  (None, 9, 110, 64)  256         ['res2b-conv2[0][0]']            \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 9, 110, 64)   0           ['res2b-batchnorm2[0][0]',       \n",
      "                                                                  'activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 9, 110, 64)   0           ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2 (Conv2D)                 (None, 9, 110, 1)    65          ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 990)          0           ['conv2[0][0]']                  \n",
      "                                                                                                  \n",
      " fc (Dense)                     (None, 100)          99100       ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 100)          0           ['fc[0][0]']                     \n",
      "                                                                                                  \n",
      " fc2 (Dense)                    (None, 32)           3232        ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " softmax (Dense)                (None, 2)            66          ['fc2[0][0]']                    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 202,559\n",
      "Trainable params: 202,047\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "16/16 [==============================] - 2s 61ms/step - loss: 2.6080 - accuracy: 0.6413 - val_loss: 2.2155 - val_accuracy: 0.6522\n",
      "Epoch 2/150\n",
      "16/16 [==============================] - 1s 38ms/step - loss: 1.9284 - accuracy: 0.6739 - val_loss: 1.7384 - val_accuracy: 0.6522\n",
      "Epoch 3/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 1.4936 - accuracy: 0.6739 - val_loss: 1.4391 - val_accuracy: 0.6522\n",
      "Epoch 4/150\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 1.1770 - accuracy: 0.7717 - val_loss: 1.2713 - val_accuracy: 0.6522\n",
      "Epoch 5/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.9263 - accuracy: 0.9239 - val_loss: 1.1671 - val_accuracy: 0.6522\n",
      "Epoch 6/150\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.7508 - accuracy: 0.9674 - val_loss: 1.0789 - val_accuracy: 0.6087\n",
      "Epoch 7/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.6297 - accuracy: 0.9783 - val_loss: 1.0005 - val_accuracy: 0.6522\n",
      "Epoch 8/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.5282 - accuracy: 1.0000 - val_loss: 0.9560 - val_accuracy: 0.6522\n",
      "Epoch 9/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.4588 - accuracy: 1.0000 - val_loss: 0.9085 - val_accuracy: 0.5652\n",
      "Epoch 10/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.3951 - accuracy: 1.0000 - val_loss: 0.8669 - val_accuracy: 0.5652\n",
      "Epoch 11/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.3472 - accuracy: 1.0000 - val_loss: 0.8177 - val_accuracy: 0.6522\n",
      "Epoch 12/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.3095 - accuracy: 1.0000 - val_loss: 0.7824 - val_accuracy: 0.6522\n",
      "Epoch 13/150\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.2763 - accuracy: 1.0000 - val_loss: 0.7507 - val_accuracy: 0.6522\n",
      "Epoch 14/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.2493 - accuracy: 1.0000 - val_loss: 0.7246 - val_accuracy: 0.6522\n",
      "Epoch 15/150\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.2260 - accuracy: 1.0000 - val_loss: 0.7153 - val_accuracy: 0.6522\n",
      "Epoch 16/150\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.2061 - accuracy: 1.0000 - val_loss: 0.6970 - val_accuracy: 0.6522\n",
      "Epoch 17/150\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.1878 - accuracy: 1.0000 - val_loss: 0.6824 - val_accuracy: 0.6087\n",
      "Epoch 18/150\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.1722 - accuracy: 1.0000 - val_loss: 0.6830 - val_accuracy: 0.6522\n",
      "Epoch 19/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.1586 - accuracy: 1.0000 - val_loss: 0.6688 - val_accuracy: 0.6957\n",
      "Epoch 20/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.1483 - accuracy: 1.0000 - val_loss: 0.6558 - val_accuracy: 0.6957\n",
      "Epoch 21/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.1383 - accuracy: 1.0000 - val_loss: 0.6545 - val_accuracy: 0.6957\n",
      "Epoch 22/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.1292 - accuracy: 1.0000 - val_loss: 0.6501 - val_accuracy: 0.7391\n",
      "Epoch 23/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.1213 - accuracy: 1.0000 - val_loss: 0.6656 - val_accuracy: 0.6522\n",
      "Epoch 24/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.1134 - accuracy: 1.0000 - val_loss: 0.6644 - val_accuracy: 0.6957\n",
      "Epoch 25/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.1079 - accuracy: 1.0000 - val_loss: 0.6786 - val_accuracy: 0.6957\n",
      "Epoch 26/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.1027 - accuracy: 1.0000 - val_loss: 0.6633 - val_accuracy: 0.6522\n",
      "Epoch 27/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0974 - accuracy: 1.0000 - val_loss: 0.6889 - val_accuracy: 0.6087\n",
      "Epoch 28/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0931 - accuracy: 1.0000 - val_loss: 0.7047 - val_accuracy: 0.6957\n",
      "Epoch 29/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0892 - accuracy: 1.0000 - val_loss: 0.6986 - val_accuracy: 0.6087\n",
      "Epoch 30/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0862 - accuracy: 1.0000 - val_loss: 0.7138 - val_accuracy: 0.6087\n",
      "Epoch 31/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0834 - accuracy: 1.0000 - val_loss: 0.7244 - val_accuracy: 0.5652\n",
      "Epoch 32/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0792 - accuracy: 1.0000 - val_loss: 0.7412 - val_accuracy: 0.5652\n",
      "Epoch 33/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0762 - accuracy: 1.0000 - val_loss: 0.7547 - val_accuracy: 0.6087\n",
      "Epoch 34/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0739 - accuracy: 1.0000 - val_loss: 0.8134 - val_accuracy: 0.5652\n",
      "Epoch 35/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0716 - accuracy: 1.0000 - val_loss: 0.8259 - val_accuracy: 0.5652\n",
      "Epoch 36/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0696 - accuracy: 1.0000 - val_loss: 0.8812 - val_accuracy: 0.6087\n",
      "Epoch 37/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0677 - accuracy: 1.0000 - val_loss: 0.9004 - val_accuracy: 0.6087\n",
      "Epoch 38/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0658 - accuracy: 1.0000 - val_loss: 0.8568 - val_accuracy: 0.5652\n",
      "Epoch 39/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0638 - accuracy: 1.0000 - val_loss: 0.9077 - val_accuracy: 0.5652\n",
      "Epoch 40/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0621 - accuracy: 1.0000 - val_loss: 0.9496 - val_accuracy: 0.5652\n",
      "Epoch 41/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0608 - accuracy: 1.0000 - val_loss: 0.9576 - val_accuracy: 0.5652\n",
      "Epoch 42/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0594 - accuracy: 1.0000 - val_loss: 1.0035 - val_accuracy: 0.5652\n",
      "Epoch 43/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0582 - accuracy: 1.0000 - val_loss: 1.0244 - val_accuracy: 0.5652\n",
      "Epoch 44/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0568 - accuracy: 1.0000 - val_loss: 1.0422 - val_accuracy: 0.5652\n",
      "Epoch 45/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0558 - accuracy: 1.0000 - val_loss: 1.0383 - val_accuracy: 0.5652\n",
      "Epoch 46/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0549 - accuracy: 1.0000 - val_loss: 1.0931 - val_accuracy: 0.5652\n",
      "Epoch 47/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0542 - accuracy: 1.0000 - val_loss: 1.1027 - val_accuracy: 0.5652\n",
      "Epoch 48/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0532 - accuracy: 1.0000 - val_loss: 1.0617 - val_accuracy: 0.5652\n",
      "Epoch 49/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0520 - accuracy: 1.0000 - val_loss: 1.1052 - val_accuracy: 0.5652\n",
      "Epoch 50/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.0510 - accuracy: 1.0000 - val_loss: 1.1070 - val_accuracy: 0.5652\n",
      "Epoch 51/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0502 - accuracy: 1.0000 - val_loss: 1.1388 - val_accuracy: 0.5652\n",
      "Epoch 52/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0495 - accuracy: 1.0000 - val_loss: 1.1688 - val_accuracy: 0.5217\n",
      "Epoch 53/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0485 - accuracy: 1.0000 - val_loss: 1.1924 - val_accuracy: 0.5217\n",
      "Epoch 54/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0481 - accuracy: 1.0000 - val_loss: 1.2042 - val_accuracy: 0.5217\n",
      "Epoch 55/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0475 - accuracy: 1.0000 - val_loss: 1.1897 - val_accuracy: 0.5217\n",
      "Epoch 56/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.0464 - accuracy: 1.0000 - val_loss: 1.0771 - val_accuracy: 0.5652\n",
      "Epoch 57/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.4325 - accuracy: 0.8804 - val_loss: 1.0516 - val_accuracy: 0.6522\n",
      "Epoch 58/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.9829 - accuracy: 0.5326 - val_loss: 0.8505 - val_accuracy: 0.6522\n",
      "Epoch 59/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.8557 - accuracy: 0.6739 - val_loss: 0.8320 - val_accuracy: 0.6522\n",
      "Epoch 60/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.8307 - accuracy: 0.6630 - val_loss: 0.8067 - val_accuracy: 0.6522\n",
      "Epoch 61/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.7939 - accuracy: 0.6739 - val_loss: 0.7785 - val_accuracy: 0.6522\n",
      "Epoch 62/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.7460 - accuracy: 0.6739 - val_loss: 0.7553 - val_accuracy: 0.6522\n",
      "Epoch 63/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.7255 - accuracy: 0.6739 - val_loss: 0.7255 - val_accuracy: 0.6522\n",
      "Epoch 64/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.6953 - accuracy: 0.6739 - val_loss: 0.7178 - val_accuracy: 0.6522\n",
      "Epoch 65/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.7065 - accuracy: 0.7065 - val_loss: 0.7230 - val_accuracy: 0.6522\n",
      "Epoch 66/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.6998 - accuracy: 0.6522 - val_loss: 0.7401 - val_accuracy: 0.6522\n",
      "Epoch 67/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.6821 - accuracy: 0.6848 - val_loss: 0.7334 - val_accuracy: 0.6522\n",
      "Epoch 68/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.5982 - accuracy: 0.7283 - val_loss: 0.7584 - val_accuracy: 0.6522\n",
      "Epoch 69/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.5244 - accuracy: 0.8587 - val_loss: 0.7805 - val_accuracy: 0.6522\n",
      "Epoch 70/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.5222 - accuracy: 0.8696 - val_loss: 0.7269 - val_accuracy: 0.7391\n",
      "Epoch 71/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.4851 - accuracy: 0.8804 - val_loss: 0.7390 - val_accuracy: 0.6087\n",
      "Epoch 72/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.3966 - accuracy: 0.9348 - val_loss: 0.6843 - val_accuracy: 0.7826\n",
      "Epoch 73/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.3792 - accuracy: 0.9348 - val_loss: 0.6569 - val_accuracy: 0.6522\n",
      "Epoch 74/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.3291 - accuracy: 0.9783 - val_loss: 0.7993 - val_accuracy: 0.6087\n",
      "Epoch 75/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.2824 - accuracy: 0.9783 - val_loss: 0.7370 - val_accuracy: 0.7826\n",
      "Epoch 76/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.2352 - accuracy: 1.0000 - val_loss: 0.7978 - val_accuracy: 0.6957\n",
      "Epoch 77/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.2198 - accuracy: 1.0000 - val_loss: 0.6933 - val_accuracy: 0.6957\n",
      "Epoch 78/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.1929 - accuracy: 1.0000 - val_loss: 0.7254 - val_accuracy: 0.6957\n",
      "Epoch 79/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.1642 - accuracy: 1.0000 - val_loss: 0.7325 - val_accuracy: 0.6957\n",
      "Epoch 80/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.1475 - accuracy: 1.0000 - val_loss: 0.7717 - val_accuracy: 0.6957\n",
      "Epoch 81/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.1317 - accuracy: 1.0000 - val_loss: 0.7472 - val_accuracy: 0.6957\n",
      "Epoch 82/150\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.1208 - accuracy: 1.0000 - val_loss: 0.7671 - val_accuracy: 0.6957\n",
      "Epoch 83/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.1116 - accuracy: 1.0000 - val_loss: 0.7473 - val_accuracy: 0.6957\n",
      "Epoch 84/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.1035 - accuracy: 1.0000 - val_loss: 0.7324 - val_accuracy: 0.6957\n",
      "Epoch 85/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0967 - accuracy: 1.0000 - val_loss: 0.7187 - val_accuracy: 0.6957\n",
      "Epoch 86/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0906 - accuracy: 1.0000 - val_loss: 0.7307 - val_accuracy: 0.6957\n",
      "Epoch 87/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0861 - accuracy: 1.0000 - val_loss: 0.7536 - val_accuracy: 0.6957\n",
      "Epoch 88/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0816 - accuracy: 1.0000 - val_loss: 0.7403 - val_accuracy: 0.6957\n",
      "Epoch 89/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0780 - accuracy: 1.0000 - val_loss: 0.7286 - val_accuracy: 0.6957\n",
      "Epoch 90/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0743 - accuracy: 1.0000 - val_loss: 0.7402 - val_accuracy: 0.6957\n",
      "Epoch 91/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0714 - accuracy: 1.0000 - val_loss: 0.7428 - val_accuracy: 0.6957\n",
      "Epoch 92/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0685 - accuracy: 1.0000 - val_loss: 0.7552 - val_accuracy: 0.6957\n",
      "Epoch 93/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0660 - accuracy: 1.0000 - val_loss: 0.7603 - val_accuracy: 0.6957\n",
      "Epoch 94/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0639 - accuracy: 1.0000 - val_loss: 0.7695 - val_accuracy: 0.6957\n",
      "Epoch 95/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0618 - accuracy: 1.0000 - val_loss: 0.7622 - val_accuracy: 0.6957\n",
      "Epoch 96/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0599 - accuracy: 1.0000 - val_loss: 0.7480 - val_accuracy: 0.6957\n",
      "Epoch 97/150\n",
      "16/16 [==============================] - 1s 49ms/step - loss: 0.0583 - accuracy: 1.0000 - val_loss: 0.7714 - val_accuracy: 0.6957\n",
      "Epoch 98/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0567 - accuracy: 1.0000 - val_loss: 0.7451 - val_accuracy: 0.6957\n",
      "Epoch 99/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0552 - accuracy: 1.0000 - val_loss: 0.7709 - val_accuracy: 0.6957\n",
      "Epoch 100/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0541 - accuracy: 1.0000 - val_loss: 0.7923 - val_accuracy: 0.6957\n",
      "Epoch 101/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0526 - accuracy: 1.0000 - val_loss: 0.7889 - val_accuracy: 0.6957\n",
      "Epoch 102/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0516 - accuracy: 1.0000 - val_loss: 0.7996 - val_accuracy: 0.6957\n",
      "Epoch 103/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.0502 - accuracy: 1.0000 - val_loss: 0.7854 - val_accuracy: 0.6957\n",
      "Epoch 104/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.0494 - accuracy: 1.0000 - val_loss: 0.8034 - val_accuracy: 0.6957\n",
      "Epoch 105/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0481 - accuracy: 1.0000 - val_loss: 0.7991 - val_accuracy: 0.6957\n",
      "Epoch 106/150\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 0.0474 - accuracy: 1.0000 - val_loss: 0.8033 - val_accuracy: 0.6957\n",
      "Epoch 107/150\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.0467 - accuracy: 1.0000 - val_loss: 0.8179 - val_accuracy: 0.6957\n",
      "Epoch 108/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0460 - accuracy: 1.0000 - val_loss: 0.7906 - val_accuracy: 0.6957\n",
      "Epoch 109/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0452 - accuracy: 1.0000 - val_loss: 0.8251 - val_accuracy: 0.6957\n",
      "Epoch 110/150\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.0447 - accuracy: 1.0000 - val_loss: 0.8282 - val_accuracy: 0.6957\n",
      "Epoch 111/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0438 - accuracy: 1.0000 - val_loss: 0.8020 - val_accuracy: 0.6957\n",
      "Epoch 112/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0431 - accuracy: 1.0000 - val_loss: 0.8119 - val_accuracy: 0.6957\n",
      "Epoch 113/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0425 - accuracy: 1.0000 - val_loss: 0.8235 - val_accuracy: 0.6957\n",
      "Epoch 114/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0421 - accuracy: 1.0000 - val_loss: 0.8220 - val_accuracy: 0.6957\n",
      "Epoch 115/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0412 - accuracy: 1.0000 - val_loss: 0.8382 - val_accuracy: 0.6957\n",
      "Epoch 116/150\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.0411 - accuracy: 1.0000 - val_loss: 0.8087 - val_accuracy: 0.6957\n",
      "Epoch 117/150\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.0403 - accuracy: 1.0000 - val_loss: 0.8583 - val_accuracy: 0.6957\n",
      "Epoch 118/150\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.0400 - accuracy: 1.0000 - val_loss: 0.8578 - val_accuracy: 0.6957\n",
      "Epoch 119/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0396 - accuracy: 1.0000 - val_loss: 0.8511 - val_accuracy: 0.6957\n",
      "Epoch 120/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0391 - accuracy: 1.0000 - val_loss: 0.8565 - val_accuracy: 0.6957\n",
      "Epoch 121/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0386 - accuracy: 1.0000 - val_loss: 0.8453 - val_accuracy: 0.6957\n",
      "Epoch 122/150\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.0383 - accuracy: 1.0000 - val_loss: 0.8826 - val_accuracy: 0.6957\n",
      "Epoch 123/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0375 - accuracy: 1.0000 - val_loss: 0.8462 - val_accuracy: 0.6957\n",
      "Epoch 124/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0376 - accuracy: 1.0000 - val_loss: 0.8501 - val_accuracy: 0.6957\n",
      "Epoch 125/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0369 - accuracy: 1.0000 - val_loss: 0.8897 - val_accuracy: 0.6957\n",
      "Epoch 126/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0367 - accuracy: 1.0000 - val_loss: 0.8849 - val_accuracy: 0.6957\n",
      "Epoch 127/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0365 - accuracy: 1.0000 - val_loss: 0.8657 - val_accuracy: 0.6957\n",
      "Epoch 128/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0360 - accuracy: 1.0000 - val_loss: 0.8868 - val_accuracy: 0.6957\n",
      "Epoch 129/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0359 - accuracy: 1.0000 - val_loss: 0.8794 - val_accuracy: 0.6957\n",
      "Epoch 130/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0353 - accuracy: 1.0000 - val_loss: 0.8775 - val_accuracy: 0.6957\n",
      "Epoch 131/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.0350 - accuracy: 1.0000 - val_loss: 0.8758 - val_accuracy: 0.6957\n",
      "Epoch 132/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0350 - accuracy: 1.0000 - val_loss: 0.8652 - val_accuracy: 0.6957\n",
      "Epoch 133/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0348 - accuracy: 1.0000 - val_loss: 0.8891 - val_accuracy: 0.6957\n",
      "Epoch 134/150\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.0342 - accuracy: 1.0000 - val_loss: 0.9182 - val_accuracy: 0.6957\n",
      "Epoch 135/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0339 - accuracy: 1.0000 - val_loss: 0.9240 - val_accuracy: 0.6957\n",
      "Epoch 136/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0339 - accuracy: 1.0000 - val_loss: 0.9067 - val_accuracy: 0.6957\n",
      "Epoch 137/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0335 - accuracy: 1.0000 - val_loss: 0.9144 - val_accuracy: 0.6957\n",
      "Epoch 138/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.0333 - accuracy: 1.0000 - val_loss: 0.9112 - val_accuracy: 0.6957\n",
      "Epoch 139/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0330 - accuracy: 1.0000 - val_loss: 0.9355 - val_accuracy: 0.6957\n",
      "Epoch 140/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0330 - accuracy: 1.0000 - val_loss: 0.8970 - val_accuracy: 0.6522\n",
      "Epoch 141/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0326 - accuracy: 1.0000 - val_loss: 0.9191 - val_accuracy: 0.6957\n",
      "Epoch 142/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0323 - accuracy: 1.0000 - val_loss: 0.9344 - val_accuracy: 0.6957\n",
      "Epoch 143/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0320 - accuracy: 1.0000 - val_loss: 0.9228 - val_accuracy: 0.6957\n",
      "Epoch 144/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0319 - accuracy: 1.0000 - val_loss: 0.9131 - val_accuracy: 0.6957\n",
      "Epoch 145/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0318 - accuracy: 1.0000 - val_loss: 0.9195 - val_accuracy: 0.6957\n",
      "Epoch 146/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0318 - accuracy: 1.0000 - val_loss: 0.9466 - val_accuracy: 0.6522\n",
      "Epoch 147/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0314 - accuracy: 1.0000 - val_loss: 0.9451 - val_accuracy: 0.6522\n",
      "Epoch 148/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0312 - accuracy: 1.0000 - val_loss: 0.9296 - val_accuracy: 0.6522\n",
      "Epoch 149/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0309 - accuracy: 1.0000 - val_loss: 0.9061 - val_accuracy: 0.6957\n",
      "Epoch 150/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0308 - accuracy: 1.0000 - val_loss: 0.9120 - val_accuracy: 0.7391\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "auc_roc: 0.7916666666666667\n",
      "auc_pr: 0.776095837033337\n",
      "f1_score: 0.7443064182194616\n",
      "mcc: 0.4643716460347527\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "[[6.6291529e-04 9.9933708e-01]\n",
      " [2.9767162e-04 9.9970233e-01]\n",
      " [1.8098888e-04 9.9981898e-01]\n",
      " [1.8812844e-04 9.9981195e-01]\n",
      " [2.2019933e-04 9.9977976e-01]\n",
      " [3.3105186e-03 9.9668950e-01]\n",
      " [9.9944705e-01 5.5298477e-04]\n",
      " [9.9970353e-01 2.9655031e-04]\n",
      " [9.9975783e-01 2.4218387e-04]\n",
      " [1.8366716e-04 9.9981636e-01]\n",
      " [2.0736600e-03 9.9792641e-01]\n",
      " [1.9715418e-04 9.9980289e-01]\n",
      " [9.9956578e-01 4.3417184e-04]\n",
      " [9.9577975e-01 4.2202501e-03]\n",
      " [8.6028844e-01 1.3971157e-01]\n",
      " [3.8237882e-01 6.1762112e-01]\n",
      " [3.7885687e-01 6.2114310e-01]\n",
      " [8.8368054e-04 9.9911636e-01]\n",
      " [9.1993546e-01 8.0064557e-02]\n",
      " [5.1947492e-01 4.8052502e-01]\n",
      " [9.9966145e-01 3.3859810e-04]\n",
      " [9.7103757e-01 2.8962387e-02]\n",
      " [2.1335353e-04 9.9978667e-01]]\n",
      "Model: \"res\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 9, 110, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " res2a-conv1 (Conv2D)           (None, 9, 110, 64)   576         ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " res2a-batchnorm1 (BatchNormali  (None, 9, 110, 64)  256         ['res2a-conv1[0][0]']            \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 9, 110, 64)   0           ['res2a-batchnorm1[0][0]']       \n",
      "                                                                                                  \n",
      " res2a-conv2 (Conv2D)           (None, 9, 110, 64)   32832       ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " res2a-batchnorm2 (BatchNormali  (None, 9, 110, 64)  256         ['res2a-conv2[0][0]']            \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 9, 110, 64)   0           ['res2a-batchnorm2[0][0]',       \n",
      "                                                                  'input_3[0][0]']                \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 9, 110, 64)   0           ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " res2b-conv1 (Conv2D)           (None, 9, 110, 64)   32832       ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " res2b-batchnorm1 (BatchNormali  (None, 9, 110, 64)  256         ['res2b-conv1[0][0]']            \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 9, 110, 64)   0           ['res2b-batchnorm1[0][0]']       \n",
      "                                                                                                  \n",
      " res2b-conv2 (Conv2D)           (None, 9, 110, 64)   32832       ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " res2b-batchnorm2 (BatchNormali  (None, 9, 110, 64)  256         ['res2b-conv2[0][0]']            \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 9, 110, 64)   0           ['res2b-batchnorm2[0][0]',       \n",
      "                                                                  'activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 9, 110, 64)   0           ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2 (Conv2D)                 (None, 9, 110, 1)    65          ['activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 990)          0           ['conv2[0][0]']                  \n",
      "                                                                                                  \n",
      " fc (Dense)                     (None, 100)          99100       ['flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 100)          0           ['fc[0][0]']                     \n",
      "                                                                                                  \n",
      " fc2 (Dense)                    (None, 32)           3232        ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " softmax (Dense)                (None, 2)            66          ['fc2[0][0]']                    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 202,559\n",
      "Trainable params: 202,047\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "16/16 [==============================] - 2s 61ms/step - loss: 2.5985 - accuracy: 0.6522 - val_loss: 2.1869 - val_accuracy: 0.6957\n",
      "Epoch 2/150\n",
      "16/16 [==============================] - 1s 41ms/step - loss: 1.8973 - accuracy: 0.6630 - val_loss: 1.6962 - val_accuracy: 0.6957\n",
      "Epoch 3/150\n",
      "16/16 [==============================] - 1s 38ms/step - loss: 1.4479 - accuracy: 0.7609 - val_loss: 1.4076 - val_accuracy: 0.6957\n",
      "Epoch 4/150\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 1.1447 - accuracy: 0.8587 - val_loss: 1.2256 - val_accuracy: 0.6957\n",
      "Epoch 5/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.9200 - accuracy: 0.9565 - val_loss: 1.1064 - val_accuracy: 0.6957\n",
      "Epoch 6/150\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.7526 - accuracy: 0.9674 - val_loss: 1.0160 - val_accuracy: 0.6957\n",
      "Epoch 7/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.6353 - accuracy: 0.9783 - val_loss: 0.9440 - val_accuracy: 0.6957\n",
      "Epoch 8/150\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.5449 - accuracy: 0.9783 - val_loss: 0.8765 - val_accuracy: 0.6957\n",
      "Epoch 9/150\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.4701 - accuracy: 0.9891 - val_loss: 0.7888 - val_accuracy: 0.7391\n",
      "Epoch 10/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.4033 - accuracy: 1.0000 - val_loss: 0.7491 - val_accuracy: 0.7391\n",
      "Epoch 11/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.3523 - accuracy: 1.0000 - val_loss: 0.7068 - val_accuracy: 0.7391\n",
      "Epoch 12/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.3114 - accuracy: 1.0000 - val_loss: 0.6507 - val_accuracy: 0.8261\n",
      "Epoch 13/150\n",
      "16/16 [==============================] - 1s 31ms/step - loss: 0.2764 - accuracy: 1.0000 - val_loss: 0.6115 - val_accuracy: 0.8261\n",
      "Epoch 14/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.2469 - accuracy: 1.0000 - val_loss: 0.5587 - val_accuracy: 0.8696\n",
      "Epoch 15/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.2222 - accuracy: 1.0000 - val_loss: 0.5288 - val_accuracy: 0.8696\n",
      "Epoch 16/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.2007 - accuracy: 1.0000 - val_loss: 0.4955 - val_accuracy: 0.8696\n",
      "Epoch 17/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.1836 - accuracy: 1.0000 - val_loss: 0.4706 - val_accuracy: 0.8696\n",
      "Epoch 18/150\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.1686 - accuracy: 1.0000 - val_loss: 0.4427 - val_accuracy: 0.8696\n",
      "Epoch 19/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.1549 - accuracy: 1.0000 - val_loss: 0.4251 - val_accuracy: 0.8696\n",
      "Epoch 20/150\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.1431 - accuracy: 1.0000 - val_loss: 0.4021 - val_accuracy: 0.8696\n",
      "Epoch 21/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.1331 - accuracy: 1.0000 - val_loss: 0.3929 - val_accuracy: 0.8696\n",
      "Epoch 22/150\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.1240 - accuracy: 1.0000 - val_loss: 0.3828 - val_accuracy: 0.8696\n",
      "Epoch 23/150\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.1169 - accuracy: 1.0000 - val_loss: 0.3609 - val_accuracy: 0.8696\n",
      "Epoch 24/150\n",
      "16/16 [==============================] - 1s 37ms/step - loss: 0.1088 - accuracy: 1.0000 - val_loss: 0.3624 - val_accuracy: 0.8696\n",
      "Epoch 25/150\n",
      "16/16 [==============================] - 1s 41ms/step - loss: 0.1039 - accuracy: 1.0000 - val_loss: 0.3557 - val_accuracy: 0.8696\n",
      "Epoch 26/150\n",
      "16/16 [==============================] - 1s 40ms/step - loss: 0.0992 - accuracy: 1.0000 - val_loss: 0.3513 - val_accuracy: 0.8696\n",
      "Epoch 27/150\n",
      "16/16 [==============================] - 1s 44ms/step - loss: 0.0940 - accuracy: 1.0000 - val_loss: 0.3718 - val_accuracy: 0.8696\n",
      "Epoch 28/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0902 - accuracy: 1.0000 - val_loss: 0.3654 - val_accuracy: 0.8696\n",
      "Epoch 29/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0852 - accuracy: 1.0000 - val_loss: 0.3594 - val_accuracy: 0.8696\n",
      "Epoch 30/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0820 - accuracy: 1.0000 - val_loss: 0.3706 - val_accuracy: 0.8696\n",
      "Epoch 31/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.0790 - accuracy: 1.0000 - val_loss: 0.3773 - val_accuracy: 0.8696\n",
      "Epoch 32/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0764 - accuracy: 1.0000 - val_loss: 0.3980 - val_accuracy: 0.8696\n",
      "Epoch 33/150\n",
      "16/16 [==============================] - 1s 45ms/step - loss: 0.0743 - accuracy: 1.0000 - val_loss: 0.4045 - val_accuracy: 0.8696\n",
      "Epoch 34/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0705 - accuracy: 1.0000 - val_loss: 0.4022 - val_accuracy: 0.8696\n",
      "Epoch 35/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.0679 - accuracy: 1.0000 - val_loss: 0.4104 - val_accuracy: 0.8696\n",
      "Epoch 36/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0659 - accuracy: 1.0000 - val_loss: 0.4405 - val_accuracy: 0.8696\n",
      "Epoch 37/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0651 - accuracy: 1.0000 - val_loss: 0.4298 - val_accuracy: 0.8696\n",
      "Epoch 38/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0633 - accuracy: 1.0000 - val_loss: 0.4385 - val_accuracy: 0.8696\n",
      "Epoch 39/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0613 - accuracy: 1.0000 - val_loss: 0.4508 - val_accuracy: 0.8696\n",
      "Epoch 40/150\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.0597 - accuracy: 1.0000 - val_loss: 0.4429 - val_accuracy: 0.8696\n",
      "Epoch 41/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0588 - accuracy: 1.0000 - val_loss: 0.4583 - val_accuracy: 0.8696\n",
      "Epoch 42/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.0573 - accuracy: 1.0000 - val_loss: 0.4604 - val_accuracy: 0.8696\n",
      "Epoch 43/150\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.0555 - accuracy: 1.0000 - val_loss: 0.4650 - val_accuracy: 0.8696\n",
      "Epoch 44/150\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.0548 - accuracy: 1.0000 - val_loss: 0.4746 - val_accuracy: 0.8696\n",
      "Epoch 45/150\n",
      "16/16 [==============================] - 1s 48ms/step - loss: 0.0538 - accuracy: 1.0000 - val_loss: 0.4744 - val_accuracy: 0.8696\n",
      "Epoch 46/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0528 - accuracy: 1.0000 - val_loss: 0.4738 - val_accuracy: 0.8696\n",
      "Epoch 47/150\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.0516 - accuracy: 1.0000 - val_loss: 0.4806 - val_accuracy: 0.8696\n",
      "Epoch 48/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0509 - accuracy: 1.0000 - val_loss: 0.4855 - val_accuracy: 0.8696\n",
      "Epoch 49/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0503 - accuracy: 1.0000 - val_loss: 0.4890 - val_accuracy: 0.8696\n",
      "Epoch 50/150\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.0488 - accuracy: 1.0000 - val_loss: 0.4951 - val_accuracy: 0.8696\n",
      "Epoch 51/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0487 - accuracy: 1.0000 - val_loss: 0.4950 - val_accuracy: 0.8696\n",
      "Epoch 52/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0477 - accuracy: 1.0000 - val_loss: 0.5116 - val_accuracy: 0.8696\n",
      "Epoch 53/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0469 - accuracy: 1.0000 - val_loss: 0.5035 - val_accuracy: 0.8696\n",
      "Epoch 54/150\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.0464 - accuracy: 1.0000 - val_loss: 0.5011 - val_accuracy: 0.8696\n",
      "Epoch 55/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0460 - accuracy: 1.0000 - val_loss: 0.4984 - val_accuracy: 0.8696\n",
      "Epoch 56/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0451 - accuracy: 1.0000 - val_loss: 0.5065 - val_accuracy: 0.8696\n",
      "Epoch 57/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0441 - accuracy: 1.0000 - val_loss: 0.5103 - val_accuracy: 0.8696\n",
      "Epoch 58/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0436 - accuracy: 1.0000 - val_loss: 0.5139 - val_accuracy: 0.8696\n",
      "Epoch 59/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0433 - accuracy: 1.0000 - val_loss: 0.5186 - val_accuracy: 0.8696\n",
      "Epoch 60/150\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.0427 - accuracy: 1.0000 - val_loss: 0.5208 - val_accuracy: 0.8696\n",
      "Epoch 61/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0418 - accuracy: 1.0000 - val_loss: 0.5149 - val_accuracy: 0.8696\n",
      "Epoch 62/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0416 - accuracy: 1.0000 - val_loss: 0.5192 - val_accuracy: 0.8696\n",
      "Epoch 63/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0418 - accuracy: 1.0000 - val_loss: 0.5236 - val_accuracy: 0.8696\n",
      "Epoch 64/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0403 - accuracy: 1.0000 - val_loss: 0.5152 - val_accuracy: 0.8696\n",
      "Epoch 65/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.0399 - accuracy: 1.0000 - val_loss: 0.5211 - val_accuracy: 0.8696\n",
      "Epoch 66/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0395 - accuracy: 1.0000 - val_loss: 0.5168 - val_accuracy: 0.8696\n",
      "Epoch 67/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0389 - accuracy: 1.0000 - val_loss: 0.5049 - val_accuracy: 0.8696\n",
      "Epoch 68/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0390 - accuracy: 1.0000 - val_loss: 0.5218 - val_accuracy: 0.8696\n",
      "Epoch 69/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0382 - accuracy: 1.0000 - val_loss: 0.5283 - val_accuracy: 0.8696\n",
      "Epoch 70/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0380 - accuracy: 1.0000 - val_loss: 0.5260 - val_accuracy: 0.8696\n",
      "Epoch 71/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0375 - accuracy: 1.0000 - val_loss: 0.5229 - val_accuracy: 0.8696\n",
      "Epoch 72/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0370 - accuracy: 1.0000 - val_loss: 0.5255 - val_accuracy: 0.8696\n",
      "Epoch 73/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0366 - accuracy: 1.0000 - val_loss: 0.5318 - val_accuracy: 0.8696\n",
      "Epoch 74/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0361 - accuracy: 1.0000 - val_loss: 0.5383 - val_accuracy: 0.8696\n",
      "Epoch 75/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.0361 - accuracy: 1.0000 - val_loss: 0.5358 - val_accuracy: 0.8696\n",
      "Epoch 76/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0352 - accuracy: 1.0000 - val_loss: 0.5286 - val_accuracy: 0.8696\n",
      "Epoch 77/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0353 - accuracy: 1.0000 - val_loss: 0.5375 - val_accuracy: 0.8696\n",
      "Epoch 78/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0346 - accuracy: 1.0000 - val_loss: 0.5473 - val_accuracy: 0.8696\n",
      "Epoch 79/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0346 - accuracy: 1.0000 - val_loss: 0.5143 - val_accuracy: 0.8696\n",
      "Epoch 80/150\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.0339 - accuracy: 1.0000 - val_loss: 0.5295 - val_accuracy: 0.8696\n",
      "Epoch 81/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0338 - accuracy: 1.0000 - val_loss: 0.5414 - val_accuracy: 0.8696\n",
      "Epoch 82/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0334 - accuracy: 1.0000 - val_loss: 0.5311 - val_accuracy: 0.8696\n",
      "Epoch 83/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.0331 - accuracy: 1.0000 - val_loss: 0.5309 - val_accuracy: 0.8696\n",
      "Epoch 84/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0329 - accuracy: 1.0000 - val_loss: 0.5379 - val_accuracy: 0.8696\n",
      "Epoch 85/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0324 - accuracy: 1.0000 - val_loss: 0.5298 - val_accuracy: 0.8696\n",
      "Epoch 86/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0320 - accuracy: 1.0000 - val_loss: 0.5399 - val_accuracy: 0.8696\n",
      "Epoch 87/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0316 - accuracy: 1.0000 - val_loss: 0.5423 - val_accuracy: 0.8696\n",
      "Epoch 88/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0314 - accuracy: 1.0000 - val_loss: 0.5359 - val_accuracy: 0.8696\n",
      "Epoch 89/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0311 - accuracy: 1.0000 - val_loss: 0.5469 - val_accuracy: 0.8696\n",
      "Epoch 90/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0311 - accuracy: 1.0000 - val_loss: 0.5414 - val_accuracy: 0.8696\n",
      "Epoch 91/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0306 - accuracy: 1.0000 - val_loss: 0.5518 - val_accuracy: 0.8696\n",
      "Epoch 92/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0308 - accuracy: 1.0000 - val_loss: 0.5466 - val_accuracy: 0.8696\n",
      "Epoch 93/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0303 - accuracy: 1.0000 - val_loss: 0.5263 - val_accuracy: 0.8696\n",
      "Epoch 94/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0302 - accuracy: 1.0000 - val_loss: 0.5399 - val_accuracy: 0.8696\n",
      "Epoch 95/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0295 - accuracy: 1.0000 - val_loss: 0.5367 - val_accuracy: 0.8696\n",
      "Epoch 96/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0291 - accuracy: 1.0000 - val_loss: 0.5349 - val_accuracy: 0.8696\n",
      "Epoch 97/150\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.0292 - accuracy: 1.0000 - val_loss: 0.5392 - val_accuracy: 0.8696\n",
      "Epoch 98/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0289 - accuracy: 1.0000 - val_loss: 0.5357 - val_accuracy: 0.8696\n",
      "Epoch 99/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0287 - accuracy: 1.0000 - val_loss: 0.5346 - val_accuracy: 0.8696\n",
      "Epoch 100/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0282 - accuracy: 1.0000 - val_loss: 0.5419 - val_accuracy: 0.8696\n",
      "Epoch 101/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.0281 - accuracy: 1.0000 - val_loss: 0.5368 - val_accuracy: 0.8696\n",
      "Epoch 102/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0278 - accuracy: 1.0000 - val_loss: 0.5373 - val_accuracy: 0.8696\n",
      "Epoch 103/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0277 - accuracy: 1.0000 - val_loss: 0.5449 - val_accuracy: 0.8696\n",
      "Epoch 104/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.0274 - accuracy: 1.0000 - val_loss: 0.5441 - val_accuracy: 0.8696\n",
      "Epoch 105/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0275 - accuracy: 1.0000 - val_loss: 0.5593 - val_accuracy: 0.8696\n",
      "Epoch 106/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0273 - accuracy: 1.0000 - val_loss: 0.5421 - val_accuracy: 0.8696\n",
      "Epoch 107/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.0272 - accuracy: 1.0000 - val_loss: 0.5617 - val_accuracy: 0.8696\n",
      "Epoch 108/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0269 - accuracy: 1.0000 - val_loss: 0.5853 - val_accuracy: 0.8696\n",
      "Epoch 109/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0266 - accuracy: 1.0000 - val_loss: 0.5593 - val_accuracy: 0.8696\n",
      "Epoch 110/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0262 - accuracy: 1.0000 - val_loss: 0.5592 - val_accuracy: 0.8696\n",
      "Epoch 111/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0262 - accuracy: 1.0000 - val_loss: 0.5908 - val_accuracy: 0.8696\n",
      "Epoch 112/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0263 - accuracy: 1.0000 - val_loss: 0.5473 - val_accuracy: 0.8696\n",
      "Epoch 113/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0259 - accuracy: 1.0000 - val_loss: 0.5722 - val_accuracy: 0.8696\n",
      "Epoch 114/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0255 - accuracy: 1.0000 - val_loss: 0.5665 - val_accuracy: 0.8696\n",
      "Epoch 115/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0253 - accuracy: 1.0000 - val_loss: 0.5577 - val_accuracy: 0.8696\n",
      "Epoch 116/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0251 - accuracy: 1.0000 - val_loss: 0.5487 - val_accuracy: 0.8696\n",
      "Epoch 117/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0247 - accuracy: 1.0000 - val_loss: 0.5582 - val_accuracy: 0.8696\n",
      "Epoch 118/150\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.0247 - accuracy: 1.0000 - val_loss: 0.5573 - val_accuracy: 0.8696\n",
      "Epoch 119/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0245 - accuracy: 1.0000 - val_loss: 0.5679 - val_accuracy: 0.8696\n",
      "Epoch 120/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0246 - accuracy: 1.0000 - val_loss: 0.5487 - val_accuracy: 0.8696\n",
      "Epoch 121/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0240 - accuracy: 1.0000 - val_loss: 0.5562 - val_accuracy: 0.8696\n",
      "Epoch 122/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0243 - accuracy: 1.0000 - val_loss: 0.5563 - val_accuracy: 0.8696\n",
      "Epoch 123/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0237 - accuracy: 1.0000 - val_loss: 0.5554 - val_accuracy: 0.8696\n",
      "Epoch 124/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0238 - accuracy: 1.0000 - val_loss: 0.5399 - val_accuracy: 0.8696\n",
      "Epoch 125/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0237 - accuracy: 1.0000 - val_loss: 0.5588 - val_accuracy: 0.8696\n",
      "Epoch 126/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0237 - accuracy: 1.0000 - val_loss: 0.5679 - val_accuracy: 0.8696\n",
      "Epoch 127/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.0234 - accuracy: 1.0000 - val_loss: 0.5641 - val_accuracy: 0.8696\n",
      "Epoch 128/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0230 - accuracy: 1.0000 - val_loss: 0.5538 - val_accuracy: 0.8696\n",
      "Epoch 129/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0230 - accuracy: 1.0000 - val_loss: 0.5636 - val_accuracy: 0.8696\n",
      "Epoch 130/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0227 - accuracy: 1.0000 - val_loss: 0.6050 - val_accuracy: 0.8261\n",
      "Epoch 131/150\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.7181 - accuracy: 0.8478 - val_loss: 0.8281 - val_accuracy: 0.6522\n",
      "Epoch 132/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 1.0171 - accuracy: 0.5652 - val_loss: 1.1508 - val_accuracy: 0.2609\n",
      "Epoch 133/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.9419 - accuracy: 0.5543 - val_loss: 0.8972 - val_accuracy: 0.6957\n",
      "Epoch 134/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.9275 - accuracy: 0.5000 - val_loss: 0.8169 - val_accuracy: 0.6957\n",
      "Epoch 135/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.8380 - accuracy: 0.6630 - val_loss: 0.7606 - val_accuracy: 0.6957\n",
      "Epoch 136/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.8086 - accuracy: 0.6630 - val_loss: 0.7324 - val_accuracy: 0.6957\n",
      "Epoch 137/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.7546 - accuracy: 0.6196 - val_loss: 0.7247 - val_accuracy: 0.6957\n",
      "Epoch 138/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.7403 - accuracy: 0.6522 - val_loss: 0.6931 - val_accuracy: 0.6957\n",
      "Epoch 139/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.7311 - accuracy: 0.6630 - val_loss: 0.6878 - val_accuracy: 0.6957\n",
      "Epoch 140/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.7158 - accuracy: 0.6196 - val_loss: 0.7250 - val_accuracy: 0.6957\n",
      "Epoch 141/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.7056 - accuracy: 0.6630 - val_loss: 0.6647 - val_accuracy: 0.6957\n",
      "Epoch 142/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.7045 - accuracy: 0.5652 - val_loss: 0.6559 - val_accuracy: 0.6957\n",
      "Epoch 143/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.7086 - accuracy: 0.6630 - val_loss: 0.6616 - val_accuracy: 0.6957\n",
      "Epoch 144/150\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.7065 - accuracy: 0.6630 - val_loss: 0.6704 - val_accuracy: 0.6957\n",
      "Epoch 145/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.7129 - accuracy: 0.6630 - val_loss: 0.6821 - val_accuracy: 0.6957\n",
      "Epoch 146/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.7269 - accuracy: 0.6522 - val_loss: 0.6976 - val_accuracy: 0.6957\n",
      "Epoch 147/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.7198 - accuracy: 0.6630 - val_loss: 0.6624 - val_accuracy: 0.6957\n",
      "Epoch 148/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.7080 - accuracy: 0.6630 - val_loss: 0.6713 - val_accuracy: 0.6957\n",
      "Epoch 149/150\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.6707 - accuracy: 0.6739 - val_loss: 0.6368 - val_accuracy: 0.6957\n",
      "Epoch 150/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.6284 - accuracy: 0.6957 - val_loss: 0.7172 - val_accuracy: 0.6957\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "auc_roc: 0.8214285714285714\n",
      "auc_pr: 0.8468004748250975\n",
      "f1_score: 0.5707915273132664\n",
      "mcc: 0.0\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "[[0.04852813 0.95147187]\n",
      " [0.01775551 0.98224455]\n",
      " [0.08259011 0.9174099 ]\n",
      " [0.04946734 0.95053273]\n",
      " [0.0550462  0.9449538 ]\n",
      " [0.10009402 0.89990604]\n",
      " [0.0397559  0.9602441 ]\n",
      " [0.05617665 0.9438234 ]\n",
      " [0.05692828 0.9430717 ]\n",
      " [0.03871456 0.9612854 ]\n",
      " [0.05016914 0.9498309 ]\n",
      " [0.06303377 0.93696624]\n",
      " [0.05603662 0.9439634 ]\n",
      " [0.07068551 0.9293145 ]\n",
      " [0.05403648 0.94596356]\n",
      " [0.05239028 0.9476097 ]\n",
      " [0.06408282 0.9359172 ]\n",
      " [0.04041886 0.9595811 ]\n",
      " [0.01616411 0.9838359 ]\n",
      " [0.04365411 0.95634586]\n",
      " [0.05743898 0.942561  ]\n",
      " [0.03829008 0.9617099 ]\n",
      " [0.07895683 0.9210432 ]]\n",
      "Model: \"res\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 9, 110, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " res2a-conv1 (Conv2D)           (None, 9, 110, 64)   576         ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " res2a-batchnorm1 (BatchNormali  (None, 9, 110, 64)  256         ['res2a-conv1[0][0]']            \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 9, 110, 64)   0           ['res2a-batchnorm1[0][0]']       \n",
      "                                                                                                  \n",
      " res2a-conv2 (Conv2D)           (None, 9, 110, 64)   32832       ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " res2a-batchnorm2 (BatchNormali  (None, 9, 110, 64)  256         ['res2a-conv2[0][0]']            \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 9, 110, 64)   0           ['res2a-batchnorm2[0][0]',       \n",
      "                                                                  'input_4[0][0]']                \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 9, 110, 64)   0           ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " res2b-conv1 (Conv2D)           (None, 9, 110, 64)   32832       ['activation_13[0][0]']          \n",
      "                                                                                                  \n",
      " res2b-batchnorm1 (BatchNormali  (None, 9, 110, 64)  256         ['res2b-conv1[0][0]']            \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 9, 110, 64)   0           ['res2b-batchnorm1[0][0]']       \n",
      "                                                                                                  \n",
      " res2b-conv2 (Conv2D)           (None, 9, 110, 64)   32832       ['activation_14[0][0]']          \n",
      "                                                                                                  \n",
      " res2b-batchnorm2 (BatchNormali  (None, 9, 110, 64)  256         ['res2b-conv2[0][0]']            \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 9, 110, 64)   0           ['res2b-batchnorm2[0][0]',       \n",
      "                                                                  'activation_13[0][0]']          \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 9, 110, 64)   0           ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2 (Conv2D)                 (None, 9, 110, 1)    65          ['activation_15[0][0]']          \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 990)          0           ['conv2[0][0]']                  \n",
      "                                                                                                  \n",
      " fc (Dense)                     (None, 100)          99100       ['flatten_3[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 100)          0           ['fc[0][0]']                     \n",
      "                                                                                                  \n",
      " fc2 (Dense)                    (None, 32)           3232        ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " softmax (Dense)                (None, 2)            66          ['fc2[0][0]']                    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 202,559\n",
      "Trainable params: 202,047\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "16/16 [==============================] - 2s 65ms/step - loss: 2.5855 - accuracy: 0.6739 - val_loss: 2.2060 - val_accuracy: 0.6957\n",
      "Epoch 2/150\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 1.9049 - accuracy: 0.6630 - val_loss: 1.7374 - val_accuracy: 0.6957\n",
      "Epoch 3/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 1.4522 - accuracy: 0.8261 - val_loss: 1.4472 - val_accuracy: 0.6957\n",
      "Epoch 4/150\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 1.1340 - accuracy: 0.8804 - val_loss: 1.2897 - val_accuracy: 0.6957\n",
      "Epoch 5/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.9081 - accuracy: 0.9674 - val_loss: 1.1781 - val_accuracy: 0.6957\n",
      "Epoch 6/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.7602 - accuracy: 0.9891 - val_loss: 1.0922 - val_accuracy: 0.6957\n",
      "Epoch 7/150\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 0.6284 - accuracy: 1.0000 - val_loss: 1.0375 - val_accuracy: 0.7391\n",
      "Epoch 8/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.5400 - accuracy: 1.0000 - val_loss: 0.9778 - val_accuracy: 0.7391\n",
      "Epoch 9/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.4656 - accuracy: 1.0000 - val_loss: 0.9389 - val_accuracy: 0.6957\n",
      "Epoch 10/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.4071 - accuracy: 1.0000 - val_loss: 0.8876 - val_accuracy: 0.6957\n",
      "Epoch 11/150\n",
      "16/16 [==============================] - 1s 38ms/step - loss: 0.3565 - accuracy: 1.0000 - val_loss: 0.8398 - val_accuracy: 0.6957\n",
      "Epoch 12/150\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.3147 - accuracy: 1.0000 - val_loss: 0.7973 - val_accuracy: 0.7826\n",
      "Epoch 13/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.2785 - accuracy: 1.0000 - val_loss: 0.7648 - val_accuracy: 0.7826\n",
      "Epoch 14/150\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.2481 - accuracy: 1.0000 - val_loss: 0.7343 - val_accuracy: 0.7826\n",
      "Epoch 15/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.2239 - accuracy: 1.0000 - val_loss: 0.7074 - val_accuracy: 0.7391\n",
      "Epoch 16/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.2015 - accuracy: 1.0000 - val_loss: 0.6937 - val_accuracy: 0.7826\n",
      "Epoch 17/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.1834 - accuracy: 1.0000 - val_loss: 0.6790 - val_accuracy: 0.7826\n",
      "Epoch 18/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.1680 - accuracy: 1.0000 - val_loss: 0.6528 - val_accuracy: 0.7391\n",
      "Epoch 19/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.1542 - accuracy: 1.0000 - val_loss: 0.6462 - val_accuracy: 0.7391\n",
      "Epoch 20/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.1416 - accuracy: 1.0000 - val_loss: 0.6349 - val_accuracy: 0.7391\n",
      "Epoch 21/150\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.1317 - accuracy: 1.0000 - val_loss: 0.6219 - val_accuracy: 0.7391\n",
      "Epoch 22/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.1221 - accuracy: 1.0000 - val_loss: 0.6120 - val_accuracy: 0.7391\n",
      "Epoch 23/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.1151 - accuracy: 1.0000 - val_loss: 0.6097 - val_accuracy: 0.7391\n",
      "Epoch 24/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.1082 - accuracy: 1.0000 - val_loss: 0.6114 - val_accuracy: 0.7391\n",
      "Epoch 25/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.1017 - accuracy: 1.0000 - val_loss: 0.5979 - val_accuracy: 0.7826\n",
      "Epoch 26/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0969 - accuracy: 1.0000 - val_loss: 0.5967 - val_accuracy: 0.7826\n",
      "Epoch 27/150\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.0912 - accuracy: 1.0000 - val_loss: 0.5995 - val_accuracy: 0.7826\n",
      "Epoch 28/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0882 - accuracy: 1.0000 - val_loss: 0.5924 - val_accuracy: 0.7826\n",
      "Epoch 29/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0830 - accuracy: 1.0000 - val_loss: 0.6083 - val_accuracy: 0.7826\n",
      "Epoch 30/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0806 - accuracy: 1.0000 - val_loss: 0.6016 - val_accuracy: 0.7826\n",
      "Epoch 31/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0772 - accuracy: 1.0000 - val_loss: 0.5803 - val_accuracy: 0.8261\n",
      "Epoch 32/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.0744 - accuracy: 1.0000 - val_loss: 0.6176 - val_accuracy: 0.7826\n",
      "Epoch 33/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0716 - accuracy: 1.0000 - val_loss: 0.6275 - val_accuracy: 0.8261\n",
      "Epoch 34/150\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.0694 - accuracy: 1.0000 - val_loss: 0.6151 - val_accuracy: 0.7826\n",
      "Epoch 35/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0671 - accuracy: 1.0000 - val_loss: 0.6169 - val_accuracy: 0.8261\n",
      "Epoch 36/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0647 - accuracy: 1.0000 - val_loss: 0.6389 - val_accuracy: 0.7826\n",
      "Epoch 37/150\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.0642 - accuracy: 1.0000 - val_loss: 0.6517 - val_accuracy: 0.8261\n",
      "Epoch 38/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0623 - accuracy: 1.0000 - val_loss: 0.6404 - val_accuracy: 0.7826\n",
      "Epoch 39/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0604 - accuracy: 1.0000 - val_loss: 0.6660 - val_accuracy: 0.7826\n",
      "Epoch 40/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0589 - accuracy: 1.0000 - val_loss: 0.6635 - val_accuracy: 0.8261\n",
      "Epoch 41/150\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.0582 - accuracy: 1.0000 - val_loss: 0.6779 - val_accuracy: 0.7826\n",
      "Epoch 42/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.0563 - accuracy: 1.0000 - val_loss: 0.6741 - val_accuracy: 0.8261\n",
      "Epoch 43/150\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.0547 - accuracy: 1.0000 - val_loss: 0.6859 - val_accuracy: 0.7826\n",
      "Epoch 44/150\n",
      "16/16 [==============================] - 1s 31ms/step - loss: 0.0545 - accuracy: 1.0000 - val_loss: 0.6950 - val_accuracy: 0.7826\n",
      "Epoch 45/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0528 - accuracy: 1.0000 - val_loss: 0.7176 - val_accuracy: 0.7826\n",
      "Epoch 46/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0520 - accuracy: 1.0000 - val_loss: 0.7029 - val_accuracy: 0.7826\n",
      "Epoch 47/150\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.0510 - accuracy: 1.0000 - val_loss: 0.7328 - val_accuracy: 0.7826\n",
      "Epoch 48/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0508 - accuracy: 1.0000 - val_loss: 0.7083 - val_accuracy: 0.7826\n",
      "Epoch 49/150\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.0497 - accuracy: 1.0000 - val_loss: 0.7431 - val_accuracy: 0.7826\n",
      "Epoch 50/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0483 - accuracy: 1.0000 - val_loss: 0.7322 - val_accuracy: 0.7826\n",
      "Epoch 51/150\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 0.0475 - accuracy: 1.0000 - val_loss: 0.7617 - val_accuracy: 0.7826\n",
      "Epoch 52/150\n",
      "16/16 [==============================] - 1s 38ms/step - loss: 0.0473 - accuracy: 1.0000 - val_loss: 0.7312 - val_accuracy: 0.7826\n",
      "Epoch 53/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0466 - accuracy: 1.0000 - val_loss: 0.7400 - val_accuracy: 0.7826\n",
      "Epoch 54/150\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.0462 - accuracy: 1.0000 - val_loss: 0.7159 - val_accuracy: 0.7826\n",
      "Epoch 55/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.0454 - accuracy: 1.0000 - val_loss: 0.7221 - val_accuracy: 0.8261\n",
      "Epoch 56/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0450 - accuracy: 1.0000 - val_loss: 0.7577 - val_accuracy: 0.7826\n",
      "Epoch 57/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0437 - accuracy: 1.0000 - val_loss: 0.7169 - val_accuracy: 0.7826\n",
      "Epoch 58/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0429 - accuracy: 1.0000 - val_loss: 0.7541 - val_accuracy: 0.7826\n",
      "Epoch 59/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.0426 - accuracy: 1.0000 - val_loss: 0.7572 - val_accuracy: 0.7826\n",
      "Epoch 60/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0418 - accuracy: 1.0000 - val_loss: 0.7601 - val_accuracy: 0.7826\n",
      "Epoch 61/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0411 - accuracy: 1.0000 - val_loss: 0.7638 - val_accuracy: 0.7826\n",
      "Epoch 62/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0410 - accuracy: 1.0000 - val_loss: 0.7667 - val_accuracy: 0.7826\n",
      "Epoch 63/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0403 - accuracy: 1.0000 - val_loss: 0.7605 - val_accuracy: 0.7826\n",
      "Epoch 64/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0401 - accuracy: 1.0000 - val_loss: 0.8019 - val_accuracy: 0.7826\n",
      "Epoch 65/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.0395 - accuracy: 1.0000 - val_loss: 0.7892 - val_accuracy: 0.7826\n",
      "Epoch 66/150\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.0389 - accuracy: 1.0000 - val_loss: 0.7644 - val_accuracy: 0.7826\n",
      "Epoch 67/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.0383 - accuracy: 1.0000 - val_loss: 0.8060 - val_accuracy: 0.7826\n",
      "Epoch 68/150\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.0381 - accuracy: 1.0000 - val_loss: 0.7880 - val_accuracy: 0.7826\n",
      "Epoch 69/150\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.0382 - accuracy: 1.0000 - val_loss: 0.7916 - val_accuracy: 0.7826\n",
      "Epoch 70/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0370 - accuracy: 1.0000 - val_loss: 0.8243 - val_accuracy: 0.7826\n",
      "Epoch 71/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0371 - accuracy: 1.0000 - val_loss: 0.7992 - val_accuracy: 0.7826\n",
      "Epoch 72/150\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 0.0362 - accuracy: 1.0000 - val_loss: 0.8031 - val_accuracy: 0.7826\n",
      "Epoch 73/150\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 0.0361 - accuracy: 1.0000 - val_loss: 0.7876 - val_accuracy: 0.7826\n",
      "Epoch 74/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0351 - accuracy: 1.0000 - val_loss: 0.8001 - val_accuracy: 0.7826\n",
      "Epoch 75/150\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.0348 - accuracy: 1.0000 - val_loss: 0.7981 - val_accuracy: 0.7826\n",
      "Epoch 76/150\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.0347 - accuracy: 1.0000 - val_loss: 0.7809 - val_accuracy: 0.7826\n",
      "Epoch 77/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0346 - accuracy: 1.0000 - val_loss: 0.7642 - val_accuracy: 0.7826\n",
      "Epoch 78/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0343 - accuracy: 1.0000 - val_loss: 0.7959 - val_accuracy: 0.7826\n",
      "Epoch 79/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.3239 - accuracy: 0.8913 - val_loss: 0.7125 - val_accuracy: 0.7391\n",
      "Epoch 80/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.6566 - accuracy: 0.7935 - val_loss: 0.9203 - val_accuracy: 0.7826\n",
      "Epoch 81/150\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.6914 - accuracy: 0.8261 - val_loss: 0.8650 - val_accuracy: 0.7826\n",
      "Epoch 82/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.5493 - accuracy: 0.9239 - val_loss: 1.0118 - val_accuracy: 0.6522\n",
      "Epoch 83/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.4595 - accuracy: 0.9783 - val_loss: 0.9431 - val_accuracy: 0.7391\n",
      "Epoch 84/150\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 0.4067 - accuracy: 0.9783 - val_loss: 0.9670 - val_accuracy: 0.6522\n",
      "Epoch 85/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.3251 - accuracy: 0.9891 - val_loss: 0.9793 - val_accuracy: 0.7391\n",
      "Epoch 86/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.2885 - accuracy: 0.9891 - val_loss: 0.9559 - val_accuracy: 0.7391\n",
      "Epoch 87/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.2482 - accuracy: 0.9891 - val_loss: 0.8900 - val_accuracy: 0.7826\n",
      "Epoch 88/150\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.2087 - accuracy: 1.0000 - val_loss: 0.8610 - val_accuracy: 0.7391\n",
      "Epoch 89/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.1836 - accuracy: 1.0000 - val_loss: 0.8816 - val_accuracy: 0.7826\n",
      "Epoch 90/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.1657 - accuracy: 1.0000 - val_loss: 0.8631 - val_accuracy: 0.7391\n",
      "Epoch 91/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.1479 - accuracy: 1.0000 - val_loss: 0.8915 - val_accuracy: 0.7826\n",
      "Epoch 92/150\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.1328 - accuracy: 1.0000 - val_loss: 0.8470 - val_accuracy: 0.7391\n",
      "Epoch 93/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.1193 - accuracy: 1.0000 - val_loss: 0.8118 - val_accuracy: 0.7391\n",
      "Epoch 94/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.1080 - accuracy: 1.0000 - val_loss: 0.7966 - val_accuracy: 0.7391\n",
      "Epoch 95/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0981 - accuracy: 1.0000 - val_loss: 0.7862 - val_accuracy: 0.7391\n",
      "Epoch 96/150\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.0898 - accuracy: 1.0000 - val_loss: 0.7731 - val_accuracy: 0.7391\n",
      "Epoch 97/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0831 - accuracy: 1.0000 - val_loss: 0.7614 - val_accuracy: 0.7826\n",
      "Epoch 98/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0767 - accuracy: 1.0000 - val_loss: 0.7678 - val_accuracy: 0.7391\n",
      "Epoch 99/150\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.0717 - accuracy: 1.0000 - val_loss: 0.7598 - val_accuracy: 0.7826\n",
      "Epoch 100/150\n",
      "16/16 [==============================] - 1s 38ms/step - loss: 0.0668 - accuracy: 1.0000 - val_loss: 0.7621 - val_accuracy: 0.7826\n",
      "Epoch 101/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.0625 - accuracy: 1.0000 - val_loss: 0.7575 - val_accuracy: 0.7826\n",
      "Epoch 102/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0593 - accuracy: 1.0000 - val_loss: 0.7518 - val_accuracy: 0.7826\n",
      "Epoch 103/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0562 - accuracy: 1.0000 - val_loss: 0.7716 - val_accuracy: 0.7826\n",
      "Epoch 104/150\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.0536 - accuracy: 1.0000 - val_loss: 0.7659 - val_accuracy: 0.7826\n",
      "Epoch 105/150\n",
      "16/16 [==============================] - 1s 31ms/step - loss: 0.0510 - accuracy: 1.0000 - val_loss: 0.7679 - val_accuracy: 0.7826\n",
      "Epoch 106/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0487 - accuracy: 1.0000 - val_loss: 0.7700 - val_accuracy: 0.7826\n",
      "Epoch 107/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0468 - accuracy: 1.0000 - val_loss: 0.7593 - val_accuracy: 0.7826\n",
      "Epoch 108/150\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.0452 - accuracy: 1.0000 - val_loss: 0.7636 - val_accuracy: 0.7826\n",
      "Epoch 109/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0435 - accuracy: 1.0000 - val_loss: 0.7604 - val_accuracy: 0.7826\n",
      "Epoch 110/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0419 - accuracy: 1.0000 - val_loss: 0.7537 - val_accuracy: 0.7826\n",
      "Epoch 111/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0407 - accuracy: 1.0000 - val_loss: 0.7599 - val_accuracy: 0.7826\n",
      "Epoch 112/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0394 - accuracy: 1.0000 - val_loss: 0.7657 - val_accuracy: 0.7826\n",
      "Epoch 113/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.0382 - accuracy: 1.0000 - val_loss: 0.7659 - val_accuracy: 0.7826\n",
      "Epoch 114/150\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.0374 - accuracy: 1.0000 - val_loss: 0.7720 - val_accuracy: 0.7826\n",
      "Epoch 115/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.0366 - accuracy: 1.0000 - val_loss: 0.7742 - val_accuracy: 0.7826\n",
      "Epoch 116/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0360 - accuracy: 1.0000 - val_loss: 0.7839 - val_accuracy: 0.7826\n",
      "Epoch 117/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.0350 - accuracy: 1.0000 - val_loss: 0.7783 - val_accuracy: 0.7826\n",
      "Epoch 118/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0344 - accuracy: 1.0000 - val_loss: 0.7860 - val_accuracy: 0.7826\n",
      "Epoch 119/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0338 - accuracy: 1.0000 - val_loss: 0.7870 - val_accuracy: 0.7826\n",
      "Epoch 120/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0329 - accuracy: 1.0000 - val_loss: 0.7864 - val_accuracy: 0.7826\n",
      "Epoch 121/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.0326 - accuracy: 1.0000 - val_loss: 0.7842 - val_accuracy: 0.7826\n",
      "Epoch 122/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0320 - accuracy: 1.0000 - val_loss: 0.7951 - val_accuracy: 0.7826\n",
      "Epoch 123/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0316 - accuracy: 1.0000 - val_loss: 0.7884 - val_accuracy: 0.7826\n",
      "Epoch 124/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0309 - accuracy: 1.0000 - val_loss: 0.7835 - val_accuracy: 0.7826\n",
      "Epoch 125/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0311 - accuracy: 1.0000 - val_loss: 0.7898 - val_accuracy: 0.7826\n",
      "Epoch 126/150\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.0300 - accuracy: 1.0000 - val_loss: 0.8070 - val_accuracy: 0.7826\n",
      "Epoch 127/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0298 - accuracy: 1.0000 - val_loss: 0.8089 - val_accuracy: 0.7826\n",
      "Epoch 128/150\n",
      "16/16 [==============================] - 1s 37ms/step - loss: 0.0297 - accuracy: 1.0000 - val_loss: 0.8089 - val_accuracy: 0.7826\n",
      "Epoch 129/150\n",
      "16/16 [==============================] - 1s 38ms/step - loss: 0.0292 - accuracy: 1.0000 - val_loss: 0.8144 - val_accuracy: 0.7826\n",
      "Epoch 130/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.0291 - accuracy: 1.0000 - val_loss: 0.8135 - val_accuracy: 0.7826\n",
      "Epoch 131/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0286 - accuracy: 1.0000 - val_loss: 0.8268 - val_accuracy: 0.7826\n",
      "Epoch 132/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.0284 - accuracy: 1.0000 - val_loss: 0.8239 - val_accuracy: 0.7826\n",
      "Epoch 133/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.0283 - accuracy: 1.0000 - val_loss: 0.8199 - val_accuracy: 0.7826\n",
      "Epoch 134/150\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.0282 - accuracy: 1.0000 - val_loss: 0.8256 - val_accuracy: 0.7826\n",
      "Epoch 135/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0280 - accuracy: 1.0000 - val_loss: 0.8594 - val_accuracy: 0.7826\n",
      "Epoch 136/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0280 - accuracy: 1.0000 - val_loss: 0.8568 - val_accuracy: 0.7826\n",
      "Epoch 137/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0275 - accuracy: 1.0000 - val_loss: 0.8274 - val_accuracy: 0.7826\n",
      "Epoch 138/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0271 - accuracy: 1.0000 - val_loss: 0.8214 - val_accuracy: 0.7826\n",
      "Epoch 139/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0266 - accuracy: 1.0000 - val_loss: 0.8256 - val_accuracy: 0.7826\n",
      "Epoch 140/150\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.0268 - accuracy: 1.0000 - val_loss: 0.8203 - val_accuracy: 0.7826\n",
      "Epoch 141/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0263 - accuracy: 1.0000 - val_loss: 0.8307 - val_accuracy: 0.7826\n",
      "Epoch 142/150\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0265 - accuracy: 1.0000 - val_loss: 0.8262 - val_accuracy: 0.7826\n",
      "Epoch 143/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0260 - accuracy: 1.0000 - val_loss: 0.8335 - val_accuracy: 0.7826\n",
      "Epoch 144/150\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.0260 - accuracy: 1.0000 - val_loss: 0.8246 - val_accuracy: 0.7826\n",
      "Epoch 145/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0257 - accuracy: 1.0000 - val_loss: 0.8423 - val_accuracy: 0.7826\n",
      "Epoch 146/150\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 0.0255 - accuracy: 1.0000 - val_loss: 0.8658 - val_accuracy: 0.7826\n",
      "Epoch 147/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0254 - accuracy: 1.0000 - val_loss: 0.8627 - val_accuracy: 0.7826\n",
      "Epoch 148/150\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.0254 - accuracy: 1.0000 - val_loss: 0.8685 - val_accuracy: 0.7826\n",
      "Epoch 149/150\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.0250 - accuracy: 1.0000 - val_loss: 0.8668 - val_accuracy: 0.7826\n",
      "Epoch 150/150\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.0249 - accuracy: 1.0000 - val_loss: 0.8608 - val_accuracy: 0.7826\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x292067ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "auc_roc: 0.7232142857142858\n",
      "auc_pr: 0.7443669708290935\n",
      "f1_score: 0.7775412992804298\n",
      "mcc: 0.46780074285319984\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "[[9.96192813e-01 3.80721339e-03]\n",
      " [2.08127662e-03 9.97918785e-01]\n",
      " [9.99675155e-01 3.24864755e-04]\n",
      " [1.19446086e-04 9.99880552e-01]\n",
      " [2.16012239e-04 9.99784052e-01]\n",
      " [5.61014967e-05 9.99943852e-01]\n",
      " [9.86284256e-01 1.37157813e-02]\n",
      " [7.17034563e-05 9.99928236e-01]\n",
      " [5.27367629e-05 9.99947309e-01]\n",
      " [9.99722302e-01 2.77678861e-04]\n",
      " [1.16461830e-04 9.99883533e-01]\n",
      " [9.87361073e-01 1.26389628e-02]\n",
      " [7.32145199e-05 9.99926805e-01]\n",
      " [5.42164780e-05 9.99945760e-01]\n",
      " [9.74255890e-05 9.99902606e-01]\n",
      " [3.15426849e-03 9.96845722e-01]\n",
      " [9.99488652e-01 5.11377468e-04]\n",
      " [2.58426392e-03 9.97415721e-01]\n",
      " [1.02400163e-03 9.98975992e-01]\n",
      " [9.03353153e-04 9.99096632e-01]\n",
      " [3.82964528e-04 9.99617040e-01]\n",
      " [7.78989779e-05 9.99922037e-01]\n",
      " [2.35925429e-04 9.99764025e-01]]\n",
      "Model: \"res\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 9, 110, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " res2a-conv1 (Conv2D)           (None, 9, 110, 64)   576         ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " res2a-batchnorm1 (BatchNormali  (None, 9, 110, 64)  256         ['res2a-conv1[0][0]']            \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 9, 110, 64)   0           ['res2a-batchnorm1[0][0]']       \n",
      "                                                                                                  \n",
      " res2a-conv2 (Conv2D)           (None, 9, 110, 64)   32832       ['activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " res2a-batchnorm2 (BatchNormali  (None, 9, 110, 64)  256         ['res2a-conv2[0][0]']            \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 9, 110, 64)   0           ['res2a-batchnorm2[0][0]',       \n",
      "                                                                  'input_5[0][0]']                \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 9, 110, 64)   0           ['add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " res2b-conv1 (Conv2D)           (None, 9, 110, 64)   32832       ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " res2b-batchnorm1 (BatchNormali  (None, 9, 110, 64)  256         ['res2b-conv1[0][0]']            \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 9, 110, 64)   0           ['res2b-batchnorm1[0][0]']       \n",
      "                                                                                                  \n",
      " res2b-conv2 (Conv2D)           (None, 9, 110, 64)   32832       ['activation_18[0][0]']          \n",
      "                                                                                                  \n",
      " res2b-batchnorm2 (BatchNormali  (None, 9, 110, 64)  256         ['res2b-conv2[0][0]']            \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 9, 110, 64)   0           ['res2b-batchnorm2[0][0]',       \n",
      "                                                                  'activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " activation_19 (Activation)     (None, 9, 110, 64)   0           ['add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2 (Conv2D)                 (None, 9, 110, 1)    65          ['activation_19[0][0]']          \n",
      "                                                                                                  \n",
      " flatten_4 (Flatten)            (None, 990)          0           ['conv2[0][0]']                  \n",
      "                                                                                                  \n",
      " fc (Dense)                     (None, 100)          99100       ['flatten_4[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 100)          0           ['fc[0][0]']                     \n",
      "                                                                                                  \n",
      " fc2 (Dense)                    (None, 32)           3232        ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " softmax (Dense)                (None, 2)            66          ['fc2[0][0]']                    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 202,559\n",
      "Trainable params: 202,047\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "data_lst = []\n",
    "\n",
    "# for i in range(k):\n",
    "#     x_train1 = x_train[i]\n",
    "#     y_train1 = y_train[i]\n",
    "#     x_test1 = x_test[i]\n",
    "#     y_test1 = y_test[i]\n",
    "\n",
    "#     model = ResNet(height = x_train1.shape[1], width = x_train1.shape[2], channels = 1, classes = 2)\n",
    "#     model.init_model()\n",
    "\n",
    "#     model.train(x_train1, y_train1, x_test1, y_test1, dataset, use_weights = False)\n",
    "#     y_pred = model.predict(x_test1)\n",
    "#     auc_roc, auc_pr, f1, mcc = model.evaluate(y_test1, y_pred)\n",
    "#     data_lst.append([auc_roc, auc_pr, f1, mcc])\n",
    "    \n",
    "#     #model.model.save_weights(path + \"/model_weights.h5\")\n",
    "\n",
    "#     print(y_test1)\n",
    "#     print(y_pred)\n",
    "    \n",
    "# print(model.model.summary())\n",
    "\n",
    "\n",
    "\n",
    "n_values = np.max(labels) + 1\n",
    "labels_oh = np.eye(n_values)[labels]\n",
    "tree_row = my_maps.shape[1]\n",
    "tree_col = my_maps.shape[2]\n",
    "\n",
    "skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "fold = 0\n",
    "for train_index, test_index in skf.split(my_maps, labels):\n",
    "    train_x, test_x = my_maps[train_index,:,:], my_maps[test_index,:,:]\n",
    "    train_y, test_y = labels_oh[train_index,:], labels_oh[test_index,:]\n",
    "        \n",
    "    train_x = np.log(train_x + 1)\n",
    "    test_x = np.log(test_x + 1)\n",
    "        \n",
    "    c_prob = [0] * len(np.unique(labels))\n",
    "    train_weights = []\n",
    "\n",
    "    for l in np.unique(labels):\n",
    "        a = float(len(labels))\n",
    "        b = 2.0 * float((np.sum(labels==l)))\n",
    "        c_prob[int(l)] = a/b\n",
    "\n",
    "    c_prob = np.array(c_prob).reshape(-1)\n",
    "\n",
    "    for l in np.argmax(train_y, 1):\n",
    "        train_weights.append(c_prob[int(l)])\n",
    "    train_weights = np.array(train_weights)\n",
    "        \n",
    "    scaler = MinMaxScaler().fit(train_x.reshape(-1, tree_row * tree_col))\n",
    "    train_x = np.clip(scaler.transform(train_x.reshape(-1, tree_row * tree_col)), 0, 1).reshape(-1, tree_row, tree_col)\n",
    "    test_x = np.clip(scaler.transform(test_x.reshape(-1, tree_row * tree_col)), 0, 1).reshape(-1, tree_row, tree_col)\n",
    "\n",
    "    train = [train_x, train_y]\n",
    "    test = [test_x, test_y]\n",
    "\n",
    "    x_train1 = train_x\n",
    "    y_train1 = train_y\n",
    "    x_test1 = test_x\n",
    "    y_test1 = test_y\n",
    "        \n",
    "#         y_train1 = train_y\n",
    "#         y_test1 = test_y\n",
    "        \n",
    "#         x_train1 = np.zeros(train_x.shape)\n",
    "#         x_train1[train_x != 0] = 1\n",
    "        \n",
    "#         x_test1 = np.zeros(test_x.shape)\n",
    "#         x_test1[test_x != 0] = 1\n",
    "        \n",
    "        # for i in range(len(train_x)):\n",
    "        #     for j in range(len(test_x)):\n",
    "        #         if np.array_equal(train_x[i], test_x[j]):\n",
    "        #             print('train')\n",
    "        #             print(train_x[i])\n",
    "        #             print('test')\n",
    "        #             print(test_x[j])\n",
    "        \n",
    "        \n",
    "    model = ResNet(height = train_x.shape[1], width = train_x.shape[2], channels = 1, classes = 2)\n",
    "    model.init_model()\n",
    "    model.train(train_x, train_y, test_x, y_test1, dataset, use_weights = False)\n",
    "    y_pred = model.predict(test_x)\n",
    "    auc_roc, auc_pr, f1, mcc = model.evaluate(test_y, y_pred)\n",
    "    data_lst.append([auc_roc, auc_pr, f1, mcc])\n",
    "    #model.model.save_weights(path + \"/model_weights.h5\")\n",
    "    print(test_y)\n",
    "    print(y_pred)\n",
    "    print(model.model.summary())\n",
    "    \n",
    "    fold += 1\n",
    "#run += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying Accuracy Metrics and Saving Metrics\n",
    "\n",
    "Option to save results of all k folds and weights of last model into same directy as data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>auc(roc)</th>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.723214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc(pr)</th>\n",
       "      <td>0.755050</td>\n",
       "      <td>0.929231</td>\n",
       "      <td>0.776096</td>\n",
       "      <td>0.846800</td>\n",
       "      <td>0.744367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.690416</td>\n",
       "      <td>0.819099</td>\n",
       "      <td>0.744306</td>\n",
       "      <td>0.570792</td>\n",
       "      <td>0.777541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mcc</th>\n",
       "      <td>0.310530</td>\n",
       "      <td>0.605598</td>\n",
       "      <td>0.464372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.467801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 1         2         3         4         5\n",
       "auc(roc)  0.733333  0.933333  0.791667  0.821429  0.723214\n",
       "auc(pr)   0.755050  0.929231  0.776096  0.846800  0.744367\n",
       "f1        0.690416  0.819099  0.744306  0.570792  0.777541\n",
       "mcc       0.310530  0.605598  0.464372  0.000000  0.467801"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = [str(i) for i in range(1,k+1)]    \n",
    "results_df = pd.DataFrame(data_lst, columns = ['auc(roc)', 'auc(pr)', 'f1', 'mcc'])\n",
    "results_df = results_df.transpose()\n",
    "results_df.columns = col\n",
    "\n",
    "#results_df.to_csv(path + \"/results.csv\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Model Weights\n",
    "\n",
    "Option to save model weights of last model in k fold into same directy as data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.model.save_weights(path + \"/model_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
