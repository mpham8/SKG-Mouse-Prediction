{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from os.path import abspath\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils.generate_network import generate_network\n",
    "from utils.prepare_data import prepare_data\n",
    "from utils.popphy_io import save_params, load_params\n",
    "from utils.popphy_io import get_stat, get_stat_dict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from models.PopPhy import PopPhyCNN\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import tensorflow as tf\n",
    "#from models.PopPhy2 import ResNet\n",
    "from models.PopPhy2 import ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "### Reading Configuration\n",
    "Configuring which data to read in, minimun threshold needed in an OTU (individual sample must have at least set threshold relative abundance), and how many k folds for k fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'skg2-wt-t14'\n",
    "threshold = 0\n",
    "k = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Features\n",
    "Reduce amount of OTU features by filtering out OTUs that contain no individual sample with a relative abundance greater than the set threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>k__Bacteria|p__Bacteroidota|c__Bacteroidia|o__Bacteroidales|f__Bacteroidaceae|g__Bacteroides|s__Bacteroides_acidifaciens</th>\n",
       "      <td>2071</td>\n",
       "      <td>237</td>\n",
       "      <td>1189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k__Bacteria|p__Firmicutes|c__Bacilli|o__Lactobacillales|f__Lactobacillaceae|g__Lactobacillus|s__Lactobacillus_murinus</th>\n",
       "      <td>150</td>\n",
       "      <td>327</td>\n",
       "      <td>435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k__Bacteria|p__Firmicutes|c__Bacilli|o__Lactobacillales|f__Lactobacillaceae|g__Lactobacillus|s__Lactobacillus_johnsonii</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k__Bacteria|p__Bacteroidota|c__Bacteroidia|o__Bacteroidales|f__Rikenellaceae|g__Alistipes|s__Alistipes_unclassified</th>\n",
       "      <td>268</td>\n",
       "      <td>900</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k__Bacteria|p__Bacteroidota|c__Bacteroidia|o__Bacteroidales|f__Bacteroidaceae|g__Bacteroides|s__Bacteroides_vulgatus</th>\n",
       "      <td>1109</td>\n",
       "      <td>192</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k__Bacteria|p__Firmicutes|c__Clostridia|o__Lachnospirales|f__Lachnospiraceae|g__28-4|s__28-4_unclassified</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k__Bacteria|p__Firmicutes|c__Clostridia|o__Oscillospirales|f__Butyricicoccaceae|g__Butyricicoccus|s__Butyricicoccus_unclassified</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k__Bacteria|p__Firmicutes|c__Clostridia|o__Oscillospirales|f__Ruminococcaceae|g__Incertae Sedis|s__Incertae Sedis_unclassified</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k__Bacteria|p__Firmicutes|c__Clostridia|o__Oscillospirales|f__Butyricicoccaceae|g__Butyricicoccus|s__Butyricicoccus_unclassified</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k__Bacteria|p__Firmicutes|c__Clostridia|o__Lachnospirales|f__Lachnospiraceae|g__unclassified|s__unclassified_unclassified</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>297 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       1    2     3\n",
       "0                                                                  \n",
       "k__Bacteria|p__Bacteroidota|c__Bacteroidia|o__B...  2071  237  1189\n",
       "k__Bacteria|p__Firmicutes|c__Bacilli|o__Lactoba...   150  327   435\n",
       "k__Bacteria|p__Firmicutes|c__Bacilli|o__Lactoba...     3    0     0\n",
       "k__Bacteria|p__Bacteroidota|c__Bacteroidia|o__B...   268  900   427\n",
       "k__Bacteria|p__Bacteroidota|c__Bacteroidia|o__B...  1109  192   264\n",
       "...                                                  ...  ...   ...\n",
       "k__Bacteria|p__Firmicutes|c__Clostridia|o__Lach...     0   13     0\n",
       "k__Bacteria|p__Firmicutes|c__Clostridia|o__Osci...     0   13     0\n",
       "k__Bacteria|p__Firmicutes|c__Clostridia|o__Osci...     0   12     0\n",
       "k__Bacteria|p__Firmicutes|c__Clostridia|o__Osci...     0    0     3\n",
       "k__Bacteria|p__Firmicutes|c__Clostridia|o__Lach...     0    0    11\n",
       "\n",
       "[297 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../data/\" + dataset\n",
    "data = pd.read_csv(path + '/abundance.tsv', index_col=0, sep='\\t', header=None)\n",
    "to_drop = data.loc[(data < threshold).all(axis=1)]\n",
    "data = data.drop(to_drop.index)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 2d Matrix Representing OTU Data\n",
    "Dai et al. PopPhy-CNN's (2019) algorithm creates Phylogenetic tree from OTUs and populates tree based on OTU abundances. This tree graph structure is then converted to a 2d Matrix by taking each parent node in the tree graph and pushing them all to the left and childrens' nodes in the same order from left to right the parents were ordered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(297, 3)\n",
      "There are 297 raw features...\n",
      "Building tree structure...\n",
      "Tree file not found...\n",
      "Constructing tree..\n",
      "Pruning Tree...\n",
      "Populating trees...\n",
      "There are 199 tree features...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>262</th>\n",
       "      <th>263</th>\n",
       "      <th>264</th>\n",
       "      <th>265</th>\n",
       "      <th>266</th>\n",
       "      <th>267</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.408199</td>\n",
       "      <td>0.372348</td>\n",
       "      <td>0.019441</td>\n",
       "      <td>0.000629</td>\n",
       "      <td>0.198925</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.047630</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>0.359998</td>\n",
       "      <td>0.372348</td>\n",
       "      <td>0.019441</td>\n",
       "      <td>0.000629</td>\n",
       "      <td>0.196180</td>\n",
       "      <td>0.002516</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010349</td>\n",
       "      <td>0.037281</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>0.359998</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.372348</td>\n",
       "      <td>0.019441</td>\n",
       "      <td>0.000629</td>\n",
       "      <td>0.185145</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.010349</td>\n",
       "      <td>0.037281</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>0.359998</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.372348</td>\n",
       "      <td>0.019441</td>\n",
       "      <td>0.000629</td>\n",
       "      <td>0.181543</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.010349</td>\n",
       "      <td>0.037281</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>0.359998</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.372348</td>\n",
       "      <td>0.019441</td>\n",
       "      <td>0.000629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.005832</td>\n",
       "      <td>0.004517</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037281</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013723</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.005832</td>\n",
       "      <td>0.001887</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006461</td>\n",
       "      <td>0.013723</td>\n",
       "      <td>0.215107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.002058</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001487</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012236</td>\n",
       "      <td>0.002459</td>\n",
       "      <td>0.063411</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009606</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 272 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  1.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  1.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.408199  0.372348  0.019441  0.000629  0.198925  0.000457  0.000000   \n",
       "3  0.047630  0.000572  0.359998  0.372348  0.019441  0.000629  0.196180   \n",
       "4  0.010349  0.037281  0.000572  0.359998  0.000000  0.372348  0.019441   \n",
       "5  0.010349  0.037281  0.000572  0.359998  0.000000  0.000000  0.372348   \n",
       "6  0.010349  0.037281  0.000572  0.359998  0.000000  0.000000  0.372348   \n",
       "7  0.005832  0.004517  0.000000  0.037281  0.000000  0.000000  0.000000   \n",
       "8  0.005832  0.001887  0.000000  0.000000  0.000000  0.006461  0.013723   \n",
       "9  0.001487  0.000000  0.012236  0.002459  0.063411  0.000000  0.009606   \n",
       "\n",
       "        7         8         9    ...       262       263       264  265  266  \\\n",
       "0  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.0  0.0   \n",
       "1  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.0  0.0   \n",
       "2  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.0  0.0   \n",
       "3  0.002516  0.000000  0.000229  ...  0.000000  0.000000  0.000000  0.0  0.0   \n",
       "4  0.000629  0.185145  0.000229  ...  0.000000  0.000000  0.000000  0.0  0.0   \n",
       "5  0.019441  0.000629  0.181543  ...  0.000000  0.000000  0.000000  0.0  0.0   \n",
       "6  0.019441  0.000629  0.000000  ...  0.000000  0.000000  0.000000  0.0  0.0   \n",
       "7  0.000000  0.000000  0.013723  ...  0.000000  0.000000  0.000000  0.0  0.0   \n",
       "8  0.215107  0.000000  0.000000  ...  0.000172  0.002058  0.000229  0.0  0.0   \n",
       "9  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.0  0.0   \n",
       "\n",
       "   267  268  269  270       271  \n",
       "0  0.0  0.0  0.0  0.0  0.000000  \n",
       "1  0.0  0.0  0.0  0.0  0.000000  \n",
       "2  0.0  0.0  0.0  0.0  0.000000  \n",
       "3  0.0  0.0  0.0  0.0  0.000000  \n",
       "4  0.0  0.0  0.0  0.0  0.000000  \n",
       "5  0.0  0.0  0.0  0.0  0.000000  \n",
       "6  0.0  0.0  0.0  0.0  0.000000  \n",
       "7  0.0  0.0  0.0  0.0  0.000000  \n",
       "8  0.0  0.0  0.0  0.0  0.000229  \n",
       "9  0.0  0.0  0.0  0.0  0.000000  \n",
       "\n",
       "[10 rows x 272 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_maps, raw_x, tree_x, raw_features, tree_features, labels, label_set, g, feature_df = prepare_data(path, data)\n",
    "\n",
    "# norms = np.linalg.norm(my_maps, axis=2, keepdims=True)\n",
    "# my_maps = my_maps / norms\n",
    "pd.DataFrame(my_maps[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating training and test sets\n",
    "Splitting data into k training and k test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding\n",
    "input = my_maps\n",
    "target = tf.keras.utils.to_categorical(labels, 2, dtype='int64')\n",
    "    \n",
    "\n",
    "# #shuffle dataset\n",
    "# seed = np.random.randint(100)\n",
    "# # np.random.seed(seed)\n",
    "# # np.random.shuffle(input)\n",
    "# np.random.seed(seed)\n",
    "# np.random.shuffle(target)\n",
    "\n",
    "# np.random.seed(seed)\n",
    "# np.random.shuffle(my_maps)\n",
    "# np.random.seed(seed)\n",
    "# np.random.shuffle(raw_x)\n",
    "# np.random.seed(seed)\n",
    "# np.random.shuffle(tree_x)\n",
    "# np.random.seed(seed)\n",
    "# np.random.shuffle(labels)\n",
    "\n",
    "\n",
    "#create k training and k test sets\n",
    "# groups_input = []\n",
    "# groups_target = []\n",
    "# k_size = len(input)//k\n",
    "# start, end = 0, k_size\n",
    "# for i in range(k):\n",
    "#     if i == k-1:\n",
    "#         group_input = input[start:]\n",
    "#         group_target = target[start:]\n",
    "#     else:\n",
    "#         group_input = input[start:end]\n",
    "#         group_target = target[start:end]\n",
    "#     start += k_size\n",
    "#     end += k_size\n",
    "#     groups_input.append(group_input)\n",
    "#     groups_target.append(group_target)\n",
    "\n",
    "# x_train = []\n",
    "# y_train = []\n",
    "# x_test = []\n",
    "# y_test = []\n",
    "# for i in range(k-1, -1, -1):\n",
    "#     x_train.append(np.concatenate((groups_input[i-1], groups_input[i-2], groups_input[i-3], groups_input[i-4])))\n",
    "#     y_train.append(np.concatenate((groups_target[i-1], groups_target[i-2], groups_target[i-3], groups_target[i-4])))\n",
    "\n",
    "#     x_test.append(groups_input[i])\n",
    "#     y_test.append(groups_target[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "### Training model\n",
    "Data is log transformed and then a MinMax transformation. Uses CNN that employs skipped residual identity blocks borrowed from the classic ResNet model then a FC Neural Network to make phenotype prediction. Model dimensions printed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "Epoch 1/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 3.1814 - accuracy: 0.0000e+00 - val_loss: 3.0385 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/150\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.8313 - accuracy: 1.0000 - val_loss: 2.8927 - val_accuracy: 1.0000\n",
      "Epoch 3/150\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 2.6251 - accuracy: 1.0000 - val_loss: 2.7516 - val_accuracy: 1.0000\n",
      "Epoch 4/150\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 2.4721 - accuracy: 1.0000 - val_loss: 2.6162 - val_accuracy: 1.0000\n",
      "Epoch 5/150\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 2.3337 - accuracy: 1.0000 - val_loss: 2.4873 - val_accuracy: 1.0000\n",
      "Epoch 6/150\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 2.2013 - accuracy: 1.0000 - val_loss: 2.3650 - val_accuracy: 1.0000\n",
      "Epoch 7/150\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 2.0797 - accuracy: 1.0000 - val_loss: 2.2494 - val_accuracy: 1.0000\n",
      "Epoch 8/150\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 1.9664 - accuracy: 1.0000 - val_loss: 2.1398 - val_accuracy: 1.0000\n",
      "Epoch 9/150\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 1.8618 - accuracy: 1.0000 - val_loss: 2.0361 - val_accuracy: 1.0000\n",
      "Epoch 10/150\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 1.7662 - accuracy: 1.0000 - val_loss: 1.9379 - val_accuracy: 1.0000\n",
      "Epoch 11/150\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 1.6764 - accuracy: 1.0000 - val_loss: 1.8451 - val_accuracy: 1.0000\n",
      "Epoch 12/150\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 1.5919 - accuracy: 1.0000 - val_loss: 1.7576 - val_accuracy: 1.0000\n",
      "Epoch 13/150\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 1.5121 - accuracy: 1.0000 - val_loss: 1.6750 - val_accuracy: 1.0000\n",
      "Epoch 14/150\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 1.4363 - accuracy: 1.0000 - val_loss: 1.5973 - val_accuracy: 1.0000\n",
      "Epoch 15/150\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 1.3644 - accuracy: 1.0000 - val_loss: 1.5243 - val_accuracy: 1.0000\n",
      "Epoch 16/150\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 1.2962 - accuracy: 1.0000 - val_loss: 1.4558 - val_accuracy: 1.0000\n",
      "Epoch 17/150\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 1.2324 - accuracy: 1.0000 - val_loss: 1.3918 - val_accuracy: 1.0000\n",
      "Epoch 18/150\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 1.1724 - accuracy: 1.0000 - val_loss: 1.3317 - val_accuracy: 1.0000\n",
      "Epoch 19/150\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 1.1172 - accuracy: 1.0000 - val_loss: 1.2754 - val_accuracy: 1.0000\n",
      "Epoch 20/150\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 1.0657 - accuracy: 1.0000 - val_loss: 1.2228 - val_accuracy: 1.0000\n",
      "Epoch 21/150\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 1.0174 - accuracy: 1.0000 - val_loss: 1.1734 - val_accuracy: 1.0000\n",
      "Epoch 22/150\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.9724 - accuracy: 1.0000 - val_loss: 1.1273 - val_accuracy: 1.0000\n",
      "Epoch 23/150\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.9319 - accuracy: 1.0000 - val_loss: 1.0842 - val_accuracy: 1.0000\n",
      "Epoch 24/150\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.8917 - accuracy: 1.0000 - val_loss: 1.0440 - val_accuracy: 1.0000\n",
      "Epoch 25/150\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.8556 - accuracy: 1.0000 - val_loss: 1.0064 - val_accuracy: 1.0000\n",
      "Epoch 26/150\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.8222 - accuracy: 1.0000 - val_loss: 0.9713 - val_accuracy: 1.0000\n",
      "Epoch 27/150\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 0.7905 - accuracy: 1.0000 - val_loss: 0.9387 - val_accuracy: 1.0000\n",
      "Epoch 28/150\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.7609 - accuracy: 1.0000 - val_loss: 0.9081 - val_accuracy: 1.0000\n",
      "Epoch 29/150\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.7337 - accuracy: 1.0000 - val_loss: 0.8796 - val_accuracy: 1.0000\n",
      "Epoch 30/150\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.7083 - accuracy: 1.0000 - val_loss: 0.8530 - val_accuracy: 1.0000\n",
      "Epoch 31/150\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 0.6847 - accuracy: 1.0000 - val_loss: 0.8282 - val_accuracy: 1.0000\n",
      "Epoch 32/150\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.6628 - accuracy: 1.0000 - val_loss: 0.8050 - val_accuracy: 1.0000\n",
      "Epoch 33/150\n",
      "1/1 [==============================] - 0s 367ms/step - loss: 0.6425 - accuracy: 1.0000 - val_loss: 0.7833 - val_accuracy: 1.0000\n",
      "Epoch 34/150\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.6234 - accuracy: 1.0000 - val_loss: 0.7630 - val_accuracy: 1.0000\n",
      "Epoch 35/150\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.6059 - accuracy: 1.0000 - val_loss: 0.7441 - val_accuracy: 1.0000\n",
      "Epoch 36/150\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.5894 - accuracy: 1.0000 - val_loss: 0.7263 - val_accuracy: 1.0000\n",
      "Epoch 37/150\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.5741 - accuracy: 1.0000 - val_loss: 0.7097 - val_accuracy: 1.0000\n",
      "Epoch 38/150\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 0.5599 - accuracy: 1.0000 - val_loss: 0.6942 - val_accuracy: 1.0000\n",
      "Epoch 39/150\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 0.5466 - accuracy: 1.0000 - val_loss: 0.6796 - val_accuracy: 1.0000\n",
      "Epoch 40/150\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.5343 - accuracy: 1.0000 - val_loss: 0.6659 - val_accuracy: 1.0000\n",
      "Epoch 41/150\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.5227 - accuracy: 1.0000 - val_loss: 0.6531 - val_accuracy: 1.0000\n",
      "Epoch 42/150\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.5119 - accuracy: 1.0000 - val_loss: 0.6410 - val_accuracy: 1.0000\n",
      "Epoch 43/150\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.5018 - accuracy: 1.0000 - val_loss: 0.6296 - val_accuracy: 1.0000\n",
      "Epoch 44/150\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.4923 - accuracy: 1.0000 - val_loss: 0.6188 - val_accuracy: 1.0000\n",
      "Epoch 45/150\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.4834 - accuracy: 1.0000 - val_loss: 0.6086 - val_accuracy: 1.0000\n",
      "Epoch 46/150\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.4750 - accuracy: 1.0000 - val_loss: 0.5990 - val_accuracy: 1.0000\n",
      "Epoch 47/150\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.4670 - accuracy: 1.0000 - val_loss: 0.5899 - val_accuracy: 1.0000\n",
      "Epoch 48/150\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.4595 - accuracy: 1.0000 - val_loss: 0.5812 - val_accuracy: 1.0000\n",
      "Epoch 49/150\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.4524 - accuracy: 1.0000 - val_loss: 0.5729 - val_accuracy: 1.0000\n",
      "Epoch 50/150\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 0.4456 - accuracy: 1.0000 - val_loss: 0.5650 - val_accuracy: 1.0000\n",
      "Epoch 51/150\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.4392 - accuracy: 1.0000 - val_loss: 0.5574 - val_accuracy: 1.0000\n",
      "Epoch 52/150\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.4330 - accuracy: 1.0000 - val_loss: 0.5501 - val_accuracy: 1.0000\n",
      "Epoch 53/150\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.4271 - accuracy: 1.0000 - val_loss: 0.5431 - val_accuracy: 1.0000\n",
      "Epoch 54/150\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.4214 - accuracy: 1.0000 - val_loss: 0.5364 - val_accuracy: 1.0000\n",
      "Epoch 55/150\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.4160 - accuracy: 1.0000 - val_loss: 0.5299 - val_accuracy: 1.0000\n",
      "Epoch 56/150\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.4107 - accuracy: 1.0000 - val_loss: 0.5236 - val_accuracy: 1.0000\n",
      "Epoch 57/150\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.4056 - accuracy: 1.0000 - val_loss: 0.5174 - val_accuracy: 1.0000\n",
      "Epoch 58/150\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.4007 - accuracy: 1.0000 - val_loss: 0.5115 - val_accuracy: 1.0000\n",
      "Epoch 59/150\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.3959 - accuracy: 1.0000 - val_loss: 0.5057 - val_accuracy: 1.0000\n",
      "Epoch 60/150\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.3913 - accuracy: 1.0000 - val_loss: 0.5001 - val_accuracy: 1.0000\n",
      "Epoch 61/150\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.3867 - accuracy: 1.0000 - val_loss: 0.4946 - val_accuracy: 1.0000\n",
      "Epoch 62/150\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.3823 - accuracy: 1.0000 - val_loss: 0.4892 - val_accuracy: 1.0000\n",
      "Epoch 63/150\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.3779 - accuracy: 1.0000 - val_loss: 0.4840 - val_accuracy: 1.0000\n",
      "Epoch 64/150\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.3737 - accuracy: 1.0000 - val_loss: 0.4788 - val_accuracy: 1.0000\n",
      "Epoch 65/150\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.3695 - accuracy: 1.0000 - val_loss: 0.4738 - val_accuracy: 1.0000\n",
      "Epoch 66/150\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.3655 - accuracy: 1.0000 - val_loss: 0.4688 - val_accuracy: 1.0000\n",
      "Epoch 67/150\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.3614 - accuracy: 1.0000 - val_loss: 0.4640 - val_accuracy: 1.0000\n",
      "Epoch 68/150\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.3575 - accuracy: 1.0000 - val_loss: 0.4592 - val_accuracy: 1.0000\n",
      "Epoch 69/150\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.3536 - accuracy: 1.0000 - val_loss: 0.4545 - val_accuracy: 1.0000\n",
      "Epoch 70/150\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.3498 - accuracy: 1.0000 - val_loss: 0.4498 - val_accuracy: 1.0000\n",
      "Epoch 71/150\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.3460 - accuracy: 1.0000 - val_loss: 0.4453 - val_accuracy: 1.0000\n",
      "Epoch 72/150\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.3423 - accuracy: 1.0000 - val_loss: 0.4407 - val_accuracy: 1.0000\n",
      "Epoch 73/150\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.3386 - accuracy: 1.0000 - val_loss: 0.4363 - val_accuracy: 1.0000\n",
      "Epoch 74/150\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.3350 - accuracy: 1.0000 - val_loss: 0.4319 - val_accuracy: 1.0000\n",
      "Epoch 75/150\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.3314 - accuracy: 1.0000 - val_loss: 0.4276 - val_accuracy: 1.0000\n",
      "Epoch 76/150\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.3279 - accuracy: 1.0000 - val_loss: 0.4233 - val_accuracy: 1.0000\n",
      "Epoch 77/150\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.3244 - accuracy: 1.0000 - val_loss: 0.4191 - val_accuracy: 1.0000\n",
      "Epoch 78/150\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.3209 - accuracy: 1.0000 - val_loss: 0.4150 - val_accuracy: 1.0000\n",
      "Epoch 79/150\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.3175 - accuracy: 1.0000 - val_loss: 0.4108 - val_accuracy: 1.0000\n",
      "Epoch 80/150\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.3141 - accuracy: 1.0000 - val_loss: 0.4068 - val_accuracy: 1.0000\n",
      "Epoch 81/150\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.3108 - accuracy: 1.0000 - val_loss: 0.4028 - val_accuracy: 1.0000\n",
      "Epoch 82/150\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.3075 - accuracy: 1.0000 - val_loss: 0.3988 - val_accuracy: 1.0000\n",
      "Epoch 83/150\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.3041 - accuracy: 1.0000 - val_loss: 0.3949 - val_accuracy: 1.0000\n",
      "Epoch 84/150\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.3009 - accuracy: 1.0000 - val_loss: 0.3910 - val_accuracy: 1.0000\n",
      "Epoch 85/150\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.2977 - accuracy: 1.0000 - val_loss: 0.3872 - val_accuracy: 1.0000\n",
      "Epoch 86/150\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.2946 - accuracy: 1.0000 - val_loss: 0.3834 - val_accuracy: 1.0000\n",
      "Epoch 87/150\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.2915 - accuracy: 1.0000 - val_loss: 0.3796 - val_accuracy: 1.0000\n",
      "Epoch 88/150\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.2884 - accuracy: 1.0000 - val_loss: 0.3759 - val_accuracy: 1.0000\n",
      "Epoch 89/150\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.2853 - accuracy: 1.0000 - val_loss: 0.3722 - val_accuracy: 1.0000\n",
      "Epoch 90/150\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.2823 - accuracy: 1.0000 - val_loss: 0.3685 - val_accuracy: 1.0000\n",
      "Epoch 91/150\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.2793 - accuracy: 1.0000 - val_loss: 0.3649 - val_accuracy: 1.0000\n",
      "Epoch 92/150\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.2764 - accuracy: 1.0000 - val_loss: 0.3613 - val_accuracy: 1.0000\n",
      "Epoch 93/150\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.2734 - accuracy: 1.0000 - val_loss: 0.3578 - val_accuracy: 1.0000\n",
      "Epoch 94/150\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.2705 - accuracy: 1.0000 - val_loss: 0.3543 - val_accuracy: 1.0000\n",
      "Epoch 95/150\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.2677 - accuracy: 1.0000 - val_loss: 0.3508 - val_accuracy: 1.0000\n",
      "Epoch 96/150\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.2649 - accuracy: 1.0000 - val_loss: 0.3474 - val_accuracy: 1.0000\n",
      "Epoch 97/150\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.2621 - accuracy: 1.0000 - val_loss: 0.3440 - val_accuracy: 1.0000\n",
      "Epoch 98/150\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.2593 - accuracy: 1.0000 - val_loss: 0.3406 - val_accuracy: 1.0000\n",
      "Epoch 99/150\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.2566 - accuracy: 1.0000 - val_loss: 0.3373 - val_accuracy: 1.0000\n",
      "Epoch 100/150\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 0.2539 - accuracy: 1.0000 - val_loss: 0.3341 - val_accuracy: 1.0000\n",
      "Epoch 101/150\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 0.2512 - accuracy: 1.0000 - val_loss: 0.3308 - val_accuracy: 1.0000\n",
      "Epoch 102/150\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.2486 - accuracy: 1.0000 - val_loss: 0.3276 - val_accuracy: 1.0000\n",
      "Epoch 103/150\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.2460 - accuracy: 1.0000 - val_loss: 0.3244 - val_accuracy: 1.0000\n",
      "Epoch 104/150\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.2434 - accuracy: 1.0000 - val_loss: 0.3213 - val_accuracy: 1.0000\n",
      "Epoch 105/150\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.2409 - accuracy: 1.0000 - val_loss: 0.3182 - val_accuracy: 1.0000\n",
      "Epoch 106/150\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.2383 - accuracy: 1.0000 - val_loss: 0.3152 - val_accuracy: 1.0000\n",
      "Epoch 107/150\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.2359 - accuracy: 1.0000 - val_loss: 0.3121 - val_accuracy: 1.0000\n",
      "Epoch 108/150\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.2334 - accuracy: 1.0000 - val_loss: 0.3091 - val_accuracy: 1.0000\n",
      "Epoch 109/150\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.2310 - accuracy: 1.0000 - val_loss: 0.3062 - val_accuracy: 1.0000\n",
      "Epoch 110/150\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.2286 - accuracy: 1.0000 - val_loss: 0.3033 - val_accuracy: 1.0000\n",
      "Epoch 111/150\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.2262 - accuracy: 1.0000 - val_loss: 0.3004 - val_accuracy: 1.0000\n",
      "Epoch 112/150\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.2238 - accuracy: 1.0000 - val_loss: 0.2975 - val_accuracy: 1.0000\n",
      "Epoch 113/150\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 0.2215 - accuracy: 1.0000 - val_loss: 0.2947 - val_accuracy: 1.0000\n",
      "Epoch 114/150\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.2192 - accuracy: 1.0000 - val_loss: 0.2919 - val_accuracy: 1.0000\n",
      "Epoch 115/150\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.2169 - accuracy: 1.0000 - val_loss: 0.2891 - val_accuracy: 1.0000\n",
      "Epoch 116/150\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.2147 - accuracy: 1.0000 - val_loss: 0.2864 - val_accuracy: 1.0000\n",
      "Epoch 117/150\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.2125 - accuracy: 1.0000 - val_loss: 0.2837 - val_accuracy: 1.0000\n",
      "Epoch 118/150\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.2103 - accuracy: 1.0000 - val_loss: 0.2810 - val_accuracy: 1.0000\n",
      "Epoch 119/150\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 0.2081 - accuracy: 1.0000 - val_loss: 0.2783 - val_accuracy: 1.0000\n",
      "Epoch 120/150\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.2060 - accuracy: 1.0000 - val_loss: 0.2757 - val_accuracy: 1.0000\n",
      "Epoch 121/150\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.2038 - accuracy: 1.0000 - val_loss: 0.2731 - val_accuracy: 1.0000\n",
      "Epoch 122/150\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.2017 - accuracy: 1.0000 - val_loss: 0.2706 - val_accuracy: 1.0000\n",
      "Epoch 123/150\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.1996 - accuracy: 1.0000 - val_loss: 0.2680 - val_accuracy: 1.0000\n",
      "Epoch 124/150\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.1976 - accuracy: 1.0000 - val_loss: 0.2655 - val_accuracy: 1.0000\n",
      "Epoch 125/150\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.1956 - accuracy: 1.0000 - val_loss: 0.2630 - val_accuracy: 1.0000\n",
      "Epoch 126/150\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.1935 - accuracy: 1.0000 - val_loss: 0.2606 - val_accuracy: 1.0000\n",
      "Epoch 127/150\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.1916 - accuracy: 1.0000 - val_loss: 0.2581 - val_accuracy: 1.0000\n",
      "Epoch 128/150\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.1896 - accuracy: 1.0000 - val_loss: 0.2557 - val_accuracy: 1.0000\n",
      "Epoch 129/150\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.1877 - accuracy: 1.0000 - val_loss: 0.2533 - val_accuracy: 1.0000\n",
      "Epoch 130/150\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 0.1857 - accuracy: 1.0000 - val_loss: 0.2510 - val_accuracy: 1.0000\n",
      "Epoch 131/150\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.1838 - accuracy: 1.0000 - val_loss: 0.2486 - val_accuracy: 1.0000\n",
      "Epoch 132/150\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.1820 - accuracy: 1.0000 - val_loss: 0.2463 - val_accuracy: 1.0000\n",
      "Epoch 133/150\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.1801 - accuracy: 1.0000 - val_loss: 0.2440 - val_accuracy: 1.0000\n",
      "Epoch 134/150\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.1782 - accuracy: 1.0000 - val_loss: 0.2418 - val_accuracy: 1.0000\n",
      "Epoch 135/150\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.1764 - accuracy: 1.0000 - val_loss: 0.2395 - val_accuracy: 1.0000\n",
      "Epoch 136/150\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.1746 - accuracy: 1.0000 - val_loss: 0.2373 - val_accuracy: 1.0000\n",
      "Epoch 137/150\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.1728 - accuracy: 1.0000 - val_loss: 0.2351 - val_accuracy: 1.0000\n",
      "Epoch 138/150\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.1711 - accuracy: 1.0000 - val_loss: 0.2329 - val_accuracy: 1.0000\n",
      "Epoch 139/150\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.1693 - accuracy: 1.0000 - val_loss: 0.2307 - val_accuracy: 1.0000\n",
      "Epoch 140/150\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.1676 - accuracy: 1.0000 - val_loss: 0.2286 - val_accuracy: 1.0000\n",
      "Epoch 141/150\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.1659 - accuracy: 1.0000 - val_loss: 0.2265 - val_accuracy: 1.0000\n",
      "Epoch 142/150\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.1642 - accuracy: 1.0000 - val_loss: 0.2244 - val_accuracy: 1.0000\n",
      "Epoch 143/150\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.1626 - accuracy: 1.0000 - val_loss: 0.2223 - val_accuracy: 1.0000\n",
      "Epoch 144/150\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.1609 - accuracy: 1.0000 - val_loss: 0.2203 - val_accuracy: 1.0000\n",
      "Epoch 145/150\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.1593 - accuracy: 1.0000 - val_loss: 0.2183 - val_accuracy: 1.0000\n",
      "Epoch 146/150\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.1577 - accuracy: 1.0000 - val_loss: 0.2163 - val_accuracy: 1.0000\n",
      "Epoch 147/150\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.1561 - accuracy: 1.0000 - val_loss: 0.2143 - val_accuracy: 1.0000\n",
      "Epoch 148/150\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.1545 - accuracy: 1.0000 - val_loss: 0.2123 - val_accuracy: 1.0000\n",
      "Epoch 149/150\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.1529 - accuracy: 1.0000 - val_loss: 0.2104 - val_accuracy: 1.0000\n",
      "Epoch 150/150\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.1514 - accuracy: 1.0000 - val_loss: 0.2084 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 138ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Only one class present in y_true. ROC AUC score is not defined in that case.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 102\u001b[0m\n\u001b[1;32m    100\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28minput\u001b[39m, target, \u001b[38;5;28minput\u001b[39m, target, dataset, use_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    101\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m--> 102\u001b[0m auc_roc, auc_pr, f1, mcc \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m data_lst\u001b[38;5;241m.\u001b[39mappend([auc_roc, auc_pr, f1, mcc])\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m#model.model.save_weights(path + \"/model_weights.h5\")\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/SKG-Mouse-Prediction/src/models/PopPhy2.py:252\u001b[0m, in \u001b[0;36mResNet.evaluate\u001b[0;34m(self, y_test, y_pred)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03mComputes evaluation metrics\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03m  mcc (float): Matthews correlation coefficient (MCC) Score\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# Compute the AUC of the ROC curve\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m auc_roc \u001b[38;5;241m=\u001b[39m \u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m# Compute the AUC of the PR curve\u001b[39;00m\n\u001b[1;32m    255\u001b[0m auc_pr \u001b[38;5;241m=\u001b[39m average_precision_score(y_test, y_pred)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:580\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _average_binary_score(\n\u001b[1;32m    573\u001b[0m         partial(_binary_roc_auc_score, max_fpr\u001b[38;5;241m=\u001b[39mmax_fpr),\n\u001b[1;32m    574\u001b[0m         y_true,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    577\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[1;32m    578\u001b[0m     )\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# multilabel-indicator\u001b[39;00m\n\u001b[0;32m--> 580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_average_binary_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_binary_roc_auc_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_fpr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_fpr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_base.py:118\u001b[0m, in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m    116\u001b[0m     y_true_c \u001b[38;5;241m=\u001b[39m y_true\u001b[38;5;241m.\u001b[39mtake([c], axis\u001b[38;5;241m=\u001b[39mnot_average_axis)\u001b[38;5;241m.\u001b[39mravel()\n\u001b[1;32m    117\u001b[0m     y_score_c \u001b[38;5;241m=\u001b[39m y_score\u001b[38;5;241m.\u001b[39mtake([c], axis\u001b[38;5;241m=\u001b[39mnot_average_axis)\u001b[38;5;241m.\u001b[39mravel()\n\u001b[0;32m--> 118\u001b[0m     score[c] \u001b[38;5;241m=\u001b[39m \u001b[43mbinary_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscore_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Average the results\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:339\u001b[0m, in \u001b[0;36m_binary_roc_auc_score\u001b[0;34m(y_true, y_score, sample_weight, max_fpr)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;124;03m\"\"\"Binary roc auc score.\"\"\"\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y_true)) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 339\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    340\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly one class present in y_true. ROC AUC score \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    341\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis not defined in that case.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    342\u001b[0m     )\n\u001b[1;32m    344\u001b[0m fpr, tpr, _ \u001b[38;5;241m=\u001b[39m roc_curve(y_true, y_score, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_fpr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m max_fpr \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: Only one class present in y_true. ROC AUC score is not defined in that case."
     ]
    }
   ],
   "source": [
    "data_lst = []\n",
    "\n",
    "# for i in range(k):\n",
    "#     x_train1 = x_train[i]\n",
    "#     y_train1 = y_train[i]\n",
    "#     x_test1 = x_test[i]\n",
    "#     y_test1 = y_test[i]\n",
    "\n",
    "#     model = ResNet(height = x_train1.shape[1], width = x_train1.shape[2], channels = 1, classes = 2)\n",
    "#     model.init_model()\n",
    "\n",
    "#     model.train(x_train1, y_train1, x_test1, y_test1, dataset, use_weights = False)\n",
    "#     y_pred = model.predict(x_test1)\n",
    "#     auc_roc, auc_pr, f1, mcc = model.evaluate(y_test1, y_pred)\n",
    "#     data_lst.append([auc_roc, auc_pr, f1, mcc])\n",
    "    \n",
    "#     #model.model.save_weights(path + \"/model_weights.h5\")\n",
    "\n",
    "#     print(y_test1)\n",
    "#     print(y_pred)\n",
    "    \n",
    "# print(model.model.summary())\n",
    "\n",
    "\n",
    "\n",
    "# n_values = np.max(labels) + 1\n",
    "# labels_oh = np.eye(n_values)[labels]\n",
    "# tree_row = my_maps.shape[1]\n",
    "# tree_col = my_maps.shape[2]\n",
    "\n",
    "# skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "# fold = 0\n",
    "# for train_index, test_index in skf.split(my_maps, labels):\n",
    "#     train_x, test_x = my_maps[train_index,:,:], my_maps[test_index,:,:]\n",
    "#     train_y, test_y = labels_oh[train_index,:], labels_oh[test_index,:]\n",
    "        \n",
    "#     train_x = np.log(train_x + 1)\n",
    "#     test_x = np.log(test_x + 1)\n",
    "        \n",
    "#     c_prob = [0] * len(np.unique(labels))\n",
    "#     train_weights = []\n",
    "\n",
    "#     for l in np.unique(labels):\n",
    "#         a = float(len(labels))\n",
    "#         b = 2.0 * float((np.sum(labels==l)))\n",
    "#         c_prob[int(l)] = a/b\n",
    "\n",
    "#     c_prob = np.array(c_prob).reshape(-1)\n",
    "\n",
    "#     for l in np.argmax(train_y, 1):\n",
    "#         train_weights.append(c_prob[int(l)])\n",
    "#     train_weights = np.array(train_weights)\n",
    "        \n",
    "#     scaler = MinMaxScaler().fit(train_x.reshape(-1, tree_row * tree_col))\n",
    "#     train_x = np.clip(scaler.transform(train_x.reshape(-1, tree_row * tree_col)), 0, 1).reshape(-1, tree_row, tree_col)\n",
    "#     test_x = np.clip(scaler.transform(test_x.reshape(-1, tree_row * tree_col)), 0, 1).reshape(-1, tree_row, tree_col)\n",
    "\n",
    "#     train = [train_x, train_y]\n",
    "#     test = [test_x, test_y]\n",
    "\n",
    "#     x_train1 = train_x\n",
    "#     y_train1 = train_y\n",
    "#     x_test1 = test_x\n",
    "#     y_test1 = test_y\n",
    "        \n",
    "#         y_train1 = train_y\n",
    "#         y_test1 = test_y\n",
    "        \n",
    "#         x_train1 = np.zeros(train_x.shape)\n",
    "#         x_train1[train_x != 0] = 1\n",
    "        \n",
    "#         x_test1 = np.zeros(test_x.shape)\n",
    "#         x_test1[test_x != 0] = 1\n",
    "        \n",
    "        # for i in range(len(train_x)):\n",
    "        #     for j in range(len(test_x)):\n",
    "        #         if np.array_equal(train_x[i], test_x[j]):\n",
    "        #             print('train')\n",
    "        #             print(train_x[i])\n",
    "        #             print('test')\n",
    "        #             print(test_x[j])\n",
    "        \n",
    "        \n",
    "#     model = ResNet(height = train_x.shape[1], width = train_x.shape[2], channels = 1, classes = 2)\n",
    "#     model.init_model()\n",
    "#     model.train(train_x, train_y, test_x, y_test1, dataset, use_weights = False)\n",
    "#     y_pred = model.predict(test_x)\n",
    "#     auc_roc, auc_pr, f1, mcc = model.evaluate(test_y, y_pred)\n",
    "#     data_lst.append([auc_roc, auc_pr, f1, mcc])\n",
    "#     #model.model.save_weights(path + \"/model_weights.h5\")\n",
    "#     print(test_y)\n",
    "#     print(y_pred)\n",
    "#     print(model.model.summary())\n",
    "    \n",
    "#     fold += 1\n",
    "#run += 1\n",
    "\n",
    "model = ResNet(height = input.shape[1], width = input.shape[2], channels = 1, classes = 2)\n",
    "model.init_model()\n",
    "model.train(input, target, input, target, dataset, use_weights = False)\n",
    "y_pred = model.predict(input)\n",
    "auc_roc, auc_pr, f1, mcc = model.evaluate(target, y_pred)\n",
    "data_lst.append([auc_roc, auc_pr, f1, mcc])\n",
    "#model.model.save_weights(path + \"/model_weights.h5\")\n",
    "print(target)\n",
    "print(y_pred)\n",
    "print(model.model.summary())\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying Accuracy Metrics and Saving Metrics\n",
    "\n",
    "Option to save results of all k folds and weights of last model into same directy as data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = [str(i) for i in range(1,k+1)]    \n",
    "results_df = pd.DataFrame(data_lst, columns = ['auc(roc)', 'auc(pr)', 'f1', 'mcc'])\n",
    "results_df = results_df.transpose()\n",
    "results_df.columns = col\n",
    "\n",
    "#results_df.to_csv(path + \"/results.csv\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Model Weights\n",
    "\n",
    "Option to save model weights of last model in k fold into same directy as data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.model.save_weights(path + \"/model_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
